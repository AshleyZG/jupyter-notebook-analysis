{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2516572it [02:55, 14298.82it/s]\n",
      "100%|██████████| 2516572/2516572 [03:01<00:00, 13869.84it/s]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import re\n",
    "\n",
    "import random\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from time import time\n",
    "\n",
    "import pickle\n",
    "\n",
    "graphs = []\n",
    "with open('./graphs/cell_with_func_python23_1_27.txt','r') as f:\n",
    "    for l in tqdm(f):\n",
    "        graphs.append(json.loads(l))\n",
    "\n",
    "def clean_code_snippet(code):\n",
    "    \n",
    "    return re.sub('[^a-zA-Z\\n]+', ' ', code)\n",
    "\n",
    "\n",
    "def split_func_name(func):\n",
    "    \"\"\"\n",
    "    split function names\n",
    "    eg. sklearn.metrics.pairwise.cosine_similarity -> [sklearn, metrics, pairwise, cosine, similarity]\n",
    "    \"\"\"\n",
    "    new_str = ''\n",
    "    for i, l in enumerate(func):\n",
    "        if i > 0 and l.isupper() and func[i - 1].islower():\n",
    "            new_str += '.'\n",
    "        elif i > 0 and i < len(func) - 1 and l.isupper() and func[i - 1].isupper() and func[i + 1].islower():\n",
    "            new_str += '.'\n",
    "        elif i > 0 and l.isdigit() and func[i - 1].isalpha():\n",
    "            new_str += '.'\n",
    "        elif i < len(func) - 1 and l.isalpha() and func[i - 1].isdigit():\n",
    "            new_str += '.'\n",
    "        else:\n",
    "            pass\n",
    "        new_str += l\n",
    "    return re.split('\\.|_|\\s', new_str.lower())\n",
    "\n",
    "\n",
    "corpus = [clean_code_snippet(g[\"context\"]) for g in graphs]\n",
    "\n",
    "clean_data = []\n",
    "for c in tqdm(corpus):\n",
    "    token = split_func_name(c)\n",
    "    token = [t for t in token if t]\n",
    "    clean_data.append(' '.join(token))\n",
    "    \n",
    "\n",
    "documents = clean_data\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=10000)\n",
    "\n",
    "X = vectorizer.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "848.4674320220947\n"
     ]
    }
   ],
   "source": [
    "lda = LatentDirichletAllocation(n_components=100, random_state=0, max_iter=1)\n",
    "\n",
    "start_time = time()\n",
    "\n",
    "lda.fit(X)\n",
    "\n",
    "print(time()-start_time)\n",
    "\n",
    "pickle.dump(lda, open('./lda.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = pickle.load(open('./lda.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "                          evaluate_every=-1, learning_decay=0.7,\n",
       "                          learning_method='batch', learning_offset=10.0,\n",
       "                          max_doc_update_iter=100, max_iter=1,\n",
       "                          mean_change_tol=0.001, n_components=50, n_jobs=None,\n",
       "                          perp_tol=0.1, random_state=0, topic_word_prior=None,\n",
       "                          total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = loaded_model.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.dump('./lda_results_1_30_2.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2516572, 100)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy.load('./lda_results_1_30_2.npy', allow_pickle=True).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['import numpy as np import pandas as pd from statsmodels sandbox regression import gmm dta pd read csv consumption csv dta iloc',\n",
       " 'def moment consumption params exog beta gamma params r forw c forw c exog t err beta r forw np power c forw c gamma return err',\n",
       " 'endog np zeros exog shape mod gmm nonlinear ivgmm endog exog instrument moment consumption k moms w inv np dot instrument t instrument len endog res mod fit maxiter inv weights w inv',\n",
       " 'print res summary yname euler eq xname discount crra',\n",
       " 'res hac s mod fit maxiter inv weights w inv weights method hac wargs maxlag print print res hac s summary yname euler eq xname discount crra',\n",
       " 'def moment consumption params exog beta gamma params r forw c forw c exog t predicted beta r forw np power c forw c gamma return predicted',\n",
       " 'endog np ones exog shape mod gmm nonlinear ivgmm endog exog instrument moment consumption k moms w inv np dot instrument t instrument len endog res hac s mod fit maxiter inv weights w inv weights method hac wargs maxlag',\n",
       " 'print res hac s summary yname euler eq xname discount crra',\n",
       " 'res hac s params',\n",
       " 'res hac s params',\n",
       " 'res hac s params res hac s params np max np abs res hac s params res hac s params',\n",
       " 'res mod fit maxiter inv weights w inv weights method hac wargs maxlag centered false optim args disp print res params print res bse',\n",
       " 'res hac i mod fit maxiter inv weights w inv weights method hac wargs maxlag optim args disp',\n",
       " 'print res hac i params',\n",
       " 'res i mod fit maxiter inv weights w inv optim args disp',\n",
       " 'print res i summary yname euler eq xname discount crra',\n",
       " 'from i python display import html display display html table tr td img src auth raw jpg td td img src auth procc png td td img src auth split png td td authorized images br left raw image br center after processing br right pickle file br td tr table display html table tr td img src unauth raw jpg td td img src unauth procc png td td img src unauth split png td td unauthorized images br left raw image br center after processing br right pickle file br td tr table',\n",
       " 'from i python display import html html script ncode show true nfunction code toggle n if code show n div input hide n else n div input show n n code show code show n n document ready code toggle n script n form action javascript code toggle input type submit value click here to toggle on off the raw code form',\n",
       " 'get ipython run line magic matplotlib inline',\n",
       " 'get ipython run line magic pylab inline',\n",
       " 'from pylab import',\n",
       " 'import matplotlib import matplotlib pyplot as plt',\n",
       " 'import numpy as np',\n",
       " 'from pylab import',\n",
       " 'x np linspace y x',\n",
       " 'figure plot x y r xlabel x ylabel y title title show',\n",
       " 'subplot plot x y r subplot plot y x g',\n",
       " 'fig plt figure axes fig add axes axes plot x y r axes set xlabel x axes set ylabel y axes set title title',\n",
       " 'fig axes plt subplots axes plot x y r axes set xlabel x axes set ylabel y axes set title title',\n",
       " 'fig axes plt subplots nrows ncols for ax in axes ax plot x y r ax set xlabel x ax set ylabel y ax set title title',\n",
       " 'fig axes plt subplots nrows ncols for ax in axes ax plot x y r ax set xlabel x ax set ylabel y ax set title title fig tight layout',\n",
       " 'fig plt figure figsize dpi',\n",
       " 'fig axes plt subplots figsize axes plot x y r axes set xlabel x axes set ylabel y axes set title title',\n",
       " 'fig savefig filename png',\n",
       " 'fig savefig filename png dpi',\n",
       " 'ax set title title',\n",
       " 'ax set xlabel x ax set ylabel y',\n",
       " 'ax legend curve curve curve',\n",
       " 'ax plot x x label curve ax plot x x label curve ax legend',\n",
       " 'ax legend loc ax legend loc ax legend loc ax legend loc ax legend loc',\n",
       " 'fig ax plt subplots ax plot x x label y x ax plot x x label y x ax legend loc ax set xlabel x ax set ylabel y ax set title title',\n",
       " 'fig ax plt subplots ax plot x x label y alpha ax plot x x label y alpha ax legend loc ax set xlabel alpha fontsize ax set ylabel y fontsize ax set title title',\n",
       " 'matplotlib rc params update font size font family serif',\n",
       " 'fig ax plt subplots ax plot x x label y alpha ax plot x x label y alpha ax legend loc ax set xlabel alpha ax set ylabel y ax set title title',\n",
       " 'matplotlib rc params update font size font family stix general mathtext fontset stix',\n",
       " 'fig ax plt subplots ax plot x x label y alpha ax plot x x label y alpha ax legend loc ax set xlabel alpha ax set ylabel y ax set title title',\n",
       " 'matplotlib rc params update font size text usetex true',\n",
       " 'fig ax plt subplots ax plot x x label y alpha ax plot x x label y alpha ax legend loc ax set xlabel alpha ax set ylabel y ax set title title',\n",
       " 'matplotlib rc params update font size font family sans text usetex false',\n",
       " 'ax plot x x b ax plot x x g',\n",
       " 'fig ax plt subplots ax plot x x color red alpha ax plot x x color dd ax plot x x color cc',\n",
       " 'matplotlib rc params xtick major pad matplotlib rc params ytick major pad',\n",
       " 'n np array',\n",
       " 'fig plt figure ax fig add axes polar true t np linspace np pi ax plot t t color blue lw',\n",
       " 'fig ax plt subplots ax plot xx xx xx xx ax text y x fontsize color blue ax text y x fontsize color green',\n",
       " 'fig ax plt subplots fig tight layout',\n",
       " 'import matplotlib gridspec as gridspec',\n",
       " 'fig plt figure figsize gs gridspec grid spec height ratios width ratios for g in gs ax fig add subplot g fig tight layout',\n",
       " 'alpha phi ext np pi def flux qubit potential phi m phi p return alpha np cos phi p np cos phi m alpha np cos phi ext phi p',\n",
       " 'phi m np linspace np pi phi p np linspace np pi x y np meshgrid phi p phi m z flux qubit potential x y t',\n",
       " 'fig ax plt subplots p ax pcolor x np pi y np pi z cmap matplotlib cm rd bu vmin abs z min vmax abs z max cb fig colorbar p ax ax',\n",
       " 'fig ax plt subplots im ax imshow z cmap matplotlib cm rd bu vmin abs z min vmax abs z max extent im set interpolation bilinear cb fig colorbar im ax ax',\n",
       " 'fig ax plt subplots cnt ax contour z cmap matplotlib cm rd bu vmin abs z min vmax abs z max extent',\n",
       " 'from mpl toolkits mplot d axes d import axes d',\n",
       " 'fig plt figure figsize ax fig add subplot projection d p ax plot wireframe x y z rstride cstride',\n",
       " 'print matplotlib rcsetup all backends',\n",
       " 'import matplotlib matplotlib use svg import matplotlib pylab as plt import numpy from i python display import image svg',\n",
       " 'fig ax plt subplots t numpy linspace ax plot t numpy cos t numpy sin t plt savefig test svg',\n",
       " 'svg filename test svg',\n",
       " 'get ipython run line magic matplotlib inline get ipython run line magic config inline backend figure format svg import matplotlib pylab as plt import numpy',\n",
       " 'fig ax plt subplots t numpy linspace ax plot t numpy cos t numpy sin t plt savefig test svg',\n",
       " 'import matplotlib matplotlib use qt agg import matplotlib pylab as plt import numpy as np',\n",
       " 'fig ax plt subplots t np linspace ax plot t np cos t np sin t plt show',\n",
       " 'preds',\n",
       " 'from future import print function import pandas as pd from xgboost import xgb classifier from sklearn cross validation import stratified k fold from sklearn cross validation import cross val score from sklearn preprocessing import label encoder import time from matplotlib import pyplot',\n",
       " 'data pd read csv train csv dataset data values x dataset y dataset',\n",
       " 'print data shape data head',\n",
       " 'with pd option context display max rows display max columns print data describe',\n",
       " 'pots data isnull sum for pot chicken in pots iteritems print pot chicken',\n",
       " 'label encoded y label encoder fit transform y kfold stratified k fold label encoded y n folds shuffle true random state',\n",
       " 'data target label encoded y covariance data cov sorted covs sorted feat val for feat val in covariance target iteritems key lambda x abs x reverse true for x in sorted covs print x',\n",
       " 'start time time model xgb classifier nthread results cross val score model x label encoded y cv kfold scoring log loss n jobs elapsed time time start print single thread xg boost parallel thread cv f elapsed',\n",
       " 'start time time model xgb classifier nthread results cross val score model x label encoded y cv kfold scoring log loss n jobs elapsed time time start print parallel thread xg boost single thread cv f elapsed',\n",
       " 'start time time model xgb classifier nthread results cross val score model x label encoded y cv kfold scoring log loss n jobs elapsed time time start print parallel thread xg boost and cv f elapsed',\n",
       " 'import numpy as np import pandas as pd import matplotlib pyplot as plt from sklearn externals import joblib get ipython magic matplotlib inline',\n",
       " 'amazon pd read csv raw data train csv encoding latin print amazon shape',\n",
       " 'print amazon head print amazon helpful mean',\n",
       " 'from sklearn feature extraction text import hashing vectorizer hv hashing vectorizer n features non negative true x hv hv fit transform amazon text print x hv shape',\n",
       " 'joblib dump hv hv pkl',\n",
       " 'from sklearn feature extraction text import tfidf transformer transformer tfidf transformer x tfidf transformer fit transform x hv joblib dump transformer transformer pkl',\n",
       " 'print type x tfidf',\n",
       " 'from scipy sparse import csr matrix hstack x quant features csr csr matrix x quant features x combined hstack x tfidf x quant features csr x matrix csr matrix x combined print x matrix shape',\n",
       " 'from sklearn preprocessing import standard scaler sc standard scaler with mean false x sc fit transform x matrix print x shape joblib dump sc sc pkl',\n",
       " 'y amazon helpful values print type y',\n",
       " 'from sklearn import linear model svm linear model sgd classifier svm fit x y joblib dump svm svm pkl svm performance binary classification performance svm predict x y svm svm performance compute measures print svm performance performance measures',\n",
       " 'from sklearn import linear model lgs linear model sgd classifier loss log n iter alpha e lgs fit x y joblib dump lgs lgs pkl lgs performance binary classification performance lgs predict x y lgs lgs performance compute measures print lgs performance performance measures',\n",
       " 'from sklearn naive bayes import multinomial nb nbs multinomial nb nbs fit x y joblib dump nbs nbs pkl nbs performance binary classification performance nbs predict x y nbs nbs performance compute measures print nbs performance performance measures',\n",
       " 'from sklearn import linear model rdg linear model ridge classifier rdg fit x y joblib dump rdg rdg pkl rdg performance binary classification performance rdg predict x y rdg rdg performance compute measures print rdg performance performance measures',\n",
       " 'from sklearn import linear model prc linear model sgd classifier loss perceptron prc fit x y joblib dump prc prc pkl prc performance binary classification performance prc predict x y prc prc performance compute measures print prc performance performance measures',\n",
       " 'from sklearn import neural network nn neural network mlp classifier hidden layer sizes nn fit x y joblib dump nn nn pkl nn performance binary classification performance nn predict x y nn nn performance compute measures print nn performance performance measures',\n",
       " 'import numpy as np from numpy random import randn import pandas as pd from scipy import stats import matplotlib as mpl import matplotlib pyplot as plt import seaborn as sns get ipython run line magic matplotlib inline',\n",
       " 'flight dframe sns load dataset flights',\n",
       " 'flight dframe head',\n",
       " 'flight dframe flight dframe pivot month year passengers',\n",
       " 'flight dframe',\n",
       " 'sns heatmap flight dframe',\n",
       " 'sns heatmap flight dframe annot true fmt d',\n",
       " 'sns heatmap flight dframe center flight dframe loc january',\n",
       " 'sns clustermap flight dframe',\n",
       " 'sns clustermap flight dframe col cluster false',\n",
       " 'sns clustermap flight dframe standard scale',\n",
       " 'sns clustermap flight dframe standard scale',\n",
       " 'sns clustermap flight dframe z score',\n",
       " 'from sklearn feature extraction text import count vectorizer cv count vectorizer analyzer word ngram range stop words my stop word list vocabulary none max df max features',\n",
       " 'texts nobody can stop me word is a building blocks of a text i like doing feature extraction on text i do not like digits in text like',\n",
       " 'transformed texts cv cv fit transform texts print obtained feature matrix x print transformed texts cv todense n',\n",
       " 'print dictionary for k v in sorted cv vocabulary items reverse false print column index token format v k',\n",
       " 'new text i like feature extraction very much new transformed cv transform new text print n new sentence transformed print new transformed todense n',\n",
       " 'import numpy as np texts sometimes you eat the bear and sometimes the bear eats you this is wrong with your brain on the left side there is nothing right and on the right side there is nothing left vocab sometimes bear left right and side eat pass',\n",
       " 'from sklearn feature extraction text import tfidf vectorizer tv tfidf vectorizer analyzer word ngram range stop words my stop word list vocabulary none max df max features smooth idf true norm l',\n",
       " 'transformed texts tv tv fit transform texts print obtained feature matrix x see l norm is used print transformed texts tv todense n',\n",
       " 'print dictionary for k v in sorted tv vocabulary items reverse false print column index token format v k',\n",
       " 'new text i like extraction very much new transformed tv transform new text print n new sentence transformed print new transformed todense n',\n",
       " 'from sklearn feature extraction text import hashing vectorizer hv hashing vectorizer analyzer word ngram range stop words my stop word list n features non negative true norm none',\n",
       " 'transformed texts hv hv fit transform texts print obtained feature matrix x see no norm is used print transformed texts hv todense n',\n",
       " 'print dictionary print oops hashing trick assumes no vocabulary will be used at all online learning print however we won t be able to do reverse transform and to get exact words',\n",
       " 'new text i like extraction very much new transformed hv transform new text print n new sentence transformed print new transformed todense n',\n",
       " 'import pandas as pd import numpy as np from textblob import word text blob import re import nltk nltk download punkt pass',\n",
       " 'reviews waste of time for the plot and for acting awful film nobody can like it wow am i impressed totally d token lf get token level features reviews token lf',\n",
       " 'token lf get text level features reviews token lf',\n",
       " 'from sklearn pipeline import feature union pipeline from sklearn preprocessing import function transformer def features x return x def features x return x features function transformer func features validate false accept sparse true features function transformer func features validate false accept sparse true',\n",
       " 'x np array print x n format x',\n",
       " 'combined features feature union f features f features print combined features',\n",
       " 'print n combined features combined features transform x',\n",
       " 'from future import division import numpy as np import matplotlib pyplot as plt import scipy import scipy io as sio import sklearn metrics as metrics import csv get ipython magic matplotlib inline',\n",
       " 'data scipy io loadmat data hw cs fa joke data joke train mat train r data train validation set np loadtxt data hw cs fa joke data validation txt delimiter validation idx validation set validation r validation set',\n",
       " 'print train r shape',\n",
       " 'zeroed r np nan to num train r print train r',\n",
       " 'def predict u x indices predictions for user joke in indices rate u user dot x t joke t if rate predictions append else predictions append return predictions',\n",
       " 'def validate new r train r d u x train svd new r d error mse u x train r print when d d mse is error prediction predict u x validation idx validation score metrics accuracy score validation r prediction print validation accuracy format validation score',\n",
       " 'validate zeroed r train r',\n",
       " 'validate zeroed r train r',\n",
       " 'validate zeroed r train r',\n",
       " 'validate zeroed r train r',\n",
       " 'd u v train svd zeroed r d u x new update zeroed r alpha max iter dim d svd u u svd v v',\n",
       " 'd u v train svd zeroed r d u x new update zeroed r alpha max iter dim d svd u u svd v v',\n",
       " 'd u v train svd zeroed r d u v new update zeroed r alpha max iter dim d svd u u svd v v',\n",
       " 'd u v train svd zeroed r d u v new update zeroed r alpha max iter dim d svd u u svd v v',\n",
       " 'd u v train svd zeroed r d u v new update zeroed r alpha max iter dim d svd u u svd v v',\n",
       " 'totalvocab stemmed totalvocab tokenized for d in docs totalvocab stemmed extend tokenize and stem d totalvocab tokenized extend tokenize only d',\n",
       " 'import pandas as pd vocab frame pd data frame words totalvocab tokenized index totalvocab stemmed print vocab frame shape print vocab frame',\n",
       " 'from sklearn feature extraction text import tfidf vectorizer tfidf vectorizer tfidf vectorizer max df max features min df stop words english use idf true tokenizer tokenize and stem ngram range tfidf matrix tfidf vectorizer fit transform docs print tfidf matrix shape',\n",
       " 'print tfidf matrix',\n",
       " 'terms tfidf vectorizer get feature names print terms',\n",
       " 'from sklearn metrics pairwise import cosine similarity dist cosine similarity tfidf matrix print dist shape',\n",
       " 'from sklearn import metrics from sklearn cluster import k means cnum km k means init k means n clusters cnum n init random state km fit tfidf matrix clusters km labels tolist',\n",
       " 'print list zip titles clusters',\n",
       " 'from sklearn manifold import mds mds mds n components dissimilarity precomputed random state pos mds fit transform dist xs ys pos pos',\n",
       " 'import random cluster colors cluster names cols b g r c m y for i in range cnum cluster colors i cols i cluster names i join cents words i',\n",
       " 'from scipy cluster hierarchy import ward dendrogram linkage matrix ward dist fig ax plt subplots figsize ax dendrogram linkage matrix orientation right labels titles plt tick params labelbottom off plt tight layout',\n",
       " 'import numpy as np fsampling tf t np arange tf fsampling f m a m m t a m np cos np pi f m t',\n",
       " 'import matplotlib pyplot as plt plt plot t m t plt grid plt title gr fico plt ylabel amplitude plt xlabel tempo s plt show',\n",
       " 'intervalo dft stop time start time intervalo fft stop time start time print a fft intervalo dft intervalo fft mais r pida que a dft',\n",
       " 'get ipython run line magic matplotlib inline from spectrum window import window n pontos h window n pontos name hamming h plot time freq',\n",
       " 'f window n pontos name flattop f plot time freq',\n",
       " 'b window n pontos name blackman b plot time freq',\n",
       " 'import pandas as pd import numpy as np import glob',\n",
       " 'path data crime filenames glob glob path csv crime df pd concat pd read csv f for f in filenames ignore index true',\n",
       " 'totalcrime df crime df groupby lsoa code size to frame crimes',\n",
       " 'path data iod file id all ranks deciles and scores for the indices of deprivation and population denominators csv iod df pd read csv path',\n",
       " 'cols iod df columns cols col for col in cols if score in col cols lsoa code cols iod df iod df cols iod df rename columns lsoa code lsoa code inplace true iod df iod df set index lsoa code',\n",
       " 'data df iod df merge totalcrime df left index true right index true',\n",
       " 'x data df drop crimes axis values y data df crimes',\n",
       " 'from sklearn import linear model reg linear model ridge alpha',\n",
       " 'from sklearn import svm reg svm svr',\n",
       " 'reg fit x y',\n",
       " 'y pred reg predict x r reg score x y r',\n",
       " 'import matplotlib pyplot as plt plt plot y y pred o plt show',\n",
       " 'import numpy as np import scipy as sc from sklearn linear model import logistic regression linear regression import plotly offline as py from plotly graph objs import from plotly offline import download plotlyjs init notebook mode plot iplot init notebook mode from itertools import product np random seed',\n",
       " 'number of observations dog weight mean dog weight stddev dog height mean dog height stddev horse weight mean horse weight stddev horse height mean horse height stddev',\n",
       " 'model logistic regression model fit x y',\n",
       " 'dog weight histogram histogram x x y name dog weights horse weight histogram histogram x x y name horse weights histogram data data dog weight histogram horse weight histogram py iplot histogram data',\n",
       " 'sample weights np linspace number of observations sample heights np linspace number of observations samples list product sample weights sample heights hypotheses model predict proba samples hypotheses np reshape hypotheses len sample weights len sample heights',\n",
       " 'import pandas as pd import numpy as np import matplotlib pyplot as plt import seaborn as sns from sklearn model selection import cross val score learning curve validation curve shuffle split train test split from sklearn ensemble import random forest classifier from sklearn metrics import confusion matrix from i python display import display import sklearn cross validation as cv',\n",
       " 'x pd read csv projects capstone model selection pro data csv y pd read csv projects capstone model selection pro y csv header none y y',\n",
       " 'import sklearn import sklearn model selection x train x test y train y test sklearn model selection train test split x y',\n",
       " 'rnd clf random forest classifier n estimators n jobs rnd clf fit x train y train',\n",
       " 'from sklearn ensemble import bagging classifier from sklearn tree import decision tree classifier bag clf bagging classifier decision tree classifier random state n estimators max samples bootstrap true n jobs random state bag clf fit x train y train y pred bag clf predict x test',\n",
       " 'from sklearn metrics import accuracy score print accuracy score y test y pred',\n",
       " 'tree clf decision tree classifier random state tree clf fit x train y train y pred tree tree clf predict x test print accuracy score y test y pred tree',\n",
       " 'bag clf bagging classifier decision tree classifier splitter random max leaf nodes random state n estimators max samples bootstrap true n jobs random state',\n",
       " 'bag clf fit x train y train y pred bag clf predict x test print accuracy score y test y pred',\n",
       " 'from sklearn ensemble import random forest classifier rnd clf random forest classifier n estimators max leaf nodes n jobs random state rnd clf fit x train y train y pred rf rnd clf predict x test',\n",
       " 'print accuracy score y test y pred rf',\n",
       " 'bag clf bagging classifier decision tree classifier random state n estimators bootstrap true n jobs oob score true random state bag clf fit x train y train bag clf oob score',\n",
       " 'bag clf oob decision function',\n",
       " 'from sklearn metrics import accuracy score y pred bag clf predict x test accuracy score y test y pred',\n",
       " 'from sklearn ensemble import random forest classifier rf clf random forest classifier n estimators max leaf nodes n jobs random state rf clf fit x train y train y pred rf rf clf predict x test',\n",
       " 'evaluate model rf clf',\n",
       " 'from sklearn grid search import grid search cv params class weight grid grid search cv random forest classifier max depth n estimators max features auto params scoring log loss evaluate model grid',\n",
       " 'grid best estimator',\n",
       " 'rf maxd grid best estimator',\n",
       " 'rf maxd fit x train y train',\n",
       " 'from sklearn metrics import classification report rf maxd score x train y train',\n",
       " 'from sklearn metrics import classification report rf pred train rf maxd predict x train target names class no class yes print classification report y train rf pred train target names target names',\n",
       " 'rf pred test rf maxd predict x test target names class no class yes print classification report y test rf pred test target names target names',\n",
       " 'feature selection x duration mins nr employed euribor m emp var rate pdays strat',\n",
       " 'from sklearn ensemble import random forest classifier rnd clf random forest classifier n estimators max leaf nodes n jobs random state rnd clf fit x train y train y pred rf rnd clf predict x test',\n",
       " 'get ipython magic matplotlib inline from pylab import x y create clustered data plt figure figsize plt scatter x x c y astype np float plt show',\n",
       " 'from sklearn import svm datasets c svc svm svc kernel linear c c fit x y',\n",
       " 'svc predict',\n",
       " 'svc predict',\n",
       " 'dims fig ax plt subplots figsize dims bargraph sns barplot x counts by pops index y counts by pops bargraph set xticklabels labels counts by pops index rotation bargraph set xlabel population ylabel number of crimes title bar graph showing the number of crimes for communities with a given population sns plt show',\n",
       " 'counts by community plot kind hist f plt show f plt gcf',\n",
       " 'fig pd scatter matrix df stats',\n",
       " 'axes df stats community comm pop crime plot bar figsize axes set xticklabels labels df stats community rotation axes set xlabel community ylabel crime and population counts title bar graph showing crime and population totals for each community',\n",
       " 'axes df stats community crimerate plot bar figsize axes set xticklabels labels df stats community rotation axes set xlabel community ylabel crime rate title bar graph showing crime rates for each community',\n",
       " 'pairplots sns pairplot df stats pairplots set xticklabels',\n",
       " 'axes counts by community plot kind hist figsize axes set xlabel number of crimes ylabel frequency title histogram displaying the frequency of crime totals f plt show f plt gcf',\n",
       " 'x df stats comm pop y df stats crime plt plot x y o df stats corr',\n",
       " 'model sm ols x y fit to get residuals model fit plt hist fit to get residuals resid plt ylabel population count plt xlabel normalized residuals plt title histogram for residuals',\n",
       " 'crime corr df stats corr crime heatmap sns heatmap crime corr xticklabels crime corr columns values yticklabels crime corr columns values crime heatmap set title heat map correlation matrix',\n",
       " 'df df loc df year j points for i in range latitude df lat j longitude df lon j points append tuple latitude longitude j j',\n",
       " 'csv file abc licenses sdcounty csv df pd read csv csv file',\n",
       " 'points j df alcohol pd read csv abc licenses sdcounty csv df alcohol df alcohol loc df alcohol premisesaddress str contains san diego ca for j row in df alcohol iterrows latitude row lat longitude row lon points append tuple latitude longitude',\n",
       " 'df clairemont df loc df community san cla df encanto df loc df community san enc df clairemont comm pop value counts',\n",
       " 'clairemont population final population',\n",
       " 'encanto population final population',\n",
       " 'import shapefile sf shapefile reader zillow neighborhoods ca dbf',\n",
       " 'metadata sf shape records metadata record',\n",
       " 'sd list counter for i in range len metadata if metadata i record san diego sd list append i counter',\n",
       " 'read shapemeta shapefile reader zillow neighborhoods ca dbf shapemeta read shapemeta shape records sorted sd list for i in range len shapemeta if metadata i record san diego sorted sd list append shapemeta i record',\n",
       " 'for i in range len shapemeta if metadata i record san diego sorted sd list append shapemeta i record',\n",
       " 'import os import numpy as np import pandas as pd import datetime as date from sklearn cross validation import train test split from sklearn import svm import seaborn as sns import matplotlib pyplot as plt import bokeh charts from bokeh plotting import figure column data source show from bokeh models import hover tool from bokeh io import output notebook import statsmodels formula api as smf get ipython magic matplotlib inline output notebook import warnings warnings filterwarnings ignore',\n",
       " 'comb comb drop player id name season ending team games played goals per game assists per game age bm goals assists total points per game seasons played seasons played on team s score norm s score standard s score axis',\n",
       " 'comb dtypes comb describe',\n",
       " 'make moons noise random state',\n",
       " 'comb comb transfer isnull',\n",
       " 'x train min',\n",
       " 'import numpy as np import matplotlib pyplot as plt from matplotlib import offsetbox from sklearn import manifold datasets decomposition ensemble discriminant analysis random projection get ipython run line magic matplotlib inline digits datasets load digits n class x digits data y digits target n samples n features x shape',\n",
       " 'print computing t sne embedding tsne manifold tsne n components init pca random state x tsne tsne fit transform x plot embedding x tsne t sne embedding of the digits plt show',\n",
       " 'import pandas as pd primary results pd read csv data primary results csv county facts pd read csv data county facts csv county facts dict pd read csv data county facts dictionary csv dict county facts k v for k v in zip county facts dict column name values county facts dict description values',\n",
       " 'trump results primary results primary results candidate donald trump df all trump results merge county facts on fips',\n",
       " 'df work df all edu edu man rhi rhi data df work values names df all state values',\n",
       " 'tsne manifold tsne n components init pca random state x tsne fit transform data x min x max np min x np max x x x x min x max x min',\n",
       " 'plt figure figsize ax plt subplot plt scatter x x',\n",
       " 'import i python numpy as np scipy as sp matplotlib pyplot as plt matplotlib sklearn librosa from i python display import audio import os os path get ipython run line magic matplotlib inline import c pickle',\n",
       " 'librosa loader librosa load',\n",
       " 'my sounds c pickle load open my sounds pkl r audio my sounds screams rate my sounds screams',\n",
       " 'mfcc librosa feature mfcc y my sounds singing sr my sounds singing',\n",
       " 'librosa display specshow mfcc x axis time',\n",
       " 'chroma librosa feature chromagram y my sounds singing sr my sounds singing chroma librosa feature chroma stft y my sounds singing sr my sounds singing',\n",
       " 'librosa display specshow chroma x axis time',\n",
       " 'librosa display specshow chroma x axis time',\n",
       " 'screams chroma librosa feature chromagram y my sounds screams sr my sounds screams',\n",
       " 'librosa display specshow screams chroma x axis time',\n",
       " 'x librosa feature rmse my sounds singing x x flatten diffs x i x i for i in xrange len x plt plot diffs',\n",
       " 'from scipy stats import gmean',\n",
       " 'from future import print function import numpy as np from quantecon markov import random discrete dp',\n",
       " 'seed',\n",
       " 'compare performance num states num actions beta k random state seed',\n",
       " 'compare performance num states num actions beta k random state seed',\n",
       " 'compare performance num states num actions beta k random state seed',\n",
       " 'compare performance num states num actions beta k random state seed',\n",
       " 'compare performance num states num actions beta k random state seed',\n",
       " 'compare performance num states num actions beta k random state seed',\n",
       " 'compare performance num states num actions beta k random state seed',\n",
       " 'compare performance num states num actions beta k random state seed',\n",
       " 'compare performance num states num actions beta k suppress vi true random state seed',\n",
       " 'import platform print platform platform',\n",
       " 'import sys print sys version',\n",
       " 'print np version',\n",
       " 'import scipy print scipy version',\n",
       " 'import numba print numba version',\n",
       " 'import sys sys path append users shayneufeld git hub mouse bandit data preprocessing code sys path append users shayneufeld git hub mouse bandit import support functions as sf import numpy as np import matplotlib pyplot as plt from matplotlib import gridspec import seaborn as sns import pandas as pd import scipy as sp import bandit preprocessing as bp import sklearn linear model import sklearn tree get ipython run line magic matplotlib inline',\n",
       " 'data pd read csv users shayneufeld git hub mouse bandit data processed data hmm matrix full csv index col data pd read csv users shayneufeld git hub mouse bandit data processed data hmm matrix full csv index col data pd read csv users shayneufeld git hub mouse bandit data processed data hmm matrix full csv index col datas data data data models',\n",
       " 'stats head',\n",
       " 'plt figure figsize sns factorplot x no parameters y bic data stats color black plt title bic vs model flexibility',\n",
       " 'for i d in enumerate data data data if i model stats coefs logreg and eval d else model stats curr coefs curr logreg and eval data test data d stats stats append stats curr coefs coefs append coefs curr stats stats drop bic negative loglikelihood pseudo r axis',\n",
       " 'stats testing condition stats',\n",
       " 'data insert condition data insert condition data insert condition',\n",
       " 'all data data append data all data all data append data all data shape',\n",
       " 'data head',\n",
       " 'import os import numpy as np import pandas as pd from scipy import stats import seaborn as sns import matplotlib import matplotlib pyplot as plt plt style use fivethirtyeight import warnings warnings filterwarnings ignore pd options display max columns get ipython magic matplotlib inline get ipython magic config inline backend figure format retina',\n",
       " 'df pd read csv users alexpapiu downloads wholesale customers data csv df head',\n",
       " 'df region name df region map lisabon porto other df channel name df channel map hotel restaurant retail',\n",
       " 'num df df fresh milk grocery frozen detergents paper delicassen good index num df np abs stats zscore num df all axis index df df loc good index',\n",
       " 'sns pairplot df loc fresh delicassen',\n",
       " 'correlations df loc fresh delicassen corr',\n",
       " 'plt figure figsize sns heatmap correlations',\n",
       " 'sns pairplot df fresh milk delicassen grocery channel name detergents paper hue channel name',\n",
       " 'df groupby channel name mean',\n",
       " 'df grocery channel name pivot columns channel name values grocery plot density plt title gorcery sales vs channel',\n",
       " 'from sklearn cluster import k means from sklearn preprocessing import standard scaler min max scaler from sklearn pipeline import make pipeline',\n",
       " 'x df loc fresh delicassen',\n",
       " 'x head',\n",
       " 'kmeans make pipeline min max scaler k means n clusters random state',\n",
       " 'kmeans fit x',\n",
       " 'x clusters kmeans named steps kmeans labels',\n",
       " 'sns pairplot x hue clusters',\n",
       " 'sns lmplot x fresh y frozen data x hue clusters fit reg false',\n",
       " 'centers kmeans named steps kmeans cluster centers',\n",
       " 'centers pd data frame centers columns x columns',\n",
       " 'centers iloc plot kind barh plt title cluster center coordinates',\n",
       " 'centers iloc plot kind barh plt title cluster center coordinates',\n",
       " 'centers iloc plot kind barh plt title cluster center coordinates',\n",
       " 'centers iloc plot kind barh plt title cluster center coordinates',\n",
       " 'x df loc fresh delicassen kmeans make pipeline min max scaler k means n clusters random state kmeans fit x x clusters kmeans named steps kmeans labels',\n",
       " 'sns pairplot x hue clusters',\n",
       " 'centers kmeans named steps kmeans cluster centers',\n",
       " 'centers kmeans named steps kmeans cluster centers centers pd data frame centers columns x columns',\n",
       " 'centers iloc plot kind barh plt title cluster center coordinates',\n",
       " 'centers iloc plot kind barh plt title cluster center coordinates',\n",
       " 'df cluster x clusters',\n",
       " 'df groupby cluster channel name frozen count unstack plot kind bar plt title channel counts based on clusters',\n",
       " 'df groupby cluster channel name frozen count',\n",
       " 'from keras datasets import cifar import numpy as np import matplotlib pyplot as plt from scipy misc import toimage import keras x train y train x test y test cifar load data',\n",
       " 'from keras utils import np utils nb classes x train x train astype float x test x test astype float x train x test print x train shape train samples print x test shape test samples y train np utils to categorical y train nb classes y test np utils to categorical y test nb classes',\n",
       " 'from keras optimizers import adam model compile loss categorical crossentropy optimizer adam metrics accuracy',\n",
       " 'batch size epochs history model fit x train y train batch size batch size epochs epochs validation split verbose',\n",
       " 'acc model evaluate x test y test verbose print accuracy format acc',\n",
       " 'from sklearn metrics import classification report confusion matrix labels pred model predict classes x test verbose print confusion matrix y test labels pred print classification report y test labels pred',\n",
       " 'refresh directories false input exists true full false augment false log info set paramters path data fish crop batch size clip bags load size aug batches',\n",
       " 'log info get vgg model vgg ft bn log info create vgg vgg vgg bn load size model vgg pop vgg pop vgg pop vgg pop vgg input shape vgg output shape vgg compile adam categorical crossentropy metrics accuracy',\n",
       " 'vgg summary',\n",
       " 'val classes trn classes val labels trn labels val filenames filenames test filenames get classes path log info read filenames raw filenames f split for f in filenames raw test filenames f split for f in test filenames raw val filenames f split for f in val filenames',\n",
       " 'if augment da trn labels np concatenate trn labels aug batches if full da val labels np concatenate val labels aug batches else da val labels val labels else da trn labels trn labels da val labels val labels',\n",
       " 'classes alb bet dol lag no f other shark yft def fish only mat return np delete mat axis trn of labels fish only da trn labels val of labels fish only da val labels',\n",
       " 'trn of labels',\n",
       " 'if full da conv trn feat np concatenate da conv trn feat da conv val feat trn of labels np concatenate trn of labels val of labels log info create and fit cnn p conv layers split at vgg convolution d nf p',\n",
       " 'def split at model layer type layers model layers layer idx index for index layer in enumerate layers if type layer is layer type return layers layer idx layers layer idx',\n",
       " 'file link subm name',\n",
       " 'get ipython magic matplotlib inline import numpy as np import matplotlib pyplot as plt import seaborn from sklearn linear model import linear regression from scipy import stats import pylab as pl seaborn set',\n",
       " 'from i python display import image image http scikit learn org dev static ml map png width',\n",
       " 'from sklearn datasets import load iris iris load iris n samples n features iris data shape print iris keys print n samples n features print iris data shape print iris target shape print iris target names print iris feature names',\n",
       " 'import numpy as np import scipy as sp from scipy import fftpack import pandas as pd import os import glob from sklearn preprocessing import standard scaler',\n",
       " 'path os getcwd data print os getcwd extension csv os chdir path titles glob glob format extension',\n",
       " 'print len datasets',\n",
       " 'def extract action stimulus action nr stimulus split splitted action nr split action splitted if len splitted nr else nr splitted return action strip nr strip',\n",
       " 'for data in datasets dataset prepare data data ss dim overlap break',\n",
       " 'import pandas as pd get ipython run line magic matplotlib inline',\n",
       " 'from sklearn import datasets from pandas tools plotting import scatter matrix from sklearn import tree from sklearn cross validation import train test split from sklearn import metrics import numpy as np',\n",
       " 'import matplotlib pyplot as plt',\n",
       " 'iris datasets load iris',\n",
       " 'iris',\n",
       " 'iris feature names',\n",
       " 'x iris data y iris target',\n",
       " 'dt tree decision tree classifier',\n",
       " 'x train x test y train y test train test split x y test size train size',\n",
       " 'dt dt fit x train y train',\n",
       " 'measure performance x test y test dt',\n",
       " 'x train x test y train y test train test split x y test size train size',\n",
       " 'dt dt fit x train y train',\n",
       " 'measure performance x test y test dt',\n",
       " 'breast cancer datasets load breast cancer',\n",
       " 'breast cancer feature names',\n",
       " 'x breast cancer data y breast cancer target',\n",
       " 'x',\n",
       " 'y',\n",
       " 'dt tree decision tree classifier',\n",
       " 'x train x test y train y test train test split x y test size train size',\n",
       " 'dt dt fit x train y train',\n",
       " 'measure performance x test y test dt',\n",
       " 'x train x test y train y test train test split x y test size train size',\n",
       " 'dt dt fit x train y train',\n",
       " 'measure performance x test y test dt',\n",
       " 'df x pd read csv attractions added csv',\n",
       " 'df y pd read csv log prices csv',\n",
       " 'miss df miss target x test y test missed points gbr df x df y',\n",
       " 'miss df shape',\n",
       " 'miss df plot scatter x longitude y latitude figsize',\n",
       " 'x test plot scatter x longitude y latitude figsize',\n",
       " 'miss df shape x test shape',\n",
       " 'numerics int int int float float float n missdf miss df select dtypes include numerics n fulldf x test select dtypes include numerics',\n",
       " 'miss mean n missdf mean',\n",
       " 'all mean n fulldf mean',\n",
       " 'diff means all mean miss mean',\n",
       " 'diff means shape',\n",
       " 'display diff means',\n",
       " 'diff means plot bar figsize',\n",
       " 'x test availability mean',\n",
       " 'miss df availability mean',\n",
       " 'missed x missed y x test y test error feat gbr df x df y',\n",
       " 'neighb list pd data frame neighb list columns neighborhood neighb sum l neighb sum m',\n",
       " 'neighb list set index neighborhood inplace true',\n",
       " 'neighb list plot bar figsize',\n",
       " 'van nest df x df x van nest',\n",
       " 'van nest shape',\n",
       " 'categorical apartment bed breakfast boat cabin castle chalet dorm earth house house lighthouse loft other tent treehouse villa entire home apt private room shared room accommodates bathrooms bedrooms beds airbed couch futon pull out sofa real bed guests included extra people new years july th christmas new years eve',\n",
       " 'cat list pd data frame cat list columns cat neighb sum l neighb sum m',\n",
       " 'cat list set index cat inplace true',\n",
       " 'cat list plot bar figsize',\n",
       " 'df x df x chalet shape',\n",
       " 'get ipython magic matplotlib inline import numpy import math import scipy import random import brewer mpl import matplotlib pyplot as plt',\n",
       " 'fig ax plt subplots ax plot numpy arange test lw label test ax plot numpy arange train lw label train ax legend loc ax set xlabel epoch ax set ylabel mse',\n",
       " 'from scipy import cluster from scipy cluster import hierarchy gene expression numpy genfromtxt yeastall public txt delimiter t skip header usecols range c hierarchy fclusterdata gene expression metric correlation',\n",
       " 'def yr sec a n convert years into seconds n return a',\n",
       " 'def m sec m n convert months into seconds n m per yr m return yr sec m per yr',\n",
       " 'def get fs t n returns sampling rate in hz from time period in s n return t',\n",
       " 'def get res bins fs get fs m sec n calculate the resolution time per bin in s n return fs bins',\n",
       " 'def bin frq bin peak bins tot return bin peak get res bins tot',\n",
       " 'def bin yr bin peak bins tot n calculates the corresponding year of a bin n if bin peak bin peak return bin peak get res bins tot yr sec',\n",
       " 'def yr bin yr ax yr n converts a year period of frequency into the corresponding bin n axis array with years is needed n bin ax np where ax yr yr closest bin np max bin ax return closest bin',\n",
       " 'def norm values data n returns normalized values n return data np max np abs data',\n",
       " 'data filtered mmlo mmlo filtered dt data index data index dt dt total seconds fs get fs t m sec t np arange dt fs',\n",
       " 'eq trend mmlo',\n",
       " 'future trend p trend mmlo t future dec p trend mmlo t future dec p trend mmlo future index pd date range data index future yr freq m synth future pd data frame index future index synth future trend future trend',\n",
       " 'peaks fft peaks merged all loc mmlo filtered yr mmlo filtered yr mmlo filtered yr mmlo filtered yr peaks drop bin bins total mag norm amp norm pha norm freq axis',\n",
       " 'peak f fft peaks merged all loc mmlo filtered yr peak f fft peaks merged all loc mmlo filtered yr peak f fft peaks merged all loc mmlo filtered yr peak f fft peaks merged all loc mmlo filtered yr peak f fft peaks merged all loc mmlo filtered yr',\n",
       " 'gain fac fac f fac f fac f fac f fac f',\n",
       " 'import gzip import numpy as np from sklearn cross validation import k fold from sklearn cross validation import stratified k fold from sklearn import svm import pandas as pd import matplotlib pyplot as plt import matplotlib image as imgplot import time from sklearn preprocessing import imputer from sklearn metrics import confusion matrix get ipython magic matplotlib inline',\n",
       " 'cells all pd read csv home dueo data genedata cells csv cells cells all iloc cell rows np shape cells',\n",
       " 'cells ix area shape area',\n",
       " 'x features np asmatrix cells ix area shape area np shape x features',\n",
       " 'imp imputer missing values na n strategy mean axis imp fit x features x features imp transform x features',\n",
       " 'xmean x features mean axis x std np sqrt x features var axis x x features xmean x std',\n",
       " 'np max x',\n",
       " 'np min x np max x np mean x',\n",
       " 'y np asarray y dtype int',\n",
       " 'split x train x split y train y split x test x split y test y split',\n",
       " 'y y idx dmso np asarray np recfromtxt dmso data csv print number of dmso format np sum idx dmso',\n",
       " 'perm np random permutation len y train x train perm x train perm y train perm y train perm',\n",
       " 'hist np histogram y train perm bins nmax np min hist nmax hist nmax',\n",
       " 'xx x train perm idx yy y train perm idx np shape xx np shape yy np shape x train perm np shape idx',\n",
       " 'np histogram yy bins',\n",
       " 'yy',\n",
       " 'cs res np zeros len cs for i c in enumerate cs res i eval fold c',\n",
       " 'plt semilogx cs res cs res',\n",
       " 'model svm svc probability true c fit xx yy',\n",
       " 'x test shape',\n",
       " 'pred model predict x test',\n",
       " 'model predict x test pred prob model predict proba x test',\n",
       " 'sum pred y test float len y test',\n",
       " 'from sklearn lda import lda clf lda clf fit x train perm y train perm',\n",
       " 'y pred lda clf predict x test',\n",
       " 'sum y pred lda y test float len y test',\n",
       " 'np histogram y pred lda bins',\n",
       " 'confusion matrix y pred lda y test',\n",
       " 'pred prob lda clf predict proba x test',\n",
       " 'from sklearn ensemble import random forest classifier',\n",
       " 'clf random forest classifier n jobs n estimators clf fit x train perm y train perm',\n",
       " 'y pred rf clf predict x test',\n",
       " 'sum y pred rf y test float len y test',\n",
       " 'get ipython magic load ext rpy ipython get ipython magic rpush pred prob get ipython magic rpush y test',\n",
       " 'get ipython run cell magic r save pred prob y test file test svm c single well nodmso rdata',\n",
       " 'm confusion matrix pred y test m',\n",
       " 'm astype float m sum axis np newaxis',\n",
       " 'names col paclitaxel true metoclopramide true digoxin true m confusion matrix pred y test df pd data frame m df columns names col names paclitaxel pred metoclopramide pred digoxin pred df index names df',\n",
       " 'cm normalized m astype float m sum axis np newaxis print normalized confusion matrix df pd data frame cm normalized df columns names col df index names df',\n",
       " 'np mean cm normalized np diag indices',\n",
       " 'np mean cm normalized np diag indices',\n",
       " 'import seaborn as sns sns set from future import division import matplotlib pyplot as plt import numpy as np get ipython run line magic matplotlib inline',\n",
       " 'from sklearn datasets samples generator import make blobs x var y var make blobs n samples centers cluster std random state np save a data x var np save a data classes y var plt scatter x var x var s plt show',\n",
       " 'x var np load a data npy y var np load a data classes npy x val x var y val x var plt scatter x val y val plt show',\n",
       " 'from sklearn preprocessing import scale x var scale x var x val x var y val x var plt scatter x val y val plt show',\n",
       " 'from sklearn svm import svc model svc kernel linear c model fit x var y var',\n",
       " 'plt rc params figure figsize line np linspace plt scatter x val y val c y var cmap prism s plt show',\n",
       " 'from theano sandbox import cuda cuda use gpu',\n",
       " 'get ipython magic matplotlib inline import utils reload utils from utils import from future import division print function',\n",
       " 'path get file nietzsche txt origin https s amazonaws com text datasets nietzsche txt text open path read print corpus length len text',\n",
       " 'chars sorted list set text vocab size len chars print total chars vocab size',\n",
       " 'chars insert x',\n",
       " 'join chars',\n",
       " 'char indices dict c i for i c in enumerate chars indices char dict i c for i c in enumerate chars',\n",
       " 'idx char indices c for c in text',\n",
       " 'idx',\n",
       " 'join indices char i for i in idx',\n",
       " 'x np stack c dat x np stack c dat x np stack c dat',\n",
       " 'y np stack c dat',\n",
       " 'x x x',\n",
       " 'y',\n",
       " 'x shape y shape',\n",
       " 'n fac',\n",
       " 'def embedding input name n in n out inp input shape dtype int name name emb embedding n in n out input length inp return inp flatten emb',\n",
       " 'c in c embedding input c vocab size n fac c in c embedding input c vocab size n fac c in c embedding input c vocab size n fac',\n",
       " 'n hidden',\n",
       " 'dense in dense n hidden activation relu',\n",
       " 'c hidden dense in c',\n",
       " 'dense hidden dense n hidden activation tanh',\n",
       " 'c dense dense in c hidden dense hidden c hidden c hidden merge c dense hidden',\n",
       " 'c dense dense in c hidden dense hidden c hidden c hidden merge c dense hidden',\n",
       " 'dense out dense vocab size activation softmax',\n",
       " 'c out dense out c hidden',\n",
       " 'model model c in c in c in c out',\n",
       " 'model compile loss sparse categorical crossentropy optimizer adam',\n",
       " 'model optimizer lr e',\n",
       " 'model fit x x x y batch size nb epoch',\n",
       " 'model optimizer lr',\n",
       " 'model fit x x x y batch size nb epoch',\n",
       " 'model optimizer lr set value e',\n",
       " 'model fit x x x y batch size nb epoch',\n",
       " 'model optimizer lr set value',\n",
       " 'model fit x x x y batch size nb epoch',\n",
       " 'def get next inp idxs char indices c for c in inp arrs np array i np newaxis for i in idxs p model predict arrs i np argmax p return chars i',\n",
       " 'get next phi',\n",
       " 'get next th',\n",
       " 'get next an',\n",
       " 'cs',\n",
       " 'c in dat idx i n for i in xrange len idx cs cs for n in range cs',\n",
       " 'c out dat idx i cs for i in xrange len idx cs cs',\n",
       " 'xs np stack c for c in c in dat',\n",
       " 'len xs xs shape',\n",
       " 'y np stack c out dat',\n",
       " 'xs n cs for n in range cs',\n",
       " 'y cs',\n",
       " 'n fac',\n",
       " 'def embedding input name n in n out inp input shape dtype int name name in emb embedding n in n out input length name name emb inp return inp flatten emb',\n",
       " 'c ins embedding input c str n vocab size n fac for n in range cs',\n",
       " 'n hidden',\n",
       " 'dense in dense n hidden activation relu dense hidden dense n hidden activation relu init identity dense out dense vocab size activation softmax',\n",
       " 'hidden dense in c ins',\n",
       " 'for i in range cs c dense dense in c ins i hidden dense hidden hidden hidden merge c dense hidden',\n",
       " 'c out dense out hidden',\n",
       " 'model model c for c in c ins c out model compile loss sparse categorical crossentropy optimizer adam',\n",
       " 'model fit xs y batch size nb epoch',\n",
       " 'def get next inp idxs np array char indices c np newaxis for c in inp p model predict idxs return chars np argmax p',\n",
       " 'get next for thos',\n",
       " 'get next part of',\n",
       " 'get next queens a',\n",
       " 'n hidden n fac cs vocab size',\n",
       " 'model sequential embedding vocab size n fac input length cs simple rnn n hidden activation relu inner init identity dense vocab size activation softmax',\n",
       " 'model summary',\n",
       " 'model compile loss sparse categorical crossentropy optimizer adam',\n",
       " 'model fit np concatenate xs axis y batch size nb epoch',\n",
       " 'def get next keras inp idxs char indices c for c in inp arrs np array idxs np newaxis p model predict arrs return chars np argmax p',\n",
       " 'get next keras this is',\n",
       " 'get next keras part of',\n",
       " 'get next keras queens a',\n",
       " 'c out dat idx i n for i in xrange len idx cs cs for n in range cs',\n",
       " 'ys np stack c for c in c out dat',\n",
       " 'xs n cs for n in range cs',\n",
       " 'ys n cs for n in range cs',\n",
       " 'dense in dense n hidden activation relu dense hidden dense n hidden activation relu init identity dense out dense vocab size activation softmax name output',\n",
       " 'inp input shape n fac name zeros hidden dense in inp',\n",
       " 'outs for i in range cs c dense dense in c ins i hidden dense hidden hidden hidden merge c dense hidden mode sum outs append dense out hidden',\n",
       " 'model model inp c for c in c ins outs model compile loss sparse categorical crossentropy optimizer adam',\n",
       " 'zeros np tile np zeros n fac len xs zeros shape',\n",
       " 'model fit zeros xs ys batch size nb epoch',\n",
       " 'get nexts this is',\n",
       " 'get nexts part of',\n",
       " 'n hidden n fac cs vocab size',\n",
       " 'model sequential embedding vocab size n fac input length cs simple rnn n hidden return sequences true activation relu inner init identity time distributed dense vocab size activation softmax',\n",
       " 'model summary',\n",
       " 'model compile loss sparse categorical crossentropy optimizer adam',\n",
       " 'xs shape',\n",
       " 'x rnn np stack xs axis y rnn np expand dims np stack ys axis',\n",
       " 'x rnn shape y rnn shape',\n",
       " 'model fit x rnn y rnn batch size nb epoch',\n",
       " 'def get nexts keras inp idxs char indices c for c in inp arr np array idxs np newaxis p model predict arr print list inp return chars np argmax o for o in p',\n",
       " 'get nexts keras this is',\n",
       " 'model sequential simple rnn n hidden return sequences true input shape cs vocab size activation relu inner init identity time distributed dense vocab size activation softmax model compile loss categorical crossentropy optimizer adam',\n",
       " 'oh ys to categorical o vocab size for o in ys oh y rnn np stack oh ys axis oh xs to categorical o vocab size for o in xs oh x rnn np stack oh xs axis oh x rnn shape oh y rnn shape',\n",
       " 'model fit oh x rnn oh y rnn batch size nb epoch',\n",
       " 'def get nexts oh inp idxs np array char indices c for c in inp arr to categorical idxs vocab size p model predict arr np newaxis print list inp return chars np argmax o for o in p',\n",
       " 'get nexts oh this is',\n",
       " 'bs',\n",
       " 'model sequential embedding vocab size n fac input length cs batch input shape bs batch normalization lstm n hidden return sequences true stateful true time distributed dense vocab size activation softmax',\n",
       " 'model compile loss sparse categorical crossentropy optimizer adam',\n",
       " 'mx len x rnn bs bs',\n",
       " 'model fit x rnn mx y rnn mx batch size bs nb epoch shuffle false',\n",
       " 'model optimizer lr',\n",
       " 'model fit x rnn mx y rnn mx batch size bs nb epoch shuffle false',\n",
       " 'model fit x rnn mx y rnn mx batch size bs nb epoch shuffle false',\n",
       " 'n input vocab size n output vocab size',\n",
       " 'def init wgts rows cols scale math sqrt rows return shared normal scale scale size rows cols astype np float def init bias rows return shared np zeros rows dtype np float',\n",
       " 'def wgts and bias n in n out return init wgts n in n out init bias n out def id and bias n return shared np eye n dtype np float init bias n',\n",
       " 't inp t matrix inp t outp t matrix outp t h t vector h lr t scalar lr all args t h t inp t outp lr',\n",
       " 'w h id and bias n hidden w x wgts and bias n input n hidden w y wgts and bias n hidden n output w all list chain from iterable w h w x w y',\n",
       " 'def step x h w h b h w x b x w y b y h nnet relu t dot x w x b x t dot h w h b h y nnet softmax t dot h w y b y return h t flatten y',\n",
       " 'v h v y theano scan step sequences t inp outputs info t h none non sequences w all',\n",
       " 'error nnet categorical crossentropy v y t outp sum g all t grad error w all',\n",
       " 'def upd dict wgts grads lr return ordered dict w w g lr for w g in zip wgts grads upd upd dict w all g all lr',\n",
       " 'fn theano function all args error updates upd allow input downcast true',\n",
       " 'x oh x rnn y oh y rnn x shape y shape',\n",
       " 'err l rate for i in range len x err fn np zeros n hidden x i y i l rate if i print error f format err err',\n",
       " 'f y theano function t h t inp v y allow input downcast true',\n",
       " 'pred np argmax f y np zeros n hidden x axis',\n",
       " 'act np argmax x axis',\n",
       " 'indices char o for o in act',\n",
       " 'indices char o for o in pred',\n",
       " 'def sigmoid x return np exp x def sigmoid d x output sigmoid x return output output',\n",
       " 'def relu x return np maximum x def relu d x return x',\n",
       " 'relu np array relu d np array',\n",
       " 'def dist a b return pow a b def dist d a b return a b',\n",
       " 'import pdb',\n",
       " 'eps e def x entropy pred actual return np sum actual np log np clip pred eps eps def x entropy d pred actual return actual pred',\n",
       " 'def softmax x return np exp x np exp x sum',\n",
       " 'def softmax d x sm softmax x res np expand dims sm sm res np diag indices from res sm sm return res',\n",
       " 'test preds np array test actuals np array nnet categorical crossentropy test preds test actuals eval',\n",
       " 'x entropy test preds test actuals',\n",
       " 'test inp t dvector test out nnet categorical crossentropy test inp test actuals test grad theano function test inp t grad test out test inp',\n",
       " 'test grad test preds',\n",
       " 'x entropy d test preds test actuals',\n",
       " 'pre pred random oh x rnn shape preds softmax pre pred actual oh x rnn',\n",
       " 'np allclose softmax d pre pred dot loss d preds actual preds actual',\n",
       " 'softmax test preds',\n",
       " 'nnet softmax test preds eval',\n",
       " 'test out t flatten nnet softmax test inp',\n",
       " 'test grad theano function test inp theano gradient jacobian test out test inp',\n",
       " 'test grad test preds',\n",
       " 'softmax d test preds',\n",
       " 'act relu act d relu d',\n",
       " 'loss x entropy loss d x entropy d',\n",
       " 'def scan fn start seq res prev start for s in seq app fn prev s res append app prev app return res',\n",
       " 'scan lambda prev curr prev curr range',\n",
       " 'inp oh x rnn outp oh y rnn n input vocab size n output vocab size',\n",
       " 'inp shape outp shape',\n",
       " 'def get chars n return zip inp n outp n def one fwd n return scan one char np zeros n hidden get chars n',\n",
       " 'scale math sqrt n input w x normal scale scale size n input n hidden w y normal scale scale size n hidden n output w h np eye n hidden dtype np float',\n",
       " 'overall error alpha for n in range res one fwd n overall error res deriv one bkwd res n if n print error f gradient f format overall error np linalg norm deriv overall error',\n",
       " 'model sequential gru n hidden return sequences true input shape cs vocab size activation relu inner init identity time distributed dense vocab size activation softmax model compile loss categorical crossentropy optimizer adam',\n",
       " 'model fit oh x rnn oh y rnn batch size nb epoch',\n",
       " 'get nexts oh this is',\n",
       " 'w h id and bias n hidden w x init wgts n input n hidden w y wgts and bias n hidden n output r w h init wgts n hidden n hidden r w x wgts and bias n input n hidden u w h init wgts n hidden n hidden u w x wgts and bias n input n hidden w all list chain from iterable w h w y u w x r w x w all extend w x u w h r w h',\n",
       " 'def gate x h w h w x b x return nnet sigmoid t dot x w x b x t dot h w h',\n",
       " 'v h v y theano scan step sequences t inp outputs info t h none non sequences w all',\n",
       " 'error nnet categorical crossentropy v y t outp sum g all t grad error w all',\n",
       " 'upd upd dict w all g all lr fn theano function all args error updates upd allow input downcast true',\n",
       " 'err l rate for i in range len x err fn np zeros n hidden x i y i l rate if i l rate print error f format err err',\n",
       " 'def gate m w b return nnet sigmoid t dot m w b',\n",
       " 'v h v y theano scan step sequences t inp outputs info t h none non sequences w all',\n",
       " 'def upd dict wgts grads lr return ordered dict w w g lr for w g in zip wgts grads',\n",
       " 'error nnet categorical crossentropy v y t outp sum g all t grad error w all',\n",
       " 'upd upd dict w all g all lr fn theano function all args error updates upd allow input downcast true',\n",
       " 'err l rate for i in range len x err fn np zeros n hidden x i y i l rate if i print error f format err err',\n",
       " 'import pandas as pd from scipy cluster hierarchy import dendrogram linkage from geopy distance import vincenty from scipy import spatial import networkx as nx import matplotlib pyplot as plt from functions import import geopandas as gpd from shapely geometry import point from geojson import polygon from scipy spatial import convex hull import json import parameters as p',\n",
       " 'data all pd read csv gis data building all null csv',\n",
       " 'in builing piping sf',\n",
       " 'data all lat lon data all apply lambda row row y lat row x lon axis',\n",
       " 'if len data all k else k len data all',\n",
       " 'x lat lon list data all lat lon low medium high query point medium tree spatial kd tree x lat lon dist index select tree query query point k k',\n",
       " 'data pd data frame for i in index select select row data all iloc i data data append select row',\n",
       " 'x lat lon select list data lat lon',\n",
       " 'data data reset index',\n",
       " 'index data data y lat query point index tolist',\n",
       " 'len cluster points',\n",
       " 'len points checked',\n",
       " 'print conveyance energy print treatment energy print treatment embodied print infrastructure',\n",
       " 'totals',\n",
       " 'log for item in log energy log append item',\n",
       " 'plt plot log plt show',\n",
       " 'ids not list set log set cluster points',\n",
       " 'output pd data frame for i in cluster points select row data iloc i output output append select row',\n",
       " 'output to csv med out test csv',\n",
       " 'lat lon array list output lat lon hull convex hull lat lon array polygon coords for simplex in hull vertices x lat lon array simplex y lat lon array simplex coords x y polygon coords append coords polygon coords array polygon coords polygon polygon polygon coords array',\n",
       " 'with open gis data polygon s geojson medium w as outfile json dump polygon outfile',\n",
       " 'data column sum pop elev treat num floor area m lat lon data all pd data frame data data columns column',\n",
       " 'data all',\n",
       " 'query point',\n",
       " 'z linkage list data all lat lon ward',\n",
       " 'clusters hierarchical cluster list data all lat lon z',\n",
       " 'clusters',\n",
       " 'find treatment energy data all sum pop',\n",
       " 'pump energy building data all floors',\n",
       " 'tot energy find treatment energy data all sum pop data all sum pop find conveyance energy data all elev data all elev data all floors pump energy building data all floors',\n",
       " 'tot energy find treatment energy data all sum pop data all sum pop find conveyance energy data all elev data all elev data all floors pump energy building data all floors',\n",
       " 'import pandas as pd import numpy as np from numpy import sqrt abs from itertools import combinations import matplotlib pyplot as plt get ipython magic matplotlib inline get ipython magic load ext autoreload get ipython magic autoreload',\n",
       " 'from scipy io import loadmat import six',\n",
       " 'infile data mat data mat to data infile',\n",
       " 'from scipy import signal freq powspec signal periodogram data data t',\n",
       " 'freq shape',\n",
       " 'plt plot freq powspec t plt xlim freq pass',\n",
       " 'plt plot data data',\n",
       " 'import pandas as pd',\n",
       " 'import seaborn as sns import matplotlib pyplot as plt',\n",
       " 'from sklearn import linear model',\n",
       " 'get ipython run line magic matplotlib inline import pandas as pd import numpy as np import matplotlib pyplot as plt import bokeh plotting as bkp from mpl toolkits axes grid import make axes locatable from scipy import stats',\n",
       " 'hospital read df pd read csv data cms hospital readmissions csv',\n",
       " 'hospital read df head',\n",
       " 'clean hospital read df hospital read df hospital read df number of discharges not available clean hospital read df loc number of discharges clean hospital read df number of discharges astype int clean hospital read df clean hospital read df sort values number of discharges',\n",
       " 'clean hospital read df iloc',\n",
       " 'clean hospital read df describe',\n",
       " 'def tstat xbar mean std n return xbar mean std n',\n",
       " 'def conf int xbar mean std n t stat tstat xbar mean std n lower mean np abs t stat std upper mean np abs t stat std return lower upper',\n",
       " 'def get pval xbar mean std n tt tstat xbar mean std n pval stats t sf np abs tt n print t statistic f pvalue f tt pval',\n",
       " 'sub readmit clean hospital read df clean hospital read df number of discharges clean hospital read df number of discharges',\n",
       " 'sub readmit describe',\n",
       " 'xbar sub readmit excess readmission ratio mean mean std sub readmit excess readmission ratio std n sub sub readmit excess readmission ratio count',\n",
       " 'get pval xbar mean std n sub',\n",
       " 'conf int xbar mean std n sub',\n",
       " 'over readmit clean hospital read df clean hospital read df number of discharges',\n",
       " 'over readmit describe',\n",
       " 'xbar over readmit excess readmission ratio mean mean std over readmit excess readmission ratio std n over over readmit excess readmission ratio count',\n",
       " 'get pval xbar mean std n over',\n",
       " 'conf int xbar mean std n over',\n",
       " 'print n sub str n sub print n over str n over print total n str len clean hospital read df print n str',\n",
       " 'excess readmit hosp clean hospital read df clean hospital read df excess readmission ratio',\n",
       " 'excess readmit hosp describe',\n",
       " 'bins range group names range excess readmit hosp discharge group pd cut excess readmit hosp number of discharges bins labels group names',\n",
       " 'excess readmit hosp count',\n",
       " 'plotdf excess readmit hosp discharge group value counts reset index plotdf plotdf sort values index',\n",
       " 'plotdf columns discharges excess readmit count',\n",
       " 'plotdf discharges plotdf discharges astype int',\n",
       " 'plotdf',\n",
       " 'plotdf plot kind scatter x discharges y excess readmit count figsize',\n",
       " 'import numpy as np import matplotlib pyplot as plt get ipython magic matplotlib inline from sklearn import svm import random',\n",
       " 'clf svm svc kernel linear clf fit x y',\n",
       " 'w clf coef print w',\n",
       " 'import numpy as np import numpy random as rnd import matplotlib pyplot as plt',\n",
       " 'nunit defs n ms m v',\n",
       " 'plt plot t my input k plt xlim start t total t plt show',\n",
       " 'x val x for x in spikes y val x for x in spikes plt plot x val y val k plt xlim start t total t plt ylim n plt show',\n",
       " 'for i in range nstim n plt plot t voltage i m v plt plot t currents i m v plt xlim start t total t plt show',\n",
       " 'get ipython run line magic matplotlib notebook',\n",
       " 'import numpy as np import matplotlib pyplot as plt from scipy import linalg from sklearn decomposition import pca factor analysis from sklearn covariance import shrunk covariance ledoit wolf from sklearn model selection import cross val score from sklearn model selection import grid search cv print doc',\n",
       " 'ret xa range for comp in xa fa factor analysis fa n components comp ret append np mean cross val score fa x hetero plt figure plt plot xa ret plt show',\n",
       " 'import pandas as pd from sklearn datasets import fetch newsgroups from sklearn feature extraction text import count vectorizer import numpy as np from sklearn preprocessing import standard scaler from sklearn decomposition import pca truncated svd from sklearn cluster import k means from sklearn import metrics import matplotlib pyplot as plt',\n",
       " 'weather frame pd read pickle c git twitter project weather data pki',\n",
       " 'tweets weathertweets f tweets shape',\n",
       " 'weather frame head',\n",
       " 'glasgow weather frame ix weather frame city glasgow date time period w observed avg temp obs glasgownp np array glasgow',\n",
       " 'g glasgow groupby date time period min',\n",
       " 'g',\n",
       " 'tweets head',\n",
       " 'a tweets sort dayofmonth hour',\n",
       " 'import datetime as dt g',\n",
       " 'x y tweets shape',\n",
       " 'g head',\n",
       " 'tweets head',\n",
       " 'complete pd merge g tweets left index true right on date time period how outer',\n",
       " 'complete head',\n",
       " 'complete sort date time period',\n",
       " 'completenp np array complete',\n",
       " 'trace scatter x completenp y completenp mode markers name temp vs count of cold tweets marker dict color green symbol x opacity fig dict data trace iplot fig',\n",
       " 'import pandas as pd import numpy as np from matplotlib import pyplot as plt get ipython run line magic matplotlib inline',\n",
       " 'fig axes plt subplots nrows ncols figsize for idx feature in enumerate df columns df plot feature cnt subplots true kind scatter ax axes idx idx',\n",
       " 'from sklearn preprocessing import scale',\n",
       " 'x scale df df columns y df cnt',\n",
       " 'from sklearn linear model import linear regression',\n",
       " 'from sklearn linear model import lasso ridge',\n",
       " 'alphas np arange coefs lasso np zeros alphas shape x shape coefs ridge np zeros alphas shape x shape',\n",
       " 'from sklearn linear model import lasso cv',\n",
       " 'alphas np arange',\n",
       " 'get ipython run line magic matplotlib inline',\n",
       " 'import matplotlib pyplot as plt import numpy as np from scipy import stats from scipy interpolate import interp d from astropy io import fits from astropy io import ascii',\n",
       " 'import pdb from pyigm cgm cos halos import cos halos from pyigm cgm import cos halos as pch reload pch cos halos pch cos halos cos halos load mega',\n",
       " 'ions cos halos ion tbl si ii si i itable ions si ii fill ion true',\n",
       " 'si i itable colnames',\n",
       " 'gg np where si i itable flag n si i itable gg name z log n sig log n flag n cgm name',\n",
       " 'si ii tbl cos halos ion tbl si iii tbl cos halos ion tbl ion tbls si ii tbl si iii tbl',\n",
       " 'ion tbls',\n",
       " 'nhi cos halos nhi flag nhi cos halos flag nhi',\n",
       " 'j absorber cos halos cgm abs j igm sys',\n",
       " 'xxx absorber list of abslines k yyy xxx k yyy',\n",
       " 'lines absorber list of abslines trans np array iline name for iline in lines aline trans si iii print trans aline aline sum aline np where trans si iii print trans aline aline',\n",
       " 'ch name ch z ch rho ch rvir ch halomass ch stellarmass ch nhi ch sig nhi ch flag nhi ch n si iii ch sig n si iii ch flag n si iii',\n",
       " 'for j in np arange print ch name j np round ch n si iii j ch sig n si iii j ch flag n si iii j',\n",
       " 'measure np array ch flag n si iii measure',\n",
       " 'from janome tokenizer import tokenizer t tokenizer malist t tokenize for n in malist print n',\n",
       " 'from janome tokenizer import tokenizer import zipfile import os path urllib request as req',\n",
       " 'from janome tokenizer import tokenizer from gensim models import word vec import zipfile import os path urllib request as req import re',\n",
       " 'from gensim models import word vec model word vec word vec load kokoro model model most similar positive',\n",
       " 'model most similar positive',\n",
       " 'import os re from janome tokenizer import tokenizer',\n",
       " 'from gensim models import word vec persons for person in persons print person data word vec line sentence text person wakati model word vec word vec data size window hs min count sg model save text person model print ok',\n",
       " 'from gensim models import word vec model word vec word vec load text model model most similar positive',\n",
       " 'model most similar positive negative',\n",
       " 'model most similar positive negative',\n",
       " 'model',\n",
       " 'from gensim models import word vec data word vec text corpus wiki wakati text model word vec word vec data size medel save wiki model print ok',\n",
       " 'import math sys from janome tokenizer import tokenizer',\n",
       " 'from janome tokenizer import tokenizer import os glob',\n",
       " 'import os glob json',\n",
       " 'from keras models import sequential from keras layers import dense dropout activation from keras wrappers scikit learn import keras classifier from keras utils import np utils from sklearn cross validation import train test split from sklearn import cross validation metrics import json',\n",
       " 'from janome tokenizer import tokenizer import os re json random',\n",
       " 'from keras models import sequential from keras layers import dense activation dropout from keras layers import lstm from keras optimizers import rm sprop from keras utils data utils import get file import numpy as np import random sys',\n",
       " 'list file tables hmsc list txt out dir eps figs multi plot sour list ascii read list file atlasgal dir fits dir atlasgal higal dir fits dir hi gal irac dir fits dir irac mips dir fits dir mips sed dir fits dir sed fitting sed dir',\n",
       " 'import os print os listdir data',\n",
       " 'import keras from keras preprocessing import sequence from keras models import sequential load model model from keras layers import lstm dense dropout activation embedding time distributed merge input concatenate import word vec utils as w v import data import numpy as np from data utils import split dataset from chat constants import',\n",
       " 'w v model w v initialize',\n",
       " 'assert len w v model vocab',\n",
       " 'a input input shape max sent length embed dim a layer lstm embed dim return sequences true name layer a dropout lstm dropout recurrent dropout lstm dropout a pretrain model inputs a input outputs a layer a input a pretrain load weights amerge b len dim w v in and out h by name true a pretrain compile optimizer rmsprop loss cosine proximity metrics accuracy',\n",
       " 'for i in range a b a sets i a train b train a train w v get training data a b a a pretrain load weights a pretrain h by name true a pretrain fit a train a train batch size epochs a pretrain save a pretrain h',\n",
       " 'b input input shape max sent length embed dim b layer lstm embed dim return sequences true name layer b dropout lstm dropout recurrent dropout lstm dropout b pretrain model inputs b input outputs b layer b input b pretrain compile optimizer rmsprop loss cosine proximity metrics accuracy',\n",
       " 'for i in range a b a sets i a train b train a train w v get training data a b a a pretrain load weights a pretrain h by name true a pretrain fit b train a train batch size epochs a pretrain save a pretrain h',\n",
       " 'chat model load weights amerge b len dim w v in and out h by name true',\n",
       " 'chat model compile optimizer rmsprop loss cosine proximity metrics accuracy a train b train a train w v get training data a b a chat model fit a train b train a train batch size epochs validation split chat model save amerge b len dim w v in and out h',\n",
       " 'a b a sets',\n",
       " 'target drugs navitoclax nutlin ag pd plx sb d na summary fill summary imp summary prepared get modeling data maximal impute response false',\n",
       " 'd features filter responses d target drugs d filter regex res info',\n",
       " 'mask d filter regex res isnull apply np any axis n len d d d mask values print removing records of due to missing response format n len d n d filter regex res info',\n",
       " 'responses d filter regex res columns tolist x y d drop responses axis d responses',\n",
       " 'd filter regex cl std describe',\n",
       " 'data save modeling cosmic multi task models cv res refit res',\n",
       " 'x shape',\n",
       " 'data save modeling cosmic multi task feat imp feat imp',\n",
       " 'feat imp head',\n",
       " 'd pred ml model summarize predictions cv res y names y columns tolist d pred info',\n",
       " 'data save modeling cosmic multi task pred d pp',\n",
       " 'g sns facet grid d pp row drug name col model name hue fold id margin titles true sharex false sharey false g map plt scatter y true y pred',\n",
       " 'from sklearn metrics import r score def get score g return g y true corr g y pred d perf d pp groupby model name drug name fold id apply get score rename score reset index d perf head',\n",
       " 'data save modeling cosmic multi task perf d perf',\n",
       " 'plt figure figsize sns facet grid d perf col drug name size aspect sharey false col wrap map sns boxplot model name score',\n",
       " 'import pandas as pd from pandas tools plotting import scatter matrix from pandas tools plotting import andrews curves import numpy as np import matplotlib pyplot as plt from sklearn import datasets get ipython magic matplotlib inline',\n",
       " 'df pd data frame datasets load iris data',\n",
       " 'df head',\n",
       " 'df columns sepal l sepal w petal l petal w df tail',\n",
       " 'names col datasets load iris target names x for x in datasets load iris target',\n",
       " 'df species names col',\n",
       " 'df species datasets load iris target df loc df species species setosa df loc df species species versicolor df loc df species species virginica',\n",
       " 'df head',\n",
       " 'df sepal l head',\n",
       " 'df sepal l head',\n",
       " 'df sepal l',\n",
       " 'df iloc',\n",
       " 'df loc sepal l',\n",
       " 'df',\n",
       " 'df iloc',\n",
       " 'df describe',\n",
       " 'df sort values by sepal l ascending true head',\n",
       " 'df df sepal l',\n",
       " 'len df df sepal l',\n",
       " 'df nan df copy deep true df nan df nan sepal l np na n df nan df nan dropna how any len df nan',\n",
       " 'df mean df copy deep true df mean df mean sepal l np na n len df mean fillna df mean mean',\n",
       " 'scatter matrix df figsize diagonal kde',\n",
       " 'df boxplot',\n",
       " 'df hist alpha',\n",
       " 'df groupby species petal l hist alpha',\n",
       " 'df groupby species hist alpha',\n",
       " 'plt figure andrews curves df species plt legend loc',\n",
       " 'import numpy as np from matplotlib mlab import griddata import scipy as sp import scipy interpolate from mpl toolkits mplot d axes d import import matplotlib pyplot as plt from matplotlib import cm get ipython magic matplotlib inline',\n",
       " 'dem file cpcrw dem dat',\n",
       " 'data np loadtxt dem file unpack false x data y data z data',\n",
       " 'fig plt figure ax axes d fig ax scatter d x y z c z cmap plt cm jet plt show',\n",
       " 'import numpy as np import pandas import scipy scipy spatial import sklearn import sys from matplotlib import pyplot as plt get ipython magic matplotlib inline',\n",
       " 'y pandas read table downloads data ml label train txt sep dtype int header none ndim y head',\n",
       " 'ymin ysplit ymax',\n",
       " 'np unique y return counts true',\n",
       " 'import pickle cstat pickle load open data sum features dat rb',\n",
       " 'gf test goodfeatures xsub ysub read random sample home vahid downloads data ml data train txt y size goodfeat gf test acc miny ymin acc maxy ymax print xsub shape print np unique ysub',\n",
       " 'ntot train y shape print ntot train df pandas read table data data tr lower txt usecols feat idx nrows ntot train header none sep xcv pandas read table data data cv lower txt usecols feat idx nrows ntot train header none sep print df shape print xcv shape',\n",
       " 'tr idx np random choice df shape ntot train replace false ts idx np setdiff d np arange df shape tr idx assume unique true xtr df iloc tr idx print xtr shape tr idx shape ts idx shape',\n",
       " 'kdt scipy spatial kd tree xtr iloc tr idx shape leafsize qt dist qt idx kdt query xtr iloc k print qt dist print qt idx',\n",
       " 'ntr xtr shape nsplit ntr kdt scipy spatial kd tree xtr iloc nsplit leafsize qt idx kdt query xcv k',\n",
       " 'engine db create root engine rawtable pd io sql read sql table listings engine index col id xtr xte ytr yte get training test set rawtable make features make features categorize rating categorize rating',\n",
       " 'gridclf grid scores',\n",
       " 'engine db create root engine rawtable pd io sql read sql table listings engine index col id xtr xte ytr yte get training test set rawtable make features make features categorize rating categorize rating',\n",
       " 'xtr head',\n",
       " 'gridclf grid scores',\n",
       " 'gridclf grid scores',\n",
       " 'engine db create root engine rawtable pd io sql read sql table listings engine index col id xtr xte ytr yte get training test set rawtable make features make features categorize rating categorize rating',\n",
       " 'gridclf grid scores',\n",
       " 'gridclf best estimator',\n",
       " 'class weighted accuracy score gridclf xte yte',\n",
       " 'clf get random forest clf',\n",
       " 'clf fit xtr ytr',\n",
       " 'class weighted accuracy score clf xte yte',\n",
       " 'import pickle',\n",
       " 'pickle dump clf open pipe pkl wb',\n",
       " 'clf dummy get dummy clf',\n",
       " 'plot learning curve clf learning curve xtr ytr dummy clf dummy scoring class weighted accuracy score ylim n jobs plot train false',\n",
       " 'sklearn metrics confusion matrix yte clf predict xte labels clf classes t',\n",
       " 'clf classes',\n",
       " '',\n",
       " 'yte value counts',\n",
       " 'plt hist np max clf predict proba xte axis',\n",
       " 'clf',\n",
       " 'calibrated clf skl calibration calibrated classifier cv clf method isotonic calibrated clf fit xtr ytr',\n",
       " 'plt hist np max calibrated clf predict proba xte axis',\n",
       " 'five ind calibrated clf predict xte xte iloc five ind',\n",
       " 'five ind calibrated clf predict xte five ex xte iloc five ind np sum np max calibrated clf predict proba five ex axis len five ex',\n",
       " 'np sum np max calibrated clf predict proba xte axis len xte',\n",
       " 'class weighted accuracy score calibrated clf xte yte',\n",
       " 'pickle dump calibrated clf open pipe pkl wb',\n",
       " 'xte index np logical and calibrated clf predict xte np max calibrated clf predict proba xte axis',\n",
       " 'xte index',\n",
       " 'import numpy as np import matplotlib pyplot as plt from scipy optimize import curve fit brentq get ipython magic matplotlib inline',\n",
       " 'e np loadtxt vmc h e dat e all np loadtxt vmc h e all dat n steps eq steps n s minimization steps np loadtxt vmc h data int dat s min s max np loadtxt vmc h data float dat s vector np loadtxt vmc h s dat',\n",
       " 'avg np min e all axis var np var e axis x np linspace s min s max n s y min np min avg y max np max avg len avg s min s vector np argmin avg d y min y max',\n",
       " 'plt plot s vector avg p plt show',\n",
       " 'from scipy import constants har constants physical constants hartree energy in e v print d e constants h constants c constants e har np sqrt pcov diagonal constants h constants c constants e har',\n",
       " 'tweets reg users for tweet in tweets if tweet regular yes tweets reg users append tweet print len tweets reg users',\n",
       " 'import json with open fold keywords txt w as outfile json dump dataset outfile',\n",
       " 'get ipython run line magic pylab inline import pandas as pd import datetime import matplotlib pyplot as plt from sklearn import svm from sklearn import cross validation from sklearn import preprocessing from sklearn import grid search import math',\n",
       " 'train data pd read csv data train csv parse dates test data pd read csv data test csv parse dates',\n",
       " 'kfold indexes cross validation k fold len train data shuffle true',\n",
       " 'set norm e norm for e in set feature sets set train data',\n",
       " 'feature set feature sets model svm svr cache size c kernel rbf model fit train data feature set train data count predictions model predict test data feature set predictions int round max e for e in predictions',\n",
       " 'predictions int round max e for e in predictions tdf pd data frame data predictions index test data datetime columns count tdf to csv data svr results csv',\n",
       " 'vocab processor learn preprocessing vocabulary processor max document length x train np array list vocab processor fit transform x train x test np array list vocab processor transform x test n words len vocab processor vocabulary print total words d n words',\n",
       " 'model fn rnn model classifier learn estimator model fn model fn classifier fit x train y train steps y predicted p class for p in classifier predict x test as iterable true score metrics accuracy score y test y predicted print accuracy f format score',\n",
       " 'from i python display import image url http upload wikimedia org wikipedia commons kosaciec szczecinkowaty iris setosa jpg image url width height',\n",
       " 'from i python display import image url http upload wikimedia org wikipedia commons iris versicolor jpg image url width height',\n",
       " 'from i python display import image url http upload wikimedia org wikipedia commons f iris virginica jpg image url width height',\n",
       " 'df pd read csv f cc new coders survey data update csv df head',\n",
       " 'df describe',\n",
       " 'df isnull sum',\n",
       " 'most common ages df groupby by age size sort values ascending false head',\n",
       " 'print most common ages',\n",
       " 'data pd read csv f cc new coders survey data update csv',\n",
       " 'g sns factorplot age data data aspect kind count g set xticklabels rotation g plt title distribution of new programmers over different ages',\n",
       " 'most common gender df groupby by gender size sort values ascending false head',\n",
       " 'print most common gender',\n",
       " 'most common bootcamp df groupby by bootcamp name size sort values ascending false head',\n",
       " 'print most common bootcamp',\n",
       " 'ax sns countplot data df x employment field hue is under employed ax set title underemployed ax set xlabel bootcamp name ax set ylabel number of coders',\n",
       " 'values data podcast change log podcast code newbie podcast code pen podcast dev tea podcast dot net podcast giant robots podcast js air podcast none podcast other podcast prog throwdown podcast ruby rogues podcast se daily podcast se radio podcast shop talk podcast talk python podcast the web ahead count bar values sort values ascending false plot bar g plt title podcasts',\n",
       " 'values data resource codecademy resource code wars resource coursera resource css resource ed x resource egghead resource hacker rank resource ka resource lynda resource mdn resource odin proj resource other resource plural sight resource skillcrush resource so resource treehouse resource udacity resource udemy resource w s count bar values sort values ascending false plot bar g plt title resources websites',\n",
       " 'values data you tube code course you tube coding train you tube coding tut you tube computerphile you tube derek banas you tube dev tips you tube engineered truth you tube fun fun function you tube google dev you tube learn code you tube level up tuts you tube mit you tube mozilla hacks you tube other you tube simplilearn you tube the new boston count bar values sort values ascending false plot bar g plt title you tube videos',\n",
       " 'labels resource you tube code event podcast sizes colors gold lightgreen lightcoral lightblue explode plt pie sizes explode explode labels labels colors colors autopct f shadow true startangle plt axis equal plt show',\n",
       " 'import matplotlib pyplot as plt import numpy as np from sklearn import datasets import tensorflow as tf from tensorflow python framework import ops ops reset default graph',\n",
       " 'iris datasets load iris binary target np array if x else for x in iris target iris d np array x x for x in iris data',\n",
       " 'batch size',\n",
       " 'sess tf session',\n",
       " 'x data tf placeholder shape none dtype tf float x data tf placeholder shape none dtype tf float y target tf placeholder shape none dtype tf float',\n",
       " 'a tf variable tf random normal shape b tf variable tf random normal shape',\n",
       " 'my mult tf matmul x data a my add tf add my mult b my output tf sub x data my add',\n",
       " 'xentropy tf nn sigmoid cross entropy with logits my output y target',\n",
       " 'my opt tf train gradient descent optimizer train step my opt minimize xentropy init tf global variables initializer sess run init',\n",
       " 'from sklearn import datasets from sklearn svm import svc from sklearn naive bayes import gaussian nb',\n",
       " 'digits datasets load digits x train digits data y train digits target x test digits data y test digits target',\n",
       " 'clf svc clf fit x train y train print clf predict x test print y test',\n",
       " 'clf gaussian nb clf fit x train y train print clf predict x test print y test',\n",
       " 'from sklearn import datasets metrics from scipy spatial distance import cosine euclidean minkowski import pandas as pd from random import shuffle import warnings import numpy as np warnings simplefilter ignore',\n",
       " 'from sklearn metrics import accuracy score',\n",
       " 'clf k nn classifier euclidean clf fit x train y train print k accuracy on train data accuracy score clf predict x train y train accuracy on test data accuracy score clf predict x test y test',\n",
       " 'clf k nn classifier euclidean clf fit x train y train print k accuracy on train data accuracy score clf predict x train y train accuracy on test data accuracy score clf predict x test y test',\n",
       " 'clf k nn classifier euclidean clf fit x train y train print k accuracy on train data accuracy score clf predict x train y train accuracy on test data accuracy score clf predict x test y test',\n",
       " 'from sklearn neighbors import k neighbors classifier from sklearn cross validation import cross val score',\n",
       " 'l subtrain train ix indices l x train np asarray subtrain list range train shape y train np asarray subtrain ravel',\n",
       " 'k int l int l l l l l l estimator k neighbors classifier algorithm ball tree',\n",
       " 'import gensim model gensim models word vec load word vec format wiki en text vector binary false',\n",
       " 'import re time import pandas as pd import numpy as np from scipy ndimage interpolation import shift import matplotlib pyplot as plt from scipy sparse import csr matrix hstack from sklearn model selection import train test split from datetime import datetime from matplotlib import pyplot as plt from sklearn feature extraction text import count vectorizer tfidf vectorizer from sklearn linear model import logistic regression from sklearn import linear model from sklearn neural network import mlp classifier from sklearn svm import nu svc svc from sklearn ensemble import ada boost classifier gradient boosting regressor from sklearn model selection import cross val predict cross val score stratified k fold from sklearn metrics import confusion matrix classification report mean squared error from sklearn import preprocessing from keras models import sequential from keras import optimizers from keras layers import dense activation dropout from keras layers import conv d global average pooling d max pooling d from keras utils import np utils',\n",
       " 'data pd read csv newstitle gbp xau csv data head',\n",
       " 'vectorizer tfidf vectorizer min df ngram range x vectorizer fit transform data title tolist x',\n",
       " 'gbp data as matrix columns gbp reshape xau data as matrix columns xau reshape',\n",
       " 'gbp preprocessing normalize gbp xau preprocessing normalize xau',\n",
       " 'gbp np gradient gbp reshape xau np gradient xau reshape gbp np gradient gbp reshape xau np gradient xau reshape',\n",
       " 'd true if d true gbp np where gbp xau np where xau y np where gbp xau print np bincount y gbp shape',\n",
       " 'num classes y np utils to categorical y num classes y',\n",
       " 'def train and evaluate model model x train y train x test y test history model fit x train y train epochs batch size verbose score model evaluate x test y test batch size time sleep print valid acc f score return score',\n",
       " 'x np array np any np all x axis',\n",
       " 'x np array y np zeros x shape y x print y',\n",
       " 'from numpy lib stride tricks import as strided x np array np random randint size dtype np int print x print unequal values print x np all x t x t axis',\n",
       " 'from itertools import product x np random randn print x print y for y in product x t',\n",
       " 'import math x np array np random randint size dtype np int def distance x y return math sqrt x y get ipython magic timeit len distance y for y in product x t',\n",
       " 'import scipy as sp from scipy import sparse',\n",
       " 'weights segment filepath os path join modeldir model segment weights hdf history segment filepath os path join modeldir model segment training history json',\n",
       " 'with open os path join procdir train files map pkl rb as f train files map pickle load f with open os path join procdir test files map pkl rb as f test files map pickle load f',\n",
       " 'with open os path join procdir data train val pkl rb as f data images train data masks train data presence train data images val data masks val data presence val pickle load f print image shapes data images train shape data images val shape print mask shapes data masks train shape data masks val shape print presence label shapes data presence train shape data presence val shape',\n",
       " 'model segment load weights weights segment filepath',\n",
       " 'presence model presence predict data images val verbose print confusion matrix n confusion matrix data presence val presence n print roc auc roc auc score data presence val presence n print classification report data presence val presence target names bp not present bp present n',\n",
       " 'preds segment model segment predict data images val verbose np mean np sum data masks val preds segment np sum data masks val np sum preds segment e axis none',\n",
       " 'import os import numpy as np import pandas as pd import statsmodels formula api as smf from sklearn import linear model pd set option display max rows pd set option display max columns',\n",
       " 'print os path join dataset dataset ucla admissions csv',\n",
       " 'df pd read csv os path join dataset dataset ucla admissions csv df dropna inplace true df',\n",
       " 'pd crosstab df prestige df admit margins true',\n",
       " 'pd crosstab df prestige df admit normalize true margins true',\n",
       " 'pd crosstab df prestige df admit normalize true sum',\n",
       " 'pd crosstab df prestige df admit normalize index',\n",
       " 'pd crosstab df prestige df admit normalize columns',\n",
       " 'df prestige df prestige astype int df',\n",
       " 'one hot pd get dummies df prestige prefix prestige one hot',\n",
       " 'df df join other one hot df',\n",
       " 'df drop prestige inplace true axis df',\n",
       " 'df df admit prestige pd crosstab df prestige df admit normalize index',\n",
       " 'df df df prestige df admit value counts',\n",
       " 'tst pd crosstab df prestige df admit normalize index p tst loc p',\n",
       " 'odds p p odds odds',\n",
       " 'p tst loc p',\n",
       " 'odds odds',\n",
       " 'odds odds',\n",
       " 'tst pd crosstab df prestige df admit normalize index p tst loc p',\n",
       " 'import statsmodels api as sm formula admit gre gpa prestige prestige prestige model sm logit df admit df gre prestige prestige prestige model fit result model fit print result summary',\n",
       " 'formula admit gre gpa prestige prestige prestige model smf glm formula formula data df family sm families binomial result model fit print result summary',\n",
       " 'print coefficients print result params print np exp result params print',\n",
       " 'dict gre gpa prestige prestige prestige test pd data frame from dict dict orient index t test result predict test',\n",
       " 'dict gre gpa prestige prestige prestige test pd data frame from dict dict orient index t test result predict test',\n",
       " 'dict gre gpa prestige prestige prestige test pd data frame from dict dict orient index t test result predict test',\n",
       " 'from sklearn linear model import logistic regression lm logistic regression c df head',\n",
       " 'x df gre gpa prestige prestige prestige y df admit lm fit x y',\n",
       " 'dat pd data frame data lm coef columns x columns dat dat t dat columns coef dat odds ratio dat coef apply np exp dat',\n",
       " 'sample pd data frame from dict gre gpa prestige prestige prestige test sample gre gpa prestige prestige prestige predictions lm predict proba test print predictions',\n",
       " 'import datetime import math import matplotlib pyplot as plt import numpy as np import pandas as pd import random import scipy import seaborn as sns get ipython magic matplotlib inline sns set style white',\n",
       " 'cars pd read csv autos csv encoding latin parse dates date crawled date created last seen',\n",
       " 'cars cars cars year of registration cars year of registration cars cars cars price cars price cars cars cars not repaired damage ja',\n",
       " 'cars cars assign mileage cat low medium med high high min int math floor x for x in cars kilometer',\n",
       " 'cars cars assign age datetime timedelta seconds x date created timestamp datetime datetime strptime str x year of registration str x month of registration y m timestamp days for i x in cars iterrows',\n",
       " 'cars cars cars age cars cars cars power ps cars cars cars kilometer',\n",
       " 'cars to csv autos mod csv',\n",
       " 'cars pd read csv autos mod csv',\n",
       " 'cars offer type value counts',\n",
       " 'plt figure sns lmplot age price data cars fit reg false hue brand plt xlim',\n",
       " 'cars model value counts',\n",
       " 'from sklearn import linear model clf linear model linear regression clf fit cars loc kilometer year of registration y cars price',\n",
       " 'clf coef',\n",
       " 'cars year of registration hist',\n",
       " 'sns lmplot year of registration price data cars cars model golf fit reg false hue mileage cat',\n",
       " 'sns lmplot year of registration price data cars cars model er fit reg false hue mileage cat',\n",
       " 'sns lmplot year of registration price data cars cars model er fit reg false hue mileage cat',\n",
       " 'sns lmplot age price data cars cars model er fit reg false hue mileage cat',\n",
       " 'sns countplot x year of registration hue mileage cat data cars cars model er',\n",
       " 'import main import importlib importlib reload main main fit params cars cars model golf loc power ps kilometer age cars price cars model golf',\n",
       " 'import main import importlib importlib reload main main fit params cars cars model er loc power ps kilometer age cars price cars model er',\n",
       " 'import main import importlib importlib reload main main fit params cars cars model er loc power ps kilometer age cars price cars model er',\n",
       " 'from future import print function',\n",
       " 'get ipython run line magic matplotlib inline',\n",
       " 'import numpy as np import matplotlib pyplot as plt',\n",
       " 'v np array v np array a np array',\n",
       " 'print v print a',\n",
       " ...]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load lda predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_results = numpy.load('./lda_results_1_30_2.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [cell_type(g[\"funcs\"], g[\"nodes\"], g[\"header\"]) for g in graphs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 4,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 5,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 5,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 5,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 5,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 5,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 5,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 5,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 5,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 5,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 5,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 5,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 5,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 5,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 5,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " ...]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cell_type(graphs[2][\"funcs\"], graphs[2][\"nodes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "STAGE_PAD = 0\n",
    "WRANGLE = 1\n",
    "EXPLORE = 2\n",
    "MODEL = 3\n",
    "EVALUATE = 4\n",
    "IMPORT = 5\n",
    "\n",
    "SPV_MODE = [STAGE_PAD, WRANGLE, EXPLORE, MODEL, EVALUATE, IMPORT]\n",
    "\n",
    "wrangle_funcs = ['pandas.read_csv', 'pandas.read_csv.dropna', 'pandas.read_csv.fillna',\n",
    "                 'pandas.DataFrame.fillna', 'sklearn.datasets.load_iris', 'scipy.misc.imread',\n",
    "                 'scipy.io.loadmat','sklearn.preprocessing.LabelEncoder', 'scipy.interpolate.interp1d']\n",
    "\n",
    "explore_funcs = ['seaborn.distplot', 'matplotlib.pyplot.show', 'matplotlib.pyplot.plot', 'matplotlib.pyplot.figure',\n",
    "                 'seaborn.pairplot', 'seaborn.heatmap', 'seaborn.lmplot','pandas.read_csv.describe',\n",
    "                 'pandas.DataFrame.describe']\n",
    "# 'matplotlib.pyplot.xlabel', 'matplotlib.pyplot.ylabel'\n",
    "model_funcs = ['sklearn.cluster.KMeans',\n",
    "               'sklearn.decomposition.PCA',\n",
    "               'sklearn.naive_bayes.GaussianNB',\n",
    "               'sklearn.ensemble.RandomForestClassifier',\n",
    "               'sklearn.linear_model.LinearRegression',\n",
    "               'sklearn.linear_model.LogisticRegression',\n",
    "               'sklearn.tree.DecisionTreeRegressor',\n",
    "               'sklearn.ensemble.BaggingRegressor',\n",
    "               'sklearn.neighbors.KNeighborsClassifier',\n",
    "               'sklearn.naive_bayes.MultinomialNB',\n",
    "               'sklearn.svm.SVC',\n",
    "               'sklearn.tree.DecisionTreeClassifier',\n",
    "               'tensorflow.Session',\n",
    "               'sklearn.linear_model.Ridge',\n",
    "               'sklearn.linear_model.Lasso']\n",
    "\n",
    "evaluate_funcs = ['sklearn.metrics.confusion_matrix', 'sklearn.cross_validation.cross_val_score',\n",
    "                  'sklearn.metrics.mean_squared_error', 'sklearn.model_selection.cross_val_score', 'scipy.stats.ttest_ind', 'sklearn.metrics.accuracy_score']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def cell_type(funcs, nodes=None, header=None):\n",
    "    # pdb.set_trace()\n",
    "    # print(header)\n",
    "    grams = [t.lower() for t in header.split() if t]\n",
    "    bi_grams = ['{} {}'.format(t, grams[i + 1])\n",
    "                for i, t in enumerate(grams[:-1])]\n",
    "\n",
    "    if sum([1 for n in nodes if (n[\"type\"] == 'Import' or n[\"type\"] == 'ImportFrom')]) / len(nodes) > 0.3:\n",
    "        return IMPORT\n",
    "\n",
    "    if any([g in bi_grams for g in ['logistic regression', 'machine learning', 'random forest']]) and len(grams) <= 3:\n",
    "        return MODEL\n",
    "    if 'cross validation' in bi_grams and len(grams) <= 3:\n",
    "        return EVALUATE\n",
    "\n",
    "    if any([f in funcs for f in model_funcs]):\n",
    "        return MODEL\n",
    "    if any([f in funcs for f in evaluate_funcs]):\n",
    "        return EVALUATE\n",
    "    if any([f in funcs for f in explore_funcs]):\n",
    "        return EXPLORE\n",
    "    if len(nodes) == 3 and nodes[1][\"type\"] == \"Expr\":\n",
    "        return EXPLORE\n",
    "\n",
    "    if any([f in funcs for f in wrangle_funcs]):\n",
    "        return WRANGLE\n",
    "        # print(h)\n",
    "    return STAGE_PAD\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(50, 6)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        return self.linear(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_results = torch.Tensor(lda_results.astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1351, 0.0043, 0.0043,  ..., 0.0043, 0.0043, 0.0043],\n",
       "        [0.0050, 0.0050, 0.0050,  ..., 0.0050, 0.0050, 0.0050],\n",
       "        [0.0046, 0.0046, 0.0046,  ..., 0.0046, 0.0046, 0.0046],\n",
       "        ...,\n",
       "        [0.0069, 0.0069, 0.0069,  ..., 0.0069, 0.0069, 0.0069],\n",
       "        [0.0074, 0.0074, 0.0074,  ..., 0.0074, 0.0074, 0.0074],\n",
       "        [0.0083, 0.0083, 0.0083,  ..., 0.0083, 0.0083, 0.0083]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1117,  0.0832, -0.0803, -0.0395, -0.1218, -0.0333],\n",
       "        [ 0.1098,  0.0388, -0.0758,  0.0370, -0.0503, -0.1018],\n",
       "        [ 0.1259,  0.0391, -0.0464,  0.0418, -0.0769, -0.1443],\n",
       "        ...,\n",
       "        [ 0.1383,  0.1006, -0.0658, -0.0789, -0.1210, -0.1581],\n",
       "        [ 0.1233,  0.0295, -0.1152,  0.0500, -0.0075, -0.0391],\n",
       "        [ 0.1318,  0.0112, -0.1470,  0.0350, -0.1115, -0.1156]],\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(lda_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "                          evaluate_every=-1, learning_decay=0.7,\n",
       "                          learning_method='batch', learning_offset=10.0,\n",
       "                          max_doc_update_iter=100, max_iter=10,\n",
       "                          mean_change_tol=0.001, n_components=50, n_jobs=None,\n",
       "                          perp_tol=0.1, random_state=0, topic_word_prior=None,\n",
       "                          total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_graphs = []\n",
    "with open('./graphs/test_cells_1_27.txt','r') as f:\n",
    "    for l in f:\n",
    "        test_graphs.append(json.loads(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1745"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_corpus = [clean_code_snippet(g[\"context\"]) for g in test_graphs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nimport pandas as pd\\n\\nfrom pandas import Series DataFrame\\n\\nimport matplotlib pyplot as plt\\n\\nimport seaborn as sns\\n\\nfrom sklearn import datasets\\n\\nfrom sklearn import metrics\\n\\nfrom sklearn naive bayes import GaussianNB\\n',\n",
       " '\\niris datasets load iris \\n\\nX iris data\\n\\nY iris target\\n\\nprint iris DESCR \\n',\n",
       " '\\nmodel GaussianNB \\n',\n",
       " '\\nfrom sklearn cross validation import train test split\\n\\n X train X test Y train Y test train test split X Y \\n',\n",
       " '\\nmodel fit X train Y train \\n',\n",
       " '\\npredicted model predict X test \\n\\nexpected Y test\\n',\n",
       " '\\nprint metrics accuracy score expected predicted \\n',\n",
       " '\\nimport plotly\\n\\nplotly offline init notebook mode \\n\\nimport pandas as pd\\n\\nimport os\\n\\nimport glob\\n\\nimport numpy as np\\n\\nget ipython run line magic matplotlib inline \\n\\nimport matplotlib\\n\\nimport matplotlib pyplot as plt\\n\\nimport seaborn as sns\\n\\nmatplotlib rcParams savefig dpi \\n\\nimport warnings\\n\\nwarnings filterwarnings ignore \\n',\n",
       " '\\nplayers pd read csv atp data atp players csv header None names playerId first name last name hand birthdate nationality \\n\\nplayers head \\n',\n",
       " '\\ncolumns tourney id tourney name surface draw size tourney level tourney date match num winner name loser name score best of round minutes winner hand winner ht winner age w ace w df w svpt w stIn w stWon w ndWon w SvGms w bpSaved w bpFaced loser hand loser ht loser age l ace l df l svpt l stIn l stWon l ndWon l SvGms l bpSaved l bpFaced \\n\\natp matches pd read csv atp data atp matches csv dtype \\n playerId np int \\n parse dates True names columns \\n',\n",
       " '\\nimport pandas as pd\\n',\n",
       " '\\ns pd Series a b c a dtype category \\n\\ns\\n',\n",
       " '\\ndf pd DataFrame \\n A a b c a \\n \\n\\ndf B df A astype category \\n\\ndf\\n',\n",
       " '\\nraw cat pd Categorical a b c a categories b c d ordered False \\n',\n",
       " '\\ns pd Series raw cat \\n\\ns\\n',\n",
       " '\\ndf pd DataFrame \\n key list bbacab \\n data range \\n \\n',\n",
       " '\\npd get dummies df key \\n',\n",
       " '\\nimport numpy as np\\n\\nimport pandas as pd\\n\\nimport matplotlib pyplot as plt\\n',\n",
       " '\\nfile open C Users Inance Documents Thesis BURST Burst Testing Data Analysis Day Testing Burst Changed TXT Sample burst TXT txt \\n\\nlst \\n\\nfor line in file \\n lst append float x for x in line split \\n\\ntime np array x for x in lst \\n\\nvoid np array x for x in lst \\n\\npressure np array x for x in lst \\n\\npressure pressure \\n',\n",
       " '\\npressure max \\n\\n fig ax plt subplots \\n\\nax scatter time np minimum pressure pressure max \\n\\naxis font \\n fontname Constantia \\n size \\n \\n\\n plt ylabel Pressure axis font plt xlabel Time axis font \\n\\nplt show \\n',\n",
       " '\\nimport numpy as np\\n\\nfrom scipy signal import butter lfilter freqz\\n\\nimport matplotlib pyplot as plt\\n\\n\\ndef butter lowpass cutoff fs order \\n nyq fs \\n normal cutoff cutoff nyq \\n b a butter order normal cutoff btype low analog False \\n return b a \\n\\n\\ndef butter lowpass filter data cutoff fs order \\n b a butter lowpass cutoff fs order order \\n y lfilter b a data \\n return y\\n\\norder \\n\\nfs \\n\\ncutoff \\n\\n b a butter lowpass cutoff fs order \\n\\n w h freqz b a worN \\n\\nplt subplot \\n\\nplt plot fs w np pi np abs h b \\n\\nplt plot cutoff np sqrt ko \\n\\nplt axvline cutoff color k \\n\\nplt xlim fs \\n\\nplt title Lowpass Filter Frequency Response \\n\\nplt xlabel Frequency Hz \\n\\nplt grid \\n\\nT \\n\\nn int T fs \\n\\nt time\\n\\ndata pressure\\n\\ny butter lowpass filter data cutoff fs order \\n\\nplt subplot \\n\\nplt plot t data b label data \\n\\nplt plot t y g linewidth label filtered data \\n\\nplt xlabel Time sec \\n\\nplt grid \\n\\nplt legend \\n\\nplt subplots adjust hspace \\n\\nplt show \\n',\n",
       " '\\npressure max \\n\\n fig ax plt subplots \\n\\nax scatter t y \\n\\naxis font \\n fontname Constantia \\n size \\n \\n\\n plt ylabel Pressure axis font plt xlabel Time axis font \\n\\nplt show \\n',\n",
       " '\\nimport numpy as np\\n\\nimport pandas as pd\\n',\n",
       " '\\nimport matplotlib pyplot as plt\\n\\nget ipython run line magic matplotlib inline \\n',\n",
       " '\\ndataset pd read csv Data csv \\n',\n",
       " '\\nX dataset iloc values\\n\\ny dataset iloc values\\n',\n",
       " '\\nfrom sklearn preprocessing import Imputer\\n',\n",
       " '\\nimputer Imputer missing values NaN strategy mean axis \\n',\n",
       " '\\nimputer imputer fit X \\n',\n",
       " '\\nX imputer transform X \\n',\n",
       " '\\nX\\n',\n",
       " '\\nfrom sklearn preprocessing import LabelEncoder OneHotEncoder\\n',\n",
       " '\\nlabelencoder X LabelEncoder \\n',\n",
       " '\\nX labelencoder X fit transform X \\n',\n",
       " '\\nonehotencoder OneHotEncoder categorical features \\n',\n",
       " '\\nX onehotencoder fit transform X toarray \\n',\n",
       " '\\nX\\n',\n",
       " '\\nlabelencoder y LabelEncoder \\n',\n",
       " '\\ny labelencoder y fit transform y \\n',\n",
       " '\\ny\\n',\n",
       " '\\nfrom sklearn cross validation import train test split\\n',\n",
       " '\\n X train X test y train y test train test split X y test size random state \\n',\n",
       " '\\nfrom sklearn preprocessing import StandardScaler\\n',\n",
       " '\\nsc X StandardScaler \\n',\n",
       " '\\nX train sc X fit transform X train \\n',\n",
       " '\\nX test sc X transform X test \\n',\n",
       " '\\nget ipython run line magic matplotlib inline \\n',\n",
       " '\\nimport numpy as np\\n\\nfrom scipy import linalg as la\\n\\nfrom os import walk\\n\\nfrom scipy ndimage import imread\\n\\nfrom matplotlib import pyplot as plt\\n\\nimport matplotlib cm as cm\\n\\nimport random\\n\\n\\nclass FacialRec \\n\\n def init self path \\n self initFaces path \\n self initMeanImage \\n self initDifferences \\n self initEigenfaces \\n\\n def initFaces self path \\n self F getFaces path \\n\\n def initMeanImage self \\n m n self F shape\\n self mu np sum self F axis float n \\n\\n def initDifferences self \\n m n self F shape\\n self mu self mu reshape len self mu \\n diff np tile self mu n \\n self Fbar self F diff \\n\\n def initEigenfaces self \\n self U s Vt la svd self Fbar full matrices False \\n\\n def project self A s \\n A s np dot self U T A \\n return A s\\n\\n def findNearest self image s \\n image image reshape len image \\n ghat self project image self mu \\n fhat self project self Fbar \\n diff fhat T ghat T T\\n i np linalg norm diff ord axis \\n return np argmin i \\n',\n",
       " '\\n\\ndef getFaces path faces \\n Traverse the directory specified by path and return an array containing n one column vector per subdirectory n \\n faces \\n for dirpath dirnames filenames in walk path \\n for f in filenames \\n if f jpg \\n face imread dirpath f mean axis ravel \\n faces append face \\n break\\n return np array faces T\\n',\n",
       " '\\n\\ndef show im w h \\n Plot the flattened grayscale image im of width w and height h \\n plt imshow im reshape w h cmap cm Greys r \\n plt show \\n\\n\\ndef show test image result w h \\n Convenience function for plotting two flattened grayscale images of n the specified width and height side by side n \\n plt subplot \\n plt title Inputed Image \\n plt imshow test image reshape w h cmap cm Greys r \\n plt axis off \\n plt subplot \\n plt title Closest Match \\n plt imshow result reshape w h cmap cm Greys r \\n plt axis off \\n plt show \\n\\n\\ndef sampleFaces n tests path faces \\n Return an array containing a sample of n tests images contained n in the path as flattened images in the columns of the output n \\n files \\n for dirpath dirnames filenames in walk path \\n for f in filenames \\n if f jpg \\n files append dirpath f \\n test files random sample files n tests \\n images np array imread f mean axis ravel for f in test files T\\n return images\\n',\n",
       " '\\nface FacialRec faces \\n\\nshow face mu \\n',\n",
       " '\\nshow face Fbar \\n',\n",
       " '\\nshow face U \\n',\n",
       " '\\n\\ndef test \\n test images sampleFaces \\n facial FacialRec faces \\n for i in test images T \\n a facial findNearest i \\n show i facial F a \\n\\ntest \\n',\n",
       " '\\nimport pandas as pd\\n\\nimport numpy as np\\n\\nimport matplotlib pyplot as plt\\n\\nimport scipy stats as sts\\n\\nget ipython run line magic matplotlib inline \\n',\n",
       " '\\nsample np random choice \\n',\n",
       " '\\nfrom collections import Counter\\n\\nc Counter sample \\n\\nprint \\n\\nprint c \\n\\nprint \\n\\nprint k v for k v in c items \\n',\n",
       " '\\nnorm rv sts norm \\n\\nsample norm rv rvs \\n',\n",
       " '\\nx np linspace \\n\\ncdf norm rv cdf x \\n\\nplt plot x cdf label theoretical CDF \\n\\nfrom statsmodels distributions empirical distribution import ECDF\\n\\necdf ECDF sample \\n\\nplt step ecdf x ecdf y label ECDF \\n\\nplt ylabel f x \\n\\nplt xlabel x \\n\\nplt legend loc upper left \\n',\n",
       " '\\nplt hist sample normed True \\n\\nplt ylabel number of samples \\n\\nplt xlabel x \\n',\n",
       " '\\nplt hist sample bins normed True \\n\\nplt ylabel number of samples \\n\\nplt xlabel x \\n',\n",
       " '\\nplt hist sample bins normed True \\n\\nplt ylabel number of samples \\n\\nplt xlabel x \\n',\n",
       " '\\ndf pd DataFrame sample columns KDE \\n\\nax df plot kind density \\n\\nx np linspace \\n\\npdf norm rv pdf x \\n\\nplt plot x pdf label theoretical pdf alpha \\n\\nplt legend \\n\\nplt ylabel f x \\n\\nplt xlabel x \\n',\n",
       " '\\nfrom sklearn import datasets\\n\\nimport numpy as np\\n\\nfrom sklearn model selection import train test split\\n\\nfrom sklearn decomposition import PCA KernelPCA\\n\\nfrom sklearn multiclass import OneVsRestClassifier\\n\\nfrom sklearn linear model import LogisticRegression\\n\\nimport matplotlib pyplot as plt\\n\\nfrom matplotlib import cm\\n\\nfrom sklearn svm import SVC\\n\\nfrom sklearn model selection import GridSearchCV\\n\\nfrom sklearn metrics import classification report\\n\\nfrom sklearn metrics import confusion matrix\\n\\nfrom PIL import Image\\n\\nfrom sklearn neural network import MLPClassifier\\n\\nget ipython run line magic matplotlib inline \\n',\n",
       " '\\nlfw data datasets fetch lfw people min faces per person \\n',\n",
       " '\\njenya \\n\\nfor i in range \\n jenya append np asarray Image open j s jpg i reshape \\n\\njenya np array jenya \\n\\njenya target \\n',\n",
       " '\\n X y lfw data data lfw data target \\n\\nX np concatenate X jenya axis \\n\\ny new np zeros \\n\\ny new y\\n\\nfor i in range \\n y new i jenya target\\n\\ny y new\\n',\n",
       " '\\n X train X test y train y test train test split X y train size random state stratify y \\n\\n X train train X train test y train train y train test train test split X train y train train size stratify y train \\n',\n",
       " '\\n n samples h w lfw data images shape\\n\\nn features X shape \\n\\ntarget names lfw data target names\\n\\nmy name np array Eugene Marshakov \\n\\ntarget names np concatenate target names my name axis \\n\\nn classes target names shape \\n\\nprint Total dataset size \\n\\nprint n samples d n samples \\n\\nprint n features d n features \\n\\nprint n classes d n classes \\n',\n",
       " '\\nfrom sklearn decomposition import PCA KernelPCA\\n\\nfrom time import time\\n',\n",
       " '\\n\\ndef plot gallery images titles h w n row n col eugene False \\n Helper function to plot a gallery of portraits \\n if eugene False \\n plt figure figsize n col n row \\n plt subplots adjust bottom left right top hspace \\n for i in range n row n col \\n plt subplot n row n col i \\n plt imshow images i reshape h w cmap plt cm gray \\n plt title titles i size \\n plt xticks \\n plt yticks \\n else \\n plt figure figsize n col n row \\n plt subplots adjust bottom left right top hspace \\n for i in range n row n col \\n plt subplot n row n col i \\n plt imshow images i reshape h w cmap plt cm gray \\n plt title titles i size \\n plt xticks \\n plt yticks \\n place prediction titles index predicted Marshakov ntrue Marshakov \\n plt subplot n row n col n row n col \\n plt imshow images place reshape h w cmap plt cm gray \\n plt title titles place size \\n plt xticks \\n plt yticks \\n\\n\\ndef title y pred y test target names i \\n pred name target names int y pred i rsplit \\n true name target names int y test i rsplit \\n return predicted s ntrue s pred name true name \\n',\n",
       " '\\nn components \\n\\nprint Extracting the top d eigenfaces from d faces n components X train shape \\n\\nt time \\n\\npca PCA n components n components svd solver randomized whiten True fit X train \\n\\neigenfaces pca components reshape n components h w \\n\\nprint Projecting the input data on the eigenfaces orthonormal basis \\n\\nt time \\n\\nX train pca pca transform X train \\n\\nX test pca pca transform X test \\n',\n",
       " '\\nx x for x in X train \\n\\nx x for x in X train \\n\\ncolors cm jet y max y train for y in y train \\n\\nplt figure figsize \\n\\nax plt axes frameon False \\n\\nplt scatter x x c colors edgecolor none s \\n\\nplt xlabel x \\n\\nplt ylabel x \\n\\nplt title Data visualisation \\n',\n",
       " '\\nprint Reduction \\n\\nprint Original dimension format len train data \\n\\npca KernelPCA n components kernel cosine \\n\\npca fit X train \\n\\ntrain pca transform X train \\n\\nprint Reduced dimension format len train \\n',\n",
       " '\\nx x for x in train \\n\\nx x for x in train \\n\\ncolors cm jet y max y train for y in train label \\n\\nplt figure figsize \\n\\nax plt axes frameon False \\n\\nplt scatter x x c colors edgecolor none s \\n\\nplt xlabel x \\n\\nplt ylabel x \\n\\nplt title PCA embedding to dimensional space \\n',\n",
       " '\\nprint Reduction \\n\\nprint Original dimension format len train data \\n\\npca KernelPCA n components kernel cosine \\n\\npca fit X train \\n\\ntrain pca transform X train \\n\\nprint Reduced dimension format len train \\n',\n",
       " '\\ncolors cm jet y max y train for y in train label \\n',\n",
       " '\\nimport matplotlib pyplot as plt\\n\\nfrom mpl toolkits mplot d import Axes D\\n\\nfig plt figure figsize \\n\\nax fig add subplot projection d \\n\\nax scatter train train train c colors \\n\\nplt show \\n',\n",
       " '\\nprint Fitting the classifier to the training set \\n\\nt time \\n\\nparam grid \\n C \\n gamma \\n \\n\\nclf GridSearchCV SVC kernel rbf class weight balanced param grid \\n\\nclf clf fit X train pca y train \\n\\nprint done in fs time t \\n\\nprint Best estimator found by grid search \\n\\nprint clf best estimator \\n',\n",
       " '\\nprint Predicting people s names on the test set \\n\\nt time \\n\\ny pred clf predict X test pca \\n\\nprint done in fs time t \\n\\nprint classification report y test y pred target names target names \\n',\n",
       " '\\nprediction titles title y pred y test target names i for i in range y pred shape \\n\\nplot gallery X test prediction titles h w eugene True \\n',\n",
       " '\\nprint Reduction \\n\\nprint Original dimension format len train data \\n\\npca PCA n components whiten True \\n\\npca fit X train \\n\\nX train pca pca transform X train \\n\\nX test pca pca transform X test \\n\\nprint Reduced dimension format len X train pca \\n',\n",
       " '\\nprint Fitting the classifier to the training set \\n\\nt time \\n\\nparam grid \\n C e \\n penalty l l \\n \\n\\nclf GridSearchCV LogisticRegression param grid \\n\\nclf clf fit X train pca y train \\n\\nprint done in fs time t \\n\\nprint Best estimator found by grid search \\n\\nprint clf best estimator \\n',\n",
       " '\\nprint Predicting people s names on the test set \\n\\nt time \\n\\ny pred clf predict X test pca \\n\\nprint done in fs time t \\n\\nprint classification report y test y pred target names target names \\n',\n",
       " '\\nprint Reduction \\n\\nprint Original dimension format len train data \\n\\npca PCA n components whiten True \\n\\npca fit X train \\n\\nX train pca pca transform X train \\n\\nX test pca pca transform X test \\n\\nprint Reduced dimension format len X train pca \\n',\n",
       " '\\nmlp MLPClassifier hidden layer sizes solver lbfgs max iter \\n',\n",
       " '\\nmlp fit X train pca y train \\n',\n",
       " '\\nprint Predicting people s names on the test set \\n\\nt time \\n\\ny pred mlp predict X test pca \\n\\nprint done in fs time t \\n\\nprint classification report y test y pred target names target names \\n',\n",
       " '\\nnp logspace \\n',\n",
       " '\\nget ipython run line magic matplotlib inline \\n\\nimport pandas as pd\\n\\nimport matplotlib pyplot as plt\\n\\nimport matplotlib\\n\\nmatplotlib style use ggplot \\n',\n",
       " '\\ntrain df pd read csv data train csv \\n\\ntrain df Label train df Label astype category \\n\\ntest df pd read csv data test csv \\n\\nvalidation df pd read csv data valid csv \\n',\n",
       " '\\ntrain df describe \\n',\n",
       " '\\ntrain df Label hist \\n\\nplt title Training Label Distribution \\n',\n",
       " '\\npd options display max colwidth \\n\\ntrain df head \\n',\n",
       " '\\nplt figure \\n\\ntrain df context len train df Context str split apply len \\n\\ntrain df context len hist bins \\n\\nplt title Training Context Length Statistics \\n\\nprint train df context len describe \\n\\nplt figure \\n\\ntrain df utterance len train df Utterance str split apply len \\n\\ntrain df utterance len hist bins \\n\\nplt title Training Utterance Length Statistics \\n\\nprint train df utterance len describe \\n',\n",
       " '\\npd options display max colwidth \\n\\ntest df head \\n',\n",
       " '\\ntest df describe \\n',\n",
       " '\\nget ipython run line magic matplotlib inline \\n\\nimport warnings\\n\\nimport numpy as np\\n\\nimport pandas as pd\\n\\nfrom numpy import random\\n\\nfrom scipy import stats\\n\\nimport matplotlib pyplot as plt\\n\\nimport seaborn as sns\\n\\nimport quandl\\n\\nimport statsmodels as sm\\n\\nimport statsmodels formula api as smf\\n\\nimport statsmodels api as sma\\n\\nimport patsy\\n\\nfrom statsmodels graphics api import abline plot\\n\\nimport numpy linalg as linalg\\n\\nfrom mpl toolkits mplot d import Axes D\\n\\nfrom matplotlib colors import ListedColormap\\n\\nfrom sklearn import neighbors datasets\\n\\nfrom tabulate import tabulate\\n\\nwarnings simplefilter ignore \\n\\nsns set context notebook style whitegrid palette deep font sans serif font scale rc None \\n',\n",
       " '\\ngdp pc quandl get FRED A RX A NBEA end date \\n\\n cycle trend sma tsa filters hpfilter gdp pc lamb \\n',\n",
       " '\\nprint gdp pc head \\n',\n",
       " '\\nplt figure figsize \\n\\ngdp pc plot color blue \\n\\nplt title Real Disposible Income per Person fontsize \\n\\nplt ylabel US fontsize \\n\\nplt xlabel Time \\n\\nplt legend \\n',\n",
       " '\\nplt figure figsize \\n\\ntrend plot color blue \\n\\nplt title Real Disposible Income per Person fontsize \\n\\nplt ylabel US fontsize \\n\\nplt legend \\n',\n",
       " '\\nnormal np random normal \\n\\nplt hist normal normed \\n\\nplt xlabel Draws \\n\\nplt ylabel Probability \\n\\nplt title Histogram of the Normal Distrubiton mu sigma \\n',\n",
       " '\\nprint normal mean \\n\\nprint normal std \\n\\nprint normal mean normal std \\n',\n",
       " '\\ngriliches pd read csv https vincentarelbundock github io Rdatasets csv Ecdat Griliches csv \\n\\ngriliches griliches drop griliches columns \\n',\n",
       " '\\nprint griliches describe \\n',\n",
       " '\\nplt hist griliches lw normed \\n\\nplt xlabel Log of Wages \\n\\nplt ylabel Probability \\n\\nplt title Griliches Original Wage Data \\n',\n",
       " '\\nplt scatter griliches school griliches lw \\n\\nplt xlabel Years of Schooling \\n\\nplt ylabel Wages \\n\\nplt title Wages versus Schooling \\n',\n",
       " '\\nplt scatter griliches age griliches lw \\n\\nplt xlabel Age \\n\\nplt ylabel Wages \\n\\nplt title Wages versus Age \\n',\n",
       " '\\nplt scatter griliches iq griliches lw \\n\\nplt xlabel IQ \\n\\nplt ylabel Wages \\n\\nplt title Wages versus IQ \\n',\n",
       " '\\nplt scatter griliches med griliches lw \\n\\nplt xlabel Education of Mother \\n\\nplt ylabel Wages \\n\\nplt title Wages versus Education of Mother \\n',\n",
       " '\\nmod smf ols formula lw school iq age expr tenure med data griliches fit \\n\\nprint mod summary \\n',\n",
       " '\\nstart date \\n\\nend date \\n\\ntbond Q quandl get FRED DGS trim start start date trim end end date \\n\\ntbond tbond Q VALUE \\n',\n",
       " '\\nplt figure figsize \\n\\ntbond plot color blue \\n\\nplt title Year U S Treasurys Present \\n\\nplt ylabel \\n\\nplt axhline y color red \\n',\n",
       " '\\nprint A Calendar Is A Markov Transition Matrix \\n\\ntable S M T W T F S S M T W T F S \\n\\nprint tabulate table tablefmt fancy grid numalign center \\n\\nprint \\n',\n",
       " '\\nprint Day of Week Vector Today is Friday \\n\\ntable S M T W T F S \\n\\nprint tabulate table tablefmt fancy grid numalign center \\n\\nprint \\n',\n",
       " '\\nMT np matrix \\n\\ndow np matrix \\n\\nprint Markov Transition Matrix \\n\\nprint MT \\n\\nprint \\n\\nprint Today \\n\\nprint dow \\n\\nprint \\n\\nprint Tomorrow \\n\\nprint dow MT \\n\\nprint \\n\\nprint The Day After Tomorrow \\n\\nprint dow MT MT \\n\\nprint \\n\\nprint The Day After Tomorrow Using Linear Algebra \\n\\nprint dow np linalg matrix power MT \\n',\n",
       " '\\nrandom seed \\n\\nedge \\n\\nnsteps \\n\\n pos pos \\n\\n walk walk pos pos \\n\\nfor i in range nsteps \\n step if np random randint else \\n step if np random randint else \\n pos step \\n pos step \\n if np abs pos or np abs pos \\n print Sailor fell off the moutain top at coordinates pos pos on step i \\n break\\n walk append pos \\n walk append pos \\n',\n",
       " '\\nplt plot walk walk r \\n\\nplt title Path of the Drunken Sailor \\n\\nplt xlim edge edge \\n\\nplt ylim edge edge \\n',\n",
       " '\\nfrom math import cos log\\n',\n",
       " '\\n\\ndef f py I J \\n res \\n for i in range I \\n for j in range J \\n res int cos log \\n return res\\n',\n",
       " '\\n I J \\n\\nget ipython run line magic time res f py I J \\n',\n",
       " '\\nprint res \\n',\n",
       " '\\n\\ndef f np I J \\n a np ones I J dtype np float \\n return int np sum np cos np log a \\n',\n",
       " '\\nget ipython run line magic time res f np I J \\n',\n",
       " '\\nprint res \\n',\n",
       " '\\nimport numba as nb\\n\\nf py nb nb jit f py \\n\\nf np nb nb jit f np \\n',\n",
       " '\\nget ipython run line magic time f py nb I J \\n',\n",
       " '\\nget ipython run line magic time f np nb I J \\n',\n",
       " '\\nfrom future import print function\\n\\nimport os\\n\\nos environ MPLCONFIGDIR tmp \\n\\nfrom pyspark import SparkContext SparkConf\\n\\nfrom numpy import random pi\\n\\nsc SparkContext appName pipy environment \\n MPLCONFIGDIR tmp \\n \\n\\nnSamples \\n\\n\\ndef sample n \\n x y random uniform random uniform \\n return if x x y y else \\n\\ncount sc parallelize xrange nSamples map sample reduce lambda a b a b \\n\\nprint Size is i nSamples \\n\\nprint Pi is roughly f count nSamples \\n\\nprint Pi is exactly f pi \\n',\n",
       " '\\nfrom PIL import Image\\n\\nimport urllib request as url\\n\\nimport io\\n\\nprint TLC s Geography of New York City \\n\\nfd url urlopen http www east harlem com images taximap jpg \\n\\nimage file io BytesIO fd read \\n\\nImage open image file \\n',\n",
       " '\\ntaxi pd read stata taximarkov dta \\n\\nprint taxi head \\n\\nytrain taxi do as matrix columns None astype int \\n\\nXtrain taxi pu tod dow passengers as matrix columns None astype int \\n',\n",
       " '\\nfrom sklearn naive bayes import GaussianNB\\n\\ngnb GaussianNB \\n\\ngnb fit Xtrain ytrain \\n\\nfrom sklearn ensemble import RandomForestClassifier\\n\\nrf RandomForestClassifier n estimators \\n\\nrf fit Xtrain ytrain \\n',\n",
       " '\\nXtest \\n\\nprint gnb predict Xtest \\n\\nXtest \\n\\nprint rf predict Xtest \\n',\n",
       " '\\nimport numpy as np\\n\\nimport pandas as pd\\n\\nfrom keras models import Sequential\\n\\nfrom keras layers import Dense\\n\\nfrom keras wrappers scikit learn import KerasRegressor\\n\\nfrom sklearn cross validation import cross val score\\n\\nfrom sklearn cross validation import KFold\\n\\nfrom sklearn preprocessing import StandardScaler\\n\\nfrom sklearn pipeline import Pipeline\\n',\n",
       " '\\ndataframe pd read csv data housing csv delim whitespace True header None \\n\\ndataset dataframe values\\n\\nX dataset \\n\\nY dataset \\n',\n",
       " '\\n\\ndef baseline model \\n model Sequential \\n model add Dense input dim init normal activation relu \\n model add Dense init normal \\n model compile loss mean squared error optimizer adam \\n return model\\n',\n",
       " '\\nseed \\n\\nnp random seed seed \\n\\nestimator KerasRegressor build fn baseline model nb epoch batch size verbose \\n',\n",
       " '\\nkfold KFold n len X n folds random state seed \\n\\nresults cross val score estimator X Y cv kfold \\n\\nprint Baseline f f MSE results mean results std \\n',\n",
       " '\\nseed \\n\\nnp random seed seed \\n\\nestimators \\n\\nestimators append standardize StandardScaler \\n\\nestimators append mlp KerasRegressor build fn baseline model nb epoch batch size verbose \\n\\npipeline Pipeline estimators \\n\\nkfold KFold n len X n folds random state seed \\n\\nresults cross val score pipeline X Y cv kfold \\n\\nprint Standardized f f MSE results mean results std \\n',\n",
       " '\\n\\ndef larger model \\n model Sequential \\n model add Dense input dim init normal activation relu \\n model add Dense init normal activation relu \\n model add Dense init normal \\n model compile loss mean squared error optimizer adam \\n return model\\n',\n",
       " '\\nseed \\n\\nnp random seed seed \\n\\nestimators \\n\\nestimators append standardize StandardScaler \\n\\nestimators append mlp KerasRegressor build fn larger model nb epoch batch size verbose \\n\\npipeline Pipeline estimators \\n\\nkfold KFold n len X n folds random state seed \\n\\nresults cross val score pipeline X Y cv kfold \\n\\nprint Standardized f f MSE results mean results std \\n',\n",
       " '\\n\\ndef wider model \\n model Sequential \\n model add Dense input dim init normal activation relu \\n model add Dense init normal \\n model compile loss mean squared error optimizer adam \\n return model\\n',\n",
       " '\\nseed \\n\\nnp random seed seed \\n\\nestimators \\n\\nestimators append standardize StandardScaler \\n\\nestimators append mlp KerasRegressor build fn wider model nb epoch batch size verbose \\n\\npipeline Pipeline estimators \\n\\nkfold KFold n len X n folds random state seed \\n\\nresults cross val score pipeline X Y cv kfold \\n\\nprint Standardized f f MSE results mean results std \\n',\n",
       " '\\nimport numpy as np\\n\\nfrom pandas import Series DataFrame\\n\\nimport pandas as pd\\n',\n",
       " '\\ndframe DataFrame np arange reshape \\n',\n",
       " '\\ndframe\\n',\n",
       " '\\ndata \\n City SF LA NYC \\n Population \\n \\n\\ncity frame DataFrame data \\n\\ncity frame\\n',\n",
       " '\\ncolors Series Blue Red index \\n\\ndframe Color colors\\n\\ncolors\\n',\n",
       " '\\nimport webbrowser\\n\\nwebsite http en wikipedia org wiki NFL win loss records \\n\\nwebbrowser open website \\n',\n",
       " '\\nnfl frame pd read clipboard \\n',\n",
       " '\\nnfl frame\\n',\n",
       " '\\nnfl frame columns\\n',\n",
       " '\\nDataFrame nfl frame columns Team First Season Total Games \\n',\n",
       " '\\nDataFrame nfl frame columns Team First Season Total Games Stadium \\n',\n",
       " '\\nnfl frame columns\\n',\n",
       " '\\nnfl frame Team\\n',\n",
       " '\\nnfl frame ix \\n',\n",
       " '\\nnfl frame Stadium Levi s Stadium \\n',\n",
       " '\\nnfl frame\\n',\n",
       " '\\nnfl frame columns\\n',\n",
       " '\\nstadiums Series Levi s Stadium AT T Stadium index \\n',\n",
       " '\\nnfl frame Stadium stadiums\\n\\nnfl frame\\n',\n",
       " '\\ndel nfl frame Stadium \\n\\nnfl frame\\n',\n",
       " '\\ndata \\n City SF LA NYC \\n Population \\n \\n\\ncity frame DataFrame data \\n\\ncity frame\\n',\n",
       " '\\n Read data dumps from disk \\n\\nimport json\\n\\nimport os\\n\\nimport track\\n\\nfolder dump \\n\\ndicts \\n\\nfor file in os listdir folder \\n with open s s folder file as inp \\n try \\n data json load inp \\n except ValueError \\n print file \\n else \\n try \\n dicts append track features data \\n except KeyError \\n print file \\n\\nprint d files processed len dicts \\n',\n",
       " '\\n Build a dataframe \\n\\nimport numpy as np\\n\\nimport pandas as pd\\n\\ndf pd DataFrame dicts \\n\\ndf df track feature list \\n\\nto int Listeners Playcount Duration \\n\\ndf to int df to int astype int \\n\\ndf sort Listeners ascending False inplace True \\n\\nclass labels np array Hit Flop \\n\\ndf insert Class pd qcut range len df labels class labels \\n',\n",
       " '\\n Is a science demo \\n\\nfeature list Class Name Artist MBID Listeners Playcount Duration ABL Average Loudness ABL Tempo BPM \\n\\ndfn df feature list \\n\\ndfn dfn dfn Duration \\n\\ndfn insert len dfn columns Tempo dfn ABL Tempo BPM dfn ABL Tempo BPM apply int \\n\\ndfn insert len dfn columns Tempo dfn ABL Tempo BPM dfn ABL Tempo BPM apply int \\n\\ndfn insert len dfn columns Tempo dfn ABL Tempo BPM dfn ABL Tempo BPM apply int \\n\\ndfn insert len dfn columns Tempo dfn ABL Tempo BPM dfn ABL Tempo BPM apply int \\n\\ndfn insert len dfn columns Tempo dfn ABL Tempo BPM dfn ABL Tempo BPM apply int \\n\\ndfn insert len dfn columns Tempo dfn ABL Tempo BPM dfn ABL Tempo BPM apply int \\n\\ndfn drop ABL Tempo BPM axis inplace True \\n\\ndfn insert len dfn columns Duration dfn Duration dfn Duration apply int \\n\\ndfn insert len dfn columns Duration dfn Duration dfn Duration apply int \\n\\ndfn insert len dfn columns Duration dfn Duration dfn Duration apply int \\n\\ndfn insert len dfn columns Duration dfn Duration apply int \\n\\nfr dfn\\n',\n",
       " '\\n Data \\n\\nfrom sklearn cross validation import train test split\\n\\nfeatures fr columns \\n\\nX fr features \\n\\n y pd factorize fr Class \\n\\n X train X test y train y test train test split X y test size \\n',\n",
       " '\\n SVM Classifier \\n\\nfrom sklearn import svm\\n\\nfrom sklearn cross validation import cross val score\\n\\nclf svm SVC kernel rbf \\n\\nclf fit X train y train \\n\\ncross val score clf X y \\n',\n",
       " '\\nimport numpy as np\\n\\nimport matplotlib pyplot as plt\\n\\nfrom sklearn import svm datasets\\n',\n",
       " '\\n Random Forest Classifier \\n\\nfrom sklearn ensemble import RandomForestClassifier\\n\\nclf RandomForestClassifier max features n jobs n estimators \\n\\nclf fit X train y train \\n\\npd crosstab class labels clf predict X test test class rownames actual colnames pred \\n',\n",
       " '\\n Cross Validation \\n\\nfrom sklearn cross validation import cross val score\\n\\ncross val score clf X Y \\n',\n",
       " '\\n Automatic Feature Selection \\n\\nX train features \\n\\n y pd factorize train class \\n\\nprint X shape \\n\\nfrom sklearn ensemble import ExtraTreesClassifier\\n\\nX new ExtraTreesClassifier fit transform X y \\n\\nprint X new shape \\n\\nfrom sklearn svm import LinearSVC\\n\\nX new LinearSVC C penalty l dual False fit transform X y \\n\\nprint X new shape \\n\\nfrom sklearn feature selection import VarianceThreshold\\n\\nX new VarianceThreshold threshold fit transform X y \\n\\nprint X new shape \\n',\n",
       " '\\nimport pandas as pd\\n\\nimport geopandas as gpd\\n\\nfrom shapely geometry import Point\\n\\nget ipython run line magic matplotlib inline \\n',\n",
       " '\\nboros gpd read file nynta d nynta shp \\n\\nboros\\n',\n",
       " '\\ndf pd read csv galleries csv \\n\\ndf head \\n',\n",
       " '\\npoints df apply lambda row Point row longitude row latitude axis \\n\\ngalleries gpd GeoDataFrame df geometry points \\n\\ngalleries crs \\n init epsg \\n \\n\\ngalleries head \\n',\n",
       " '\\nax boros plot edgecolor white color lightgrey \\n\\nax axis off \\n',\n",
       " '\\nax galleries plot figsize linewidth markersize alpha color pink \\n\\nax axis off \\n',\n",
       " '\\nax boros plot figsize linewidth edgecolor white color lightgrey \\n\\ngalleries plot figsize linewidth markersize alpha color pink ax ax \\n\\nax axis off \\n',\n",
       " '\\ngalleries crs\\n',\n",
       " '\\nboros crs\\n',\n",
       " '\\nboros plot \\n\\ngalleries plot \\n',\n",
       " '\\nboros boros to crs galleries crs \\n\\nboros plot \\n',\n",
       " '\\nax boros plot figsize linewidth edgecolor white color lightgrey \\n\\ngalleries plot figsize linewidth markersize alpha color pink ax ax \\n\\nax axis off \\n',\n",
       " '\\nget ipython run line magic matplotlib inline \\n\\nimport numpy as np\\n\\nimport matplotlib pyplot as plt\\n\\nimport matplotlib image as imgplot\\n\\nimport numpy as np\\n\\nimport time\\n\\nimport tensorflow as tf\\n\\ntf set random seed \\n\\nfrom keras models import Sequential\\n\\nfrom keras layers import Dense Activation Dropout BatchNormalization\\n\\nfrom keras layers import Convolution D MaxPooling D Flatten\\n\\nimport keras\\n\\nimport sys\\n\\nprint Keras TF Python format keras version tf version sys version info \\n',\n",
       " '\\ntry \\n import cPickle as pickle\\nexcept ImportError \\n import pickle\\n\\nimport gzip\\n\\nwith gzip open mnist pkl gz rb as f \\n if sys version info major \\n X y pickle load f encoding latin \\n else \\n X y pickle load f \\n\\nPIXELS len X \\n\\nprint X shape y shape PIXELS \\n\\nX X reshape \\n\\nnp shape X \\n',\n",
       " '\\nX train X \\n\\nY train y \\n\\nX val X \\n\\nY val y \\n',\n",
       " '\\nX train np reshape X train \\n\\nX val np reshape X val \\n\\nprint X train shape \\n\\nprint X val shape \\n',\n",
       " '\\nplt imshow X train cmap gray \\n',\n",
       " '\\n\\ndef convertToOneHot vector num classes None \\n result np zeros len vector num classes dtype float \\n result np arange len vector vector \\n return result\\n\\nprint class label \\n\\nprint Y train \\n\\nprint class label in OneHot encodig \\n\\nprint convertToOneHot Y train \\n',\n",
       " '\\nY train convertToOneHot Y train num classes \\n\\nprint Y train shape \\n\\nY val convertToOneHot Y val num classes \\n\\nprint Y val shape \\n',\n",
       " '\\nX mean np mean X train axis \\n\\nX std np std X train axis \\n\\nX train X train X mean X std \\n\\nX val X val X mean X std \\n',\n",
       " '\\nbatch size \\n\\nnb classes \\n\\nnb epoch \\n\\n img rows img cols \\n\\nkernel size \\n\\ninput shape img rows img cols \\n\\npool size \\n',\n",
       " '\\nname cnn \\n\\nmodel Sequential \\n\\nname cnn \\n\\nmodel add Convolution D kernel size kernel size init he normal border mode same input shape input shape \\n\\nmodel add keras layers normalization BatchNormalization \\n\\nmodel add Activation relu \\n\\nmodel add Convolution D kernel size kernel size init he normal border mode same \\n\\nmodel add keras layers normalization BatchNormalization \\n\\nmodel add Activation relu \\n\\nmodel add MaxPooling D pool size pool size \\n\\nmodel add Convolution D kernel size kernel size init he normal border mode same \\n\\nmodel add keras layers normalization BatchNormalization \\n\\nmodel add Activation relu \\n\\nmodel add Convolution D kernel size kernel size init he normal border mode same \\n\\nmodel add keras layers normalization BatchNormalization \\n\\nmodel add Activation relu \\n\\nmodel add MaxPooling D pool size pool size \\n\\nmodel add Flatten \\n\\nmodel add Dense init he normal \\n\\nmodel add keras layers normalization BatchNormalization \\n\\nmodel add Dropout \\n\\nmodel add Activation relu \\n\\nmodel add Dense nb classes init he normal \\n\\nmodel add Activation softmax \\n\\nmodel compile loss categorical crossentropy optimizer adam metrics accuracy \\n',\n",
       " '\\nmodel summary \\n',\n",
       " '\\ntensorboard keras callbacks TensorBoard log dir tensorboard mnist name write graph True histogram freq \\n',\n",
       " '\\nhistory model fit X train Y train batch size nb epoch verbose validation data X val Y val callbacks tensorboard \\n',\n",
       " '\\nplt plot history history acc \\n\\nplt plot history history val acc \\n\\nplt title model accuracy \\n\\nplt ylabel accuracy \\n\\nplt xlabel epoch \\n\\nplt legend train valid loc lower right \\n\\nplt show \\n',\n",
       " '\\nplt plot history history loss \\n\\nplt plot history history val loss \\n\\nplt title model loss \\n\\nplt ylabel loss \\n\\nplt xlabel epoch \\n\\nplt legend train valid loc upper right \\n\\nplt show \\n',\n",
       " '\\nimport numpy as np\\n\\nimport pandas as pd\\n\\nfrom sklearn preprocessing import Imputer\\n\\nimport seaborn as sns\\n\\nimport matplotlib pyplot as plt\\n\\nget ipython run line magic matplotlib inline \\n\\nget ipython run line magic config InlineBackend figure format retina \\n\\nplt rcParams figure figsize \\n\\npd set option display max columns \\n',\n",
       " '\\ndf pd read csv bball assets RegularSeasonDetailedResults csv \\n',\n",
       " '\\ndf shape\\n',\n",
       " '\\ndf info \\n',\n",
       " '\\ndf head \\n',\n",
       " '\\ndf df copy \\n',\n",
       " '\\ndf columns tolist \\n',\n",
       " '\\ndf drop Wloc Numot Wfgm Wfgm Wftm Wor Wdr Wast Wto Wstl Wblk Wpf Lfgm Lfgm Lftm Lor Ldr Last Lto Lstl Lblk Lpf axis inplace True \\n',\n",
       " '\\ndf head \\n',\n",
       " '\\ndf Wfgas df Wfga df Wfga \\n',\n",
       " '\\ndf Lfgas df Lfga df Lfga \\n',\n",
       " '\\ndf head \\n',\n",
       " '\\ndf corr \\n',\n",
       " '\\nsns heatmap df corr annot True \\n',\n",
       " '\\ndf head \\n',\n",
       " '\\ndf Wfgas df Wfga df Wfga \\n',\n",
       " '\\ndf Lfgas df Lfga df Lfga \\n',\n",
       " '\\ndf WefgPct df Wfgm df Wfgm df Wfgas \\n',\n",
       " '\\ndf LefgPct df Lfgm df Lfgm df Lfgas \\n',\n",
       " '\\ndf WftPct df Wftm df Wfta \\n',\n",
       " '\\ndf LftPct df Lftm df Lfta \\n',\n",
       " '\\ndf S score df Wscore df Lscore \\n',\n",
       " '\\ndf S fgas df Wfgas df Lfgas \\n',\n",
       " '\\ndf S efgPct df WefgPct df LefgPct \\n',\n",
       " '\\ndf S ftPct df WftPct df LftPct \\n',\n",
       " '\\ndf S fgm df Wfgm df Lfgm \\n',\n",
       " '\\ndf S fga df Wfga df Lfga \\n',\n",
       " '\\ndf S fgm df Wfgm df Lfgm \\n',\n",
       " '\\ndf S fga df Wfga df Lfga \\n',\n",
       " '\\ndf S ftm df Wftm df Lftm \\n',\n",
       " '\\ndf S fta df Wfta df Lfta \\n',\n",
       " '\\ndf S or df Wor df Lor \\n',\n",
       " '\\ndf S dr df Wdr df Ldr \\n',\n",
       " '\\ndf S ast df Wast df Last \\n',\n",
       " '\\ndf S to df Wto df Lto \\n',\n",
       " '\\ndf S stl df Wstl df Lstl \\n',\n",
       " '\\ndf S blk df Wblk df Lblk \\n',\n",
       " '\\ndf S pf df Wpf df Lpf \\n',\n",
       " '\\ndf S Treb df Wor df Wdr df Lor df Ldr \\n',\n",
       " '\\ndf S NetTo df Wto df Wstl df Lto df Lstl \\n',\n",
       " '\\ndf head \\n',\n",
       " '\\ndf columns tolist \\n',\n",
       " '\\ndfS df S score S fgas S efgPct S ftPct S fgm S fga S fgm S fga S ftm S fta S or S dr S ast S to S stl S blk S pf S Treb S NetTo \\n',\n",
       " '\\ndfS S ftPct describe \\n',\n",
       " '\\ndfS info \\n',\n",
       " '\\ndfS S ftPct isnull sum \\n',\n",
       " '\\ninds pd isnull dfS any nonzero \\n',\n",
       " '\\ninds\\n',\n",
       " '\\ndfS info \\n',\n",
       " '\\ndfS dropna axis how any inplace True \\n',\n",
       " '\\ndfS info \\n',\n",
       " '\\nsns boxplot dfS S dr \\n',\n",
       " '\\nsns boxplot dfS S Treb \\n',\n",
       " '\\nsns boxplot dfS S ast \\n',\n",
       " '\\nsns boxplot dfS S NetTo \\n',\n",
       " '\\nsns violinplot dfS S ast \\n',\n",
       " '\\ndfS S ast hist \\n',\n",
       " '\\nimport pylab as pl\\n\\nimport pandas as pd\\n\\nimport numpy as np\\n\\nimport os\\n\\nget ipython run line magic pylab inline \\n\\nimport json\\n\\ns json load open os getenv PUI fbb matplotlibrc json \\n\\npl rcParams update s \\n',\n",
       " '\\npl hist np random randn \\n',\n",
       " '\\npl hist np random randn \\n\\npl hist np random randn \\n',\n",
       " '\\npl hist np random randn \\n\\npl hist np random randn alpha \\n',\n",
       " '\\npl hist np random randn \\n\\npl hist np random poisson alpha \\n',\n",
       " '\\npl hist np random randn \\n\\npl hist np random standard cauchy alpha \\n',\n",
       " '\\nimport numpy as np\\n\\n\\ndef fitness member \\n member np asarray member \\n if member ndim \\n n genes member shape\\n indiv fitnesses member sum axis \\n return indiv fitnesses mean \\n return member sum \\n',\n",
       " '\\n\\ndef produce offspring parents mutate False p \\n n parents n genes parents shape\\n gene to pull np random randint n parents size n genes \\n child parents gene to pull k k for k in range n genes \\n child np array child \\n if mutate \\n genes to flip np random choice size child shape p p p \\n i np argwhere genes to flip \\n child i child i \\n return child\\n',\n",
       " '\\nimport numpy as np\\n\\nimport matplotlib pyplot as plt\\n\\nimport glob\\n\\nimport pandas as pds\\n',\n",
       " '\\nfor folder in glob glob \\n inputsig pds read csv folder input txt sep header None \\n inputsig columns CH format i for i in range inputsig shape \\n outputsig pds read csv folder output f txt sep header None \\n outputsig columns Tstamp CH format i for i in range outputsig shape \\n f axarr plt subplots figsize \\n inputsig plot ax axarr \\n axarr legend loc upper right \\n outputsig filter regex CH plot ax axarr \\n axarr legend loc upper right \\n plt show \\n',\n",
       " '\\nfolder simulation \\n\\ninputsig pds read csv folder input txt sep header None \\n\\ninputsig columns CH format i for i in range inputsig shape \\n\\noutputsig pds read csv folder output f txt sep header None \\n\\noutputsig columns Tstamp CH format i for i in range outputsig shape \\n\\n f axarr plt subplots figsize \\n\\n outputsig filter regex CH inputsig plot ax axarr \\n\\naxarr legend loc upper right \\n\\n outputsig filter regex CH plot ax axarr \\n\\naxarr legend loc upper right \\n\\nplt show \\n',\n",
       " '\\nfrom pandas import Series DataFrame\\n\\nimport pandas as pd\\n',\n",
       " '\\nnew series Series \\n\\nnew series\\n',\n",
       " '\\nnew series values\\n',\n",
       " '\\nnew series index\\n',\n",
       " '\\nnew series Series index \\n',\n",
       " '\\nnew series \\n',\n",
       " '\\nnew series index\\n',\n",
       " '\\nnew series \\n',\n",
       " '\\nnew series \\n\\nnew series \\n',\n",
       " '\\nnew series new series \\n',\n",
       " '\\n new series \\n',\n",
       " '\\nimport numpy as np\\n\\nnp exp new series \\n',\n",
       " '\\n in new series \\n',\n",
       " '\\ndata \\n Data Science \\n Data Analyst \\n \\n\\ns Series data \\n\\ns \\n',\n",
       " '\\njobs Data Science Data Engineer Data Analyst \\n\\ns Series data index jobs \\n\\ns \\n',\n",
       " '\\ns isnull \\n',\n",
       " '\\ns notnull \\n',\n",
       " '\\n s s \\n',\n",
       " '\\ns name Jobs \\n\\ns index name Job Titles \\n',\n",
       " '\\ns \\n',\n",
       " '\\nimport os\\n\\nimport io\\n\\nimport numpy\\n\\nfrom pandas import DataFrame\\n\\nfrom sklearn feature extraction text import CountVectorizer\\n\\nfrom sklearn naive bayes import MultinomialNB\\n\\n\\ndef readFiles path \\n for root dirnames filenames in os walk path \\n for filename in filenames \\n path os path join root filename \\n inBody False\\n lines \\n f io open path r encoding latin \\n for line in f \\n if inBody \\n lines append line \\n elif line n \\n inBody True\\n f close \\n message n join lines \\n yield path message \\n\\n\\ndef dataFrameFromDirectory path classification \\n rows \\n index \\n for filename message in readFiles path \\n rows append \\n message message \\n class classification \\n \\n index append filename \\n return DataFrame rows index index \\n\\ndata DataFrame \\n message \\n class \\n \\n\\ndata data append dataFrameFromDirectory e sundog consult Udemy DataScience emails spam spam \\n\\ndata data append dataFrameFromDirectory e sundog consult Udemy DataScience emails ham ham \\n',\n",
       " '\\ndata head \\n',\n",
       " '\\nvectorizer CountVectorizer \\n\\ncounts vectorizer fit transform data message values \\n\\nclassifier MultinomialNB \\n\\ntargets data class values\\n\\nclassifier fit counts targets \\n',\n",
       " '\\nexamples Free Viagra now Hi Bob how about a game of golf tomorrow \\n\\nexample counts vectorizer transform examples \\n\\npredictions classifier predict example counts \\n\\npredictions\\n',\n",
       " '\\nimport pandas as pd\\n\\nimport numpy as np\\n\\nimport matplotlib pyplot as plt\\n\\nimport seaborn as sns\\n\\nfrom scipy cluster vq import kmeans\\n\\nget ipython run line magic matplotlib inline \\n',\n",
       " '\\ndata url https raw githubusercontent com Thinkful Ed curric data data sets master un un csv \\n\\nUN data pd read csv data url header \\n',\n",
       " '\\nprint The UN dataset contains d rows n len UN data \\n\\nfor col in UN data columns \\n print Column s has d null values col UN data col isnull sum \\n print Column s contains a s data type n col type UN data col \\n',\n",
       " '\\ndata grp lifeMale lifeFemale infantMortality GDPperCapita \\n\\nUN data UN data dropna subset data grp \\n\\nscore \\n\\nnum nodes \\n\\nfor code in range \\n centroids kmeans np array UN data data grp code \\n distance len UN data \\n ctr label len UN data \\n for k indx in enumerate UN data index \\n point UN data data grp loc indx values\\n for i center in enumerate centroids \\n target \\n for j ctr pt in enumerate center \\n target target pow point j ctr pt \\n target pow target \\n if target distance k \\n distance k target\\n ctr label k i\\n score append np mean distance \\n num nodes append code \\n\\nplt plot num nodes score \\n\\nplt scatter num nodes score c green marker o \\n\\nplt title Sum of Squares \\n\\nplt xlabel Average within cluster sum of squares \\n\\nplt ylabel Number of Nodes \\n\\nplt savefig Average within cluster sum of squares dpi \\n',\n",
       " '\\ncode \\n\\ncentroids kmeans np array UN data data grp code \\n\\ndistance len UN data \\n\\nctr label len UN data \\n\\nfor k indx in enumerate UN data index \\n point UN data data grp loc indx values\\n for i center in enumerate centroids \\n target \\n for j ctr pt in enumerate center \\n target target pow point j ctr pt \\n target pow target \\n if target distance k \\n distance k target\\n ctr label k i\\n\\nUN data distance distance\\n\\nUN data ctr label ctr label\\n\\nUN data ctr label UN data ctr label astype category \\n\\ncolor red green blue black \\n\\n fig ax plt subplots nrows ncols figsize \\n\\nfor j y axis in enumerate data grp \\n if y axis GDPperCapita \\n for i cat in enumerate UN data ctr label cat categories \\n ax j scatter UN data UN data ctr label cat GDPperCapita UN data UN data ctr label cat y axis c color i label cat \\n ax j set xlabel GDPperCapita \\n ax j set ylabel y axis \\n ax j legend \\n\\nfig savefig GDPperCapita dpi \\n',\n",
       " '\\nimport numpy as np\\n\\nimport matplotlib pyplot as plt\\n\\nget ipython run line magic matplotlib inline \\n\\nplt rcParams figure dpi \\n\\nnp set printoptions precision suppress True \\n\\nimport pandas as pd\\n\\nfrom sklearn model selection import train test split cross val score\\n\\nfrom sklearn pipeline import make pipeline\\n\\nfrom sklearn preprocessing import scale StandardScaler\\n',\n",
       " '\\nrng np random RandomState \\n\\nn samples \\n\\nn samples \\n\\nX syn np r rng randn n samples rng randn n samples \\n\\ny syn np array n samples n samples \\n\\n X syn train X syn test y syn train y syn test train test split X syn y syn \\n',\n",
       " '\\nimport openml\\n\\ndata openml datasets get dataset \\n\\n X y data get data target data default target attribute \\n',\n",
       " '\\nX shape\\n',\n",
       " '\\nnp bincount y \\n',\n",
       " '\\ndf pd DataFrame X \\n',\n",
       " '\\ndf hist bins auto \\n',\n",
       " '\\npd scatter matrix df c y alpha \\n',\n",
       " '\\n X train X test y train y test train test split X y stratify y random state \\n',\n",
       " '\\nfrom sklearn decomposition import PCA\\n\\npca PCA \\n\\nX train pca pca fit transform X train \\n\\nplt plot pca explained variance ratio \\n',\n",
       " '\\nsorting np argsort y train \\n\\n fig axes plt subplots \\n\\naxes scatter X train pca X train pca c y train alpha \\n\\naxes scatter X train pca sorting X train pca sorting c y train sorting alpha \\n',\n",
       " '\\nfrom sklearn preprocessing import RobustScaler\\n\\nrs RobustScaler fit X train \\n\\nX train scaled rs transform X train \\n',\n",
       " '\\npca scaled PCA \\n\\nX train pca scaled pca scaled fit transform X train scaled \\n\\nplt plot pca scaled explained variance ratio \\n',\n",
       " '\\n fig axes plt subplots \\n\\naxes scatter X train pca scaled X train pca scaled c y train alpha \\n\\naxes scatter X train pca scaled sorting X train pca scaled sorting c y train sorting alpha \\n',\n",
       " '\\nsorting np argsort y train \\n\\n fig axes plt subplots figsize \\n\\naxes scatter X train X train c y train alpha \\n\\naxes scatter X train sorting X train sorting c y train sorting alpha \\n\\naxes set title Feature vs random order \\n\\naxes set title Feature vs sorted \\n',\n",
       " '\\nfrom sklearn linear model import LogisticRegression\\n\\nscores cross val score LogisticRegression X train y train cv scoring roc auc \\n\\nprint scores mean \\n',\n",
       " '\\nfrom sklearn linear model import LogisticRegressionCV\\n\\nscores cross val score LogisticRegressionCV scoring roc auc X train y train cv scoring roc auc \\n\\nprint scores mean \\n',\n",
       " '\\nfrom imblearn under sampling import RandomUnderSampler\\n\\nrus RandomUnderSampler replacement False \\n\\n X train subsample y train subsample rus fit sample X train y train \\n\\nprint X train shape \\n\\nprint X train subsample shape \\n\\nprint np bincount y train subsample \\n',\n",
       " '\\nfrom imblearn pipeline import make pipeline as make imb pipeline\\n\\nundersample pipe make imb pipeline RandomUnderSampler LogisticRegressionCV \\n\\nscores cross val score undersample pipe X train y train cv scoring roc auc \\n\\nprint np mean scores \\n',\n",
       " '\\nfrom imblearn over sampling import RandomOverSampler\\n\\nros RandomOverSampler \\n\\n X train oversample y train oversample ros fit sample X train y train \\n\\nprint X train shape \\n\\nprint X train oversample shape \\n\\nprint np bincount y train oversample \\n',\n",
       " '\\noversample pipe make imb pipeline RandomOverSampler LogisticRegression \\n\\nscores cross val score oversample pipe X train y train cv scoring roc auc \\n\\nprint np mean scores \\n',\n",
       " '\\nfrom sklearn metrics import roc curve\\n\\noversample pipe fit X train y train \\n\\nprops oversample oversample pipe predict proba X test \\n\\n fpr over tpr over roc curve y test props oversample \\n\\nundersample pipe fit X train y train \\n\\nprops undersample undersample pipe predict proba X test \\n\\n fpr under tpr under roc curve y test props undersample \\n\\nlr LogisticRegression fit X train y train \\n\\nprops original lr predict proba X test \\n\\n fpr org tpr org roc curve y test props original \\n\\nplt plot fpr org tpr org label original \\n\\nplt plot fpr over tpr over label oversample \\n\\nplt plot fpr under tpr under label undersample \\n\\nplt legend \\n\\nplt xlabel FPR \\n\\nplt ylabel TPR \\n',\n",
       " '\\nfrom sklearn ensemble import RandomForestClassifier\\n\\nscores cross val score RandomForestClassifier n estimators X train y train cv scoring roc auc \\n\\nprint np mean scores \\n',\n",
       " '\\nundersample pipe rf make imb pipeline RandomUnderSampler RandomForestClassifier \\n\\nscores cross val score undersample pipe rf X train y train cv scoring roc auc \\n\\nprint np mean scores \\n',\n",
       " '\\noversample pipe rf make imb pipeline RandomOverSampler RandomForestClassifier \\n\\nscores cross val score oversample pipe rf X train y train cv scoring roc auc \\n\\nprint np mean scores \\n',\n",
       " '\\nfrom sklearn metrics import roc curve\\n\\noversample pipe rf fit X train y train \\n\\nprops oversample oversample pipe rf predict proba X test \\n\\n fpr over tpr over roc curve y test props oversample \\n\\nundersample pipe rf fit X train y train \\n\\nprops undersample undersample pipe rf predict proba X test \\n\\n fpr under tpr under roc curve y test props undersample \\n\\nrf RandomForestClassifier n estimators fit X train y train \\n\\nprops original rf predict proba X test \\n\\n fpr org tpr org roc curve y test props original \\n\\nplt plot fpr org tpr org label original \\n\\nplt plot fpr over tpr over label oversample \\n\\nplt plot fpr under tpr under label undersample \\n\\nplt legend \\n\\nplt xlabel FPR \\n\\nplt ylabel TPR \\n\\nplt title RF comparison \\n',\n",
       " '\\nfrom sklearn linear model import LogisticRegression\\n\\nscores cross val score LogisticRegression class weight balanced X train y train cv scoring roc auc \\n\\nprint scores mean \\n',\n",
       " '\\nfrom sklearn ensemble import RandomForestClassifier\\n\\nscores cross val score RandomForestClassifier n estimators class weight balanced X train y train cv scoring roc auc \\n\\nprint np mean scores \\n',\n",
       " '\\nfrom sklearn ensemble import VotingClassifier\\n\\nfrom sklearn tree import DecisionTreeClassifier\\n\\n\\ndef make resample tree random state \\n tree make imb pipeline RandomUnderSampler random state random state replacement True DecisionTreeClassifier max features auto random state random state \\n return tree i format random state tree \\n\\nclassifiers make resample tree i for i in range \\n\\nresampled rf VotingClassifier classifiers voting soft \\n',\n",
       " '\\nscores cross val score resampled rf X train y train cv scoring roc auc \\n\\nprint np mean scores \\n',\n",
       " '\\nfrom sklearn base import clone\\n\\n\\ndef make resampled ensemble estimator n estimators \\n estimators \\n for i in range n estimators \\n est clone estimator \\n if hasattr est random state \\n est random state i\\n pipe make imb pipeline RandomUnderSampler random state i replacement True est \\n estimators append est i format i pipe \\n return VotingClassifier estimators voting soft \\n',\n",
       " '\\nresampled tree test make resampled ensemble DecisionTreeClassifier max features auto \\n\\nscores cross val score resampled tree test X train y train cv scoring roc auc \\n\\nprint np mean scores \\n',\n",
       " '\\nresampled lr make resampled ensemble LogisticRegression \\n\\nscores cross val score resampled lr X train y train cv scoring roc auc \\n\\nprint np mean scores \\n',\n",
       " '\\nfrom imblearn under sampling import EditedNearestNeighbours\\n\\nenn EditedNearestNeighbours n neighbors \\n\\n X train enn y train enn enn fit sample X train y train \\n\\nenn mode EditedNearestNeighbours kind sel mode n neighbors \\n\\n X train enn mode y train enn mode enn mode fit sample X train y train \\n',\n",
       " '\\n fig axes plt subplots figsize \\n\\ndatasets X train y train X train enn mode y train enn mode X train enn y train enn \\n\\nfor ax X y in zip axes datasets \\n sorting np argsort y \\n ax scatter X sorting X sorting c plt cm Vega y sorting alpha \\n ax set xlim \\n ax set ylim \\n ax set title np bincount y \\n',\n",
       " '\\nenn pipe make imb pipeline EditedNearestNeighbours n neighbors LogisticRegression \\n\\nscores cross val score enn pipe X train y train cv scoring roc auc \\n\\nprint np mean scores \\n',\n",
       " '\\nenn pipe rf make imb pipeline EditedNearestNeighbours n neighbors RandomForestClassifier n estimators \\n\\nscores cross val score enn pipe rf X train y train cv scoring roc auc \\n\\nprint np mean scores \\n',\n",
       " '\\nfrom sklearn model selection import GridSearchCV\\n\\nparam grid \\n editednearestneighbours n neighbors \\n editednearestneighbours kind sel mode all \\n \\n\\nsearch GridSearchCV enn pipe param grid cv scoring roc auc \\n\\nsearch fit X train y train \\n',\n",
       " '\\nresults pd DataFrame search cv results \\n\\nres pivot results pivot table values mean test score mean train score columns param editednearestneighbours kind sel index param editednearestneighbours n neighbors \\n',\n",
       " '\\nres pivot plot \\n',\n",
       " '\\nfrom sklearn model selection import GridSearchCV\\n\\nparam grid \\n editednearestneighbours n neighbors \\n editednearestneighbours kind sel mode all \\n \\n\\nsearch GridSearchCV enn pipe rf param grid cv scoring roc auc \\n\\nsearch fit X train y train \\n',\n",
       " '\\nresults pd DataFrame search cv results \\n\\nres pivot results pivot table values mean test score mean train score columns param editednearestneighbours kind sel index param editednearestneighbours n neighbors \\n',\n",
       " '\\nres pivot plot \\n',\n",
       " '\\nsearch best score \\n',\n",
       " '\\n n nTomek links n nAn illustration of the Tomek links method n \\n\\nimport numpy as np\\n\\nimport matplotlib pyplot as plt\\n\\nfrom sklearn datasets import make blobs\\n\\nfrom imblearn under sampling import TomekLinks CondensedNearestNeighbour\\n\\nrng np random RandomState \\n\\nn samples \\n\\nn samples \\n\\nX syn np r rng randn n samples rng randn n samples \\n\\ny syn np array n samples n samples \\n\\n X syn y syn shuffle X syn y syn \\n\\n X syn train X syn test y syn train y syn test train test split X syn y syn \\n\\ntl TomekLinks return indices True \\n\\ntl CondensedNearestNeighbour return indices True \\n\\n X resampled y resampled idx resampled tl fit sample X syn y syn \\n\\n fig axes plt subplots \\n\\nplt title Under sampling removing Tomek links \\n\\naxes scatter X syn X syn c y syn \\n\\naxes scatter X resampled X resampled c y resampled \\n',\n",
       " '\\nidx resampled\\n',\n",
       " '\\nfrom imblearn under sampling import TomekLinks CondensedNearestNeighbour\\n\\nrng np random RandomState \\n\\nn samples \\n\\nn samples \\n\\nX syn np r rng randn n samples rng randn n samples \\n\\ny syn np array n samples n samples \\n\\n X syn y syn shuffle X syn y syn \\n\\n X syn train X syn test y syn train y syn test train test split X syn y syn \\n\\ntl TomekLinks return indices True \\n\\n X resampled y resampled idx resampled tl fit sample X syn y syn \\n\\n\\ndef plot resampled X org y org X res y res idx ax None \\n if ax is None \\n ax plt gca \\n idx samples removed np setdiff d np arange X org shape idx \\n idx class y res \\n ax scatter X res idx class X res idx class c g alpha label Class \\n ax scatter X res idx class X res idx class c b alpha label Class \\n ax scatter X org idx samples removed X org idx samples removed c g alpha s label Samples removed from Class \\n ax legend \\n\\nplot resampled X syn y syn X resampled y resampled idx resampled \\n\\nplt title Under sampling removing Tomek links \\n\\nplt show \\n',\n",
       " '\\ntl TomekLinks return indices True \\n\\n X resampled y resampled idx resampled tl fit sample X syn y syn \\n\\nnp bincount y resampled \\n',\n",
       " '\\ncnn CondensedNearestNeighbour return indices True \\n\\n X resampled y resampled idx resampled cnn fit sample X syn y syn \\n\\nplot resampled X syn y syn X resampled y resampled idx resampled \\n\\nplt title Condensed Nearest Neighbor \\n',\n",
       " '\\nenn EditedNearestNeighbours return indices True \\n\\n X resampled y resampled idx resampled enn fit sample X syn y syn \\n\\nplot resampled X syn y syn X resampled y resampled idx resampled \\n\\nplt title Edited Nearest Neighbor \\n',\n",
       " '\\n fig ax plt subplots figsize \\n\\nenn EditedNearestNeighbours return indices True \\n\\n X resampled y resampled idx resampled enn fit sample X syn y syn \\n\\nplot resampled X syn y syn X resampled y resampled idx resampled ax ax \\n\\nax set title Edited Nearest Neighbor \\n\\n X resampled y resampled idx resampled CondensedNearestNeighbour return indices True fit sample X syn y syn \\n\\nplot resampled X syn y syn X resampled y resampled idx resampled ax ax \\n\\nax set title Condensed Nearest Neighbor \\n',\n",
       " '\\ncnn CondensedNearestNeighbour \\n\\n X train cnn y train cnn cnn fit sample X train y train \\n\\nprint X train cnn shape \\n\\nprint np bincount y train cnn \\n',\n",
       " '\\n fig axes plt subplots figsize \\n\\nsorting np argsort y train \\n\\nsorting cnn np argsort y train cnn \\n\\naxes scatter X train sorting X train sorting c y train sorting alpha \\n\\naxes scatter X train cnn sorting cnn X train cnn sorting cnn c y train cnn sorting cnn alpha \\n',\n",
       " '\\ncnn pipe make imb pipeline CondensedNearestNeighbour LogisticRegression \\n\\nscores cross val score cnn pipe X train y train cv scoring roc auc \\n\\nprint np mean scores \\n',\n",
       " '\\ncnn pipe make imb pipeline CondensedNearestNeighbour RandomForestClassifier n estimators \\n\\nscores cross val score cnn pipe X train y train cv scoring roc auc \\n\\nprint np mean scores \\n',\n",
       " '\\nfrom imblearn over sampling import SMOTE\\n\\nsmote SMOTE \\n\\n X train smote y train smote smote fit sample X train y train \\n\\nprint X train smote shape \\n\\nprint np bincount y train smote \\n',\n",
       " '\\n X resampled y resampled SMOTE fit sample X syn y syn \\n\\n fig axes plt subplots \\n\\naxes scatter X syn X syn c plt cm Vega y syn alpha \\n\\naxes scatter X resampled X resampled c plt cm Vega y resampled alpha \\n',\n",
       " '\\n fig axes plt subplots figsize \\n\\nsorting np argsort y train \\n\\naxes scatter X train sorting X train sorting c y train sorting alpha \\n\\naxes scatter X train smote X train smote c y train smote alpha \\n',\n",
       " '\\nfrom sklearn utils import shuffle\\n\\n fig axes plt subplots figsize \\n\\n X smote sh y smote sh shuffle X train smote y train smote \\n\\naxes scatter X train X train c y train alpha \\n\\naxes scatter X smote sh X smote sh c y smote sh alpha \\n',\n",
       " '\\nsmote pipe make imb pipeline SMOTE LogisticRegression \\n\\nscores cross val score smote pipe X train y train cv scoring roc auc \\n\\nprint np mean scores \\n',\n",
       " '\\nsmote pipe rf make imb pipeline SMOTE RandomForestClassifier n estimators \\n\\nscores cross val score smote pipe rf X train y train cv scoring roc auc \\n\\nprint np mean scores \\n',\n",
       " '\\nparam grid \\n smote k neighbors \\n \\n\\nsearch GridSearchCV smote pipe rf param grid cv scoring roc auc \\n\\nsearch fit X train y train \\n',\n",
       " '\\nsearch best score \\n',\n",
       " '\\nresults pd DataFrame search cv results \\n\\nresults plot param smote k neighbors mean test score mean train score \\n',\n",
       " '\\nsmote pipe rf make imb pipeline SMOTE k neighbors RandomForestClassifier n estimators \\n\\nscores cross val score smote pipe rf X train y train cv scoring roc auc \\n\\nprint np mean scores \\n',\n",
       " '\\nfrom imblearn over sampling import SMOTE\\n\\nsmote SMOTE k neighbors \\n\\n X train smote y train smote smote fit sample X train y train \\n\\n fig axes plt subplots figsize \\n\\n X smote sh y smote sh shuffle X train smote y train smote \\n\\naxes scatter X smote sh X smote sh c y smote sh alpha \\n\\naxes scatter X smote sh X smote sh c y smote sh alpha \\n\\naxes set title SMOTE k neighbors \\n\\naxes set title SMOTE k neighbors \\n',\n",
       " '\\nfrom imblearn combine import SMOTEENN SMOTETomek\\n\\nsmoteenn pipe rf make imb pipeline SMOTEENN smote SMOTE k neighbors RandomForestClassifier n estimators \\n\\nscores cross val score smoteenn pipe rf X train y train cv scoring roc auc \\n\\nprint np mean scores \\n',\n",
       " '\\nsmoteenn SMOTEENN smote SMOTE k neighbors \\n\\n X train smoteenn y train smoteenn smoteenn fit sample X train y train \\n\\n fig axes plt subplots figsize \\n\\n X smote shenn y smote shenn shuffle X train smoteenn y train smoteenn \\n\\naxes scatter X smote sh X smote sh c y smote sh alpha \\n\\naxes scatter X smote shenn X smote shenn c y smote shenn alpha \\n\\naxes set title SMOTE \\n\\naxes set title SMOTE ENN \\n',\n",
       " '\\nnp bincount y train smote \\n',\n",
       " '\\nnp bincount y train smoteenn \\n',\n",
       " '\\n X resampled y resampled SMOTE fit sample X syn y syn \\n\\n X resampled enn y resampled enn SMOTEENN fit sample X syn y syn \\n\\n X resampled cnn y resampled cnn make imb pipeline SMOTE CondensedNearestNeighbour fit sample X syn y syn \\n\\n fig axes plt subplots \\n\\naxes scatter X resampled X resampled c plt cm Vega y resampled alpha \\n\\naxes set title SMOTE \\n\\naxes scatter X resampled enn X resampled enn c plt cm Vega y resampled enn alpha \\n\\naxes set title SMOTE ENN \\n\\naxes scatter X resampled cnn X resampled cnn c plt cm Vega y resampled cnn alpha \\n\\naxes set title SMOTE CNN \\n',\n",
       " '\\nimport os\\n\\nimport math\\n\\nimport numpy as np\\n\\nimport pandas as pd\\n\\nget ipython run line magic matplotlib inline \\n\\nimport matplotlib pyplot as plt\\n\\nfrom sklearn import feature extraction ensemble cross validation metrics\\n\\npd set option display max rows \\n\\npd set option display notebook repr html True \\n\\npd set option display max columns \\n\\nplt style use ggplot \\n',\n",
       " '\\nreviews \\n\\nsentiments \\n\\nwith open os path join datasets amazon reviews txt as f \\n for line in f readlines \\n line line strip n \\n review sentiment line split t \\n sentiment np nan if sentiment else int sentiment \\n reviews append review \\n sentiments append sentiment \\n\\ndf pd DataFrame \\n review reviews \\n sentiment sentiments \\n \\n',\n",
       " '\\ndf\\n',\n",
       " '\\ndf dropna inplace True \\n',\n",
       " '\\ndf head \\n',\n",
       " '\\nX df review\\n\\ny df sentiment\\n',\n",
       " '\\n train X test X train y test y cross validation train test split X y test size \\n',\n",
       " '\\ntrain X\\n',\n",
       " '\\nvectorizer feature extraction text CountVectorizer stop words english \\n',\n",
       " '\\nvectorizer\\n',\n",
       " '\\nvectorizer fit train X \\n',\n",
       " '\\nvectorizer get feature names \\n',\n",
       " '\\ntrain X transformed vectorizer transform train X \\n\\ntest X transformed vectorizer transform test X \\n',\n",
       " '\\ntrain X transformed\\n',\n",
       " '\\ntrain X transformed todense \\n',\n",
       " '\\nmodel ensemble RandomForestClassifier n estimators \\n\\ncross validation cross val score model train X transformed train y scoring roc auc \\n',\n",
       " '\\nmodel fit train X transformed train y \\n',\n",
       " '\\nmodel score train X transformed train y \\n',\n",
       " '\\ntrain y hat model predict train X transformed \\n\\n fpr tpr thresholds metrics roc curve train y train y hat \\n\\nplt figure \\n\\nplt plot fpr tpr label ROC curve area f metrics auc fpr tpr \\n\\nplt plot k \\n\\nplt xlim \\n\\nplt ylim \\n\\nplt xlabel FPR Fall out \\n\\nplt ylabel TPR Sensitivity \\n\\nplt title Testing Sentiment ROC \\n\\nplt legend loc lower right \\n\\nplt show \\n',\n",
       " '\\nmodel score test X transformed test y \\n',\n",
       " '\\ntest y hat model predict test X transformed \\n\\n fpr tpr thresholds metrics roc curve test y test y hat \\n\\nplt figure \\n\\nplt plot fpr tpr label ROC curve area f metrics auc fpr tpr \\n\\nplt plot k \\n\\nplt xlim \\n\\nplt ylim \\n\\nplt xlabel FPR Fall out \\n\\nplt ylabel TPR Sensitivity \\n\\nplt title Training Sentiment ROC \\n\\nplt legend loc lower right \\n\\nplt show \\n',\n",
       " '\\nimportance pd DataFrame \\n words vectorizer get feature names \\n importance model feature importances \\n \\n\\nimportance importance importance sort values by importance tail \\n',\n",
       " '\\nimport bokeh\\n',\n",
       " '\\nfrom bokeh plotting import figure output file show\\n\\nfrom bokeh charts import Bar output notebook show\\n',\n",
       " '\\noutput notebook \\n',\n",
       " '\\noutput file Bokeh Grafico Interativo html \\n',\n",
       " '\\np figure \\n',\n",
       " '\\ntype p \\n',\n",
       " '\\np line line width \\n',\n",
       " '\\nshow p \\n',\n",
       " '\\nfrom bokeh charts import Bar output notebook show\\n',\n",
       " '\\ndata \\n y \\n \\n',\n",
       " '\\noutput notebook \\n',\n",
       " '\\ngr Bar data title Gr fico Bokeh xlabel x ylabel valores width height \\n',\n",
       " '\\nshow gr \\n',\n",
       " '\\nfrom bokeh charts import Scatter output file show\\n\\nfrom bokeh sampledata autompg import autompg as df\\n\\np Scatter df x mpg y hp color cyl title Consumo x Pot ncia legend top right xlabel Km Litro ylabel Pot ncia do Motor \\n\\noutput file Bokeh Chart Interativo html \\n\\nshow p \\n',\n",
       " '\\nimport seaborn as sea\\n\\nfrom bokeh import mpl\\n\\nfrom bokeh plotting import output file show\\n\\ntips sea load dataset tips \\n\\nsea set style whitegrid \\n\\nax sea violinplot x day y total bill hue sex data tips palette Set split True scale count inner stick \\n\\noutput file Bokeh ViolinPlot html \\n\\nshow mpl to bokeh \\n',\n",
       " '\\nfrom bokeh plotting import figure output file show\\n\\noutput file Bokeh Grafico Linha html \\n\\np figure plot width plot height \\n\\np circle size color navy alpha \\n\\nshow p \\n',\n",
       " '\\nfrom bokeh io import output file show\\n\\nfrom bokeh models import GeoJSONDataSource\\n\\nfrom bokeh plotting import figure\\n\\nfrom bokeh sampledata sample geojson import geojson\\n\\ngeo source GeoJSONDataSource geojson geojson \\n\\np figure \\n\\np circle x x y y alpha source geo source \\n\\noutput file Bokeh GeoJSON html \\n\\nshow p \\n',\n",
       " '\\nimport pandas as pd\\n\\nfrom bokeh plotting import figure output file show\\n\\nAAPL pd read csv http ichart yahoo com table csv s AAPL a b c d e f parse dates Date \\n\\noutput file Bokeh Datetime html \\n\\np figure width height x axis type datetime \\n\\np line AAPL Date AAPL Close color navy alpha \\n\\nshow p \\n',\n",
       " '\\nfrom bokeh io import output file show\\n\\nfrom bokeh models import GMapPlot GMapOptions ColumnDataSource Circle DataRange d PanTool WheelZoomTool BoxSelectTool\\n\\nmap options GMapOptions lat lng map type roadmap zoom \\n\\nplot GMapPlot x range DataRange d y range DataRange d map options map options title Austin \\n\\nsource ColumnDataSource data dict lat lon \\n\\ncircle Circle x lon y lat size fill color blue fill alpha line color None \\n\\nplot add glyph source circle \\n\\nplot add tools PanTool WheelZoomTool BoxSelectTool \\n\\noutput file gmap plot html \\n\\nshow plot \\n',\n",
       " '\\nimport pandas as pd\\n\\nimport matplotlib pyplot as plt\\n\\nget ipython run line magic matplotlib inline \\n\\nfrom hw b import load data LogisticRegression HiddenLayer MLP\\n',\n",
       " '\\ndf pd read csv log analysis csv \\n',\n",
       " '\\nidx b df part b \\n\\nidx b df part b \\n\\nidx b df part b \\n',\n",
       " '\\ndf loc idx b parity df layers map \\n even \\n odd \\n \\n\\ndf loc idx b tot nodes df layers df nodes \\n',\n",
       " '\\ndf b df idx b set index layers nodes parity sort index \\n\\ndf b df idx b set index layers tot nodes sort index \\n\\ndf b df idx b set index nodes layers sort index \\n',\n",
       " '\\ns df b test unstack nodes parity \\n\\ns s fillna method ffill s fillna method bfill \\n\\nxticks range \\n\\nax plt gca \\n\\nax set color cycle blue green red magenta \\n\\nax s plot ax ax xticks xticks linestyle \\n\\nax s plot ax ax xticks xticks marker o linestyle \\n\\nax set title Fig \\n\\ns fillna \\n',\n",
       " '\\ns df b test unstack tot nodes \\n\\nxticks range \\n\\nax plt gca \\n\\nax set color cycle blue green \\n\\nax s plot ax ax xticks xticks marker o linestyle \\n\\nax set title Fig \\n\\ns fillna \\n',\n",
       " '\\ns df b test unstack layers \\n\\nax plt gca \\n\\nax set color cycle blue green \\n\\nxticks pd Series range \\n\\nax s plot ax ax marker o linestyle xticks xticks \\n\\nax set ybound lower upper \\n\\nax set title Fig \\n\\n s fillna astype int \\n',\n",
       " '\\nfrom hw c import load data LogisticRegression DropoutHiddenLayer\\n',\n",
       " '\\nimport gensim\\n\\nfrom gensim import corpora models\\n\\nimport langdetect\\n\\nfrom langdetect import detect langs\\n\\nfrom nltk stem porter import PorterStemmer\\n\\nimport sklearn feature extraction text as text\\n\\nfrom sklearn import decomposition\\n\\nfrom stop words import get stop words\\n\\nimport os\\n\\nimport numpy as np\\n',\n",
       " '\\nCORPUS PATH os path join data output es \\n\\nprint CORPUS PATH \\n\\nfilenames sorted os path join CORPUS PATH fn for fn in os listdir CORPUS PATH \\n',\n",
       " '\\nlen filenames \\n\\nfilenames \\n',\n",
       " '\\nwith open filenames as f \\n fdata line rstrip for line in f \\n g fdata \\n lang detect langs g decode utf \\n\\nprint lang \\n',\n",
       " '\\nl str lang \\n\\nlang l rstrip split \\n\\nlang stop get stop words lang \\n\\nmore stopwords online www http espa a gratis tienda decode utf \\n\\nlang stop more stopwords split \\n\\nvectorizer text CountVectorizer input filename stop words lang stop min df \\n\\ndtm vectorizer fit transform filenames toarray \\n\\nvocab np array vectorizer get feature names \\n\\nprint lang stop \\n',\n",
       " '\\nnum topics \\n\\nnum top words \\n\\nclf decomposition NMF n components num topics random state \\n\\ndoctopic clf fit transform dtm \\n\\np stemmer PorterStemmer \\n\\ntopic words \\n\\nfor topic in clf components \\n word idx np argsort topic num top words \\n topic words append p stemmer stem vocab i encode utf for i in word idx \\n\\nprint topic words \\n',\n",
       " '\\ndoctopic doctopic np sum doctopic axis keepdims True \\n',\n",
       " '\\ndoc names \\n\\nfor fn in filenames \\n basename os path basename fn \\n name ext os path splitext basename \\n name name rstrip \\n doc names append name \\n\\ndoc names np asarray doc names \\n\\ndoctopic orig doctopic copy \\n\\nnum groups len set doc names \\n\\ndoctopic grouped np zeros num groups num topics \\n\\nfor i name in enumerate sorted set doc names \\n doctopic grouped i np mean doctopic doc names name axis \\n\\ndoctopic doctopic grouped\\n',\n",
       " '\\ndocs sorted set doc names \\n\\nprint Top NMF topics in \\n\\nfor i in range len doctopic \\n top topics np argsort doctopic i \\n top topics str join str t for t in top topics \\n print format docs i top topics str \\n',\n",
       " '\\nfor t in range len topic words \\n print Topic format t join topic words t \\n',\n",
       " '\\n video indices noticia indices \\n\\nfor index fn in enumerate sorted set doc names \\n if video in fn \\n video indices append index \\n elif noticia in fn \\n noticia indices append index \\n\\nvideo avg np mean doctopic video indices axis \\n\\nnoticia avg np mean doctopic noticia indices axis \\n\\nkeyness np abs video avg noticia avg \\n\\nranking np argsort keyness \\n\\nranking \\n',\n",
       " '\\nget ipython run line magic matplotlib inline \\n\\nimport matplotlib pyplot as plt\\n\\n N K doctopic shape\\n\\nind np arange N \\n\\nwidth \\n\\nplt bar ind doctopic width width \\n\\nplt xticks ind width docs \\n\\nplt title Share of Topic \\n',\n",
       " '\\nget ipython run line magic matplotlib inline \\n\\nimport numpy as np\\n\\nfrom sklearn import datasets\\n\\nimport matplotlib pyplot as plt\\n\\n\\ndef generate data NTrials \\n n generate data n return X input data y given labels n \\n np random seed \\n X y datasets make moons NTrials noise \\n return X y \\n\\n\\ndef plot decision boundary pred func X y \\n n plot the decision boundary n param pred func function used to predict the label n param X input data n param y given labels n return n \\n x min x max X min X max \\n y min y max X min X max \\n h \\n xx yy np meshgrid np arange x min x max h np arange y min y max h \\n Z pred func np c xx ravel yy ravel \\n Z Z reshape xx shape \\n plt contourf xx yy Z cmap plt cm Spectral \\n plt scatter X X s c y cmap plt cm Spectral \\n plt show \\n\\n\\nclass NeuralNetwork object \\n n This class builds and trains a neural network n \\n\\n def init self nn input dim nn hidden dim nn output dim actFun type tanh reg lambda seed \\n n param nn input dim input dimension n param nn hidden dim the number of hidden units n param nn output dim output dimension n param actFun type type of activation function options tanh sigmoid relu n param reg lambda regularization coefficient n param seed random seed n \\n self nn input dim nn input dim\\n self nn hidden dim nn hidden dim\\n self nn output dim nn output dim\\n self actFun type actFun type\\n self reg lambda reg lambda\\n np random seed seed \\n self W np random randn self nn input dim self nn hidden dim np sqrt self nn input dim \\n self b np zeros self nn hidden dim \\n self W np random randn self nn hidden dim self nn output dim np sqrt self nn hidden dim \\n self b np zeros self nn output dim \\n\\n def actFun self z type \\n n actFun computes the activation functions n param z net input n param type Tanh Sigmoid or ReLU n return activations n \\n if type tanh \\n return np tanh z \\n elif type sigmoid \\n return np exp z \\n elif type relu \\n return np maximum z \\n\\n def diff actFun self z type \\n n diff actFun computes the derivatives of the activation functions wrt the net input n param z net input n param type Tanh Sigmoid or ReLU n return the derivatives of the activation functions wrt the net input n \\n if type tanh \\n return pow self actFun z type \\n elif type sigmoid \\n return np exp z np exp z \\n elif type relu \\n return np greater z astype float \\n\\n def feedforward self X actFun \\n n feedforward builds a layer neural network and computes the two probabilities n one for class and one for class n param X input data n param actFun activation function n return n \\n self z np dot X self W self b \\n self a self actFun self z self actFun type \\n self z np dot self a self W self b \\n exp scores np exp self z \\n self probs exp scores np sum exp scores axis keepdims True \\n return None\\n\\n def calculate loss self X y \\n n calculate loss computes the loss for prediction n param X input data n param y given labels n return the loss for prediction n \\n num examples len X \\n self feedforward X lambda x self actFun x type self actFun type \\n yhat self probs range num examples y \\n log yhat np log yhat \\n data loss np sum y log yhat \\n data loss self reg lambda np sum np square self W np sum np square self W \\n return num examples data loss \\n\\n def predict self X \\n n predict infers the label of a given data point X n param X input data n return label inferred n \\n self feedforward X lambda x self actFun x type self actFun type \\n return np argmax self probs axis \\n\\n def backprop self X y \\n n implements backpropagation n param X input data n param y given labels n return dL dW dL b dL dW dL db n \\n num examples len X \\n delta self probs\\n delta range num examples y \\n dW np dot self a T delta \\n db np sum delta \\n first arg np dot delta self W T \\n second arg np multiply first arg self diff actFun self z self actFun type \\n dW np dot X T second arg \\n db np sum second arg \\n return dW dW db db \\n\\n def fit model self X y epsilon num passes print loss True \\n n fit model uses backpropagation to train the network n param X input data n param y given labels n param num passes the number of times that the algorithm runs through the whole dataset n param print loss print the loss or not n return n \\n for i in range num passes \\n self feedforward X lambda x self actFun x type self actFun type \\n dW dW db db self backprop X y \\n dW self reg lambda self W \\n dW self reg lambda self W \\n self W epsilon dW \\n self b epsilon db \\n self W epsilon dW \\n self b epsilon db \\n if print loss and i \\n print Loss after iteration i f i self calculate loss X y \\n\\n def visualize decision boundary self X y \\n n visualize decision boundary plots the decision boundary created by the trained network n param X input data n param y given labels n return n \\n plot decision boundary lambda x self predict x X y \\n\\n\\ndef main \\n X y generate data \\n model NeuralNetwork nn input dim nn hidden dim nn output dim actFun type tanh \\n model fit model X y \\n model visualize decision boundary X y \\n\\nif name main \\n main \\n',\n",
       " '\\nimport pandas as pd\\n\\nimport numpy as np\\n\\nimport math\\n\\nimport matplotlib pyplot as plt\\n\\nget ipython run line magic matplotlib inline \\n\\nfrom datetime import datetime timedelta\\n\\nfrom utils import qualityRMSE qualityMedianAE qualityMACAPE\\n\\nfrom utils import SimpleExponentialSmoothing AdaptiveExponentialSmoothing\\n',\n",
       " '\\n\\ndef AdaptiveSelection x h Params \\n n Parameters n x array time series n h integer scalar forecasting delay n Params dict dictionary with n gamma scalar in smoothing parameter of error n eps scalar bound for best indistinctive models n BaseAlgs array of dict with params n BaseAlg string name of base algorithm n BaseAlfParams dict dictionary of base algorithm s params n \\n T len x \\n FORECAST np NaN T h \\n BaseAlgs Params BaseAlgsParams \\n N len BaseAlgs \\n FORECAST BA np array np NAN T h N reshape N T h \\n for ba in range len BaseAlgs \\n FORECAST BA ba eval BaseAlgs ba BaseAlg x h BaseAlgs str ba BaseAlgParams \\n gamma Params gamma \\n eps Params eps \\n if gamma \\n w warn Gamma can not be more than \\n return FORECAST\\n if gamma \\n w warn Gamma can not be less than \\n return FORECAST\\n e np zeros N \\n j best np zeros N \\n y \\n for t in range T \\n if not math isnan x t \\n if t h \\n ee np abs x t FORECAST BA transpose t \\n if not np any np isnan ee \\n e gamma ee gamma e \\n if not np all np isnan e \\n j best np nanargmin e \\n idx bestinsdistinctive np where e e j best eps \\n y FORECAST BA idx bestinsdistinctive t mean \\n FORECAST t h y\\n return FORECAST\\n',\n",
       " '\\n\\ndef AdaptiveCombination x h Params \\n n Parameters n x array time series n h integer scalar forecasting delay n Params dict dictionary with n gamma scalar in smoothing parameter of error n BaseAlgs array of dict with params n BaseAlg string name of base algorithm n BaseAlfParams dict dictionary of base algorithm s params n \\n T len x \\n FORECAST np NaN T h \\n BaseAlgs Params BaseAlgsParams \\n N len BaseAlgs \\n FORECAST BA np array np NAN T h N reshape N T h \\n for ba in range N \\n FORECAST BA ba eval BaseAlgs ba BaseAlg x h BaseAlgs str ba BaseAlgParams \\n gamma Params gamma \\n if gamma \\n w warn Gamma can not be more than \\n return FORECAST\\n if gamma \\n w warn Gamma can not be less than \\n return FORECAST\\n e np zeros N \\n weights np zeros N \\n y \\n for t in range T \\n if not math isnan x t \\n if t h \\n ee np abs x t FORECAST BA transpose t \\n if not np any np isnan ee \\n e gamma ee gamma e \\n if not np all np isnan e \\n w best e np sum e \\n y np dot w best FORECAST BA T t \\n FORECAST t h y\\n return FORECAST\\n',\n",
       " '\\nBaseAlgs \\n BaseAlg SimpleExponentialSmoothing \\n BaseAlgParams \\n alpha \\n AdaptationPeriod \\n \\n \\n BaseAlg AdaptiveExponentialSmoothing \\n BaseAlgParams \\n alpha \\n gamma \\n AdaptationPeriod \\n \\n \\n',\n",
       " '\\nfor ba in range len BaseAlgs \\n print BaseAlgs ba BaseAlg x h BaseAlgs str ba BaseAlgsParams \\n',\n",
       " '\\nts pd read csv data TimeSeries Data csv sep decimal parse dates True dayfirst True index col Dates \\n\\nts index names Timestamp \\n\\nts ts sort index \\n',\n",
       " '\\nh \\n\\nfrc ts pd DataFrame index ts index append pd date range ts index timedelta ts index timedelta h columns ts columns \\n\\nFRC TS dict \\n',\n",
       " '\\ngamma \\n\\neps \\n\\nfor cntr in ts columns \\n frc ts cntr AdaptiveSelection ts cntr h \\n gamma gamma \\n eps eps \\n BaseAlgsParams BaseAlgs \\n \\n\\nFRC TS AS gamma f eps f gamma eps frc ts\\n',\n",
       " '\\nfor cntr in frc ts columns \\n plt figure figsize \\n plt plot frc ts cntr label forecast \\n plt plot ts cntr label ts \\n plt legend fontsize \\n plt title cntr \\n plt show \\n',\n",
       " '\\ngamma \\n\\nfor cntr in ts columns \\n frc ts cntr AdaptiveCombination ts cntr h \\n gamma gamma \\n BaseAlgsParams BaseAlgs \\n \\n\\nFRC TS AC gamma f gamma frc ts\\n',\n",
       " '\\nfor cntr in frc ts columns \\n plt figure figsize \\n plt plot frc ts cntr label forecast \\n plt plot ts cntr label ts \\n plt legend fontsize \\n plt title cntr \\n plt show \\n',\n",
       " '\\nix range \\n\\nQualityStr pd DataFrame index ts columns columns sorted FRC TS keys \\n\\nfor model in QualityStr columns \\n frc ts FRC TS model \\n for ts num in ts columns \\n ix pd date range ts ts num first valid index ts ts num first valid index timedelta \\n QualityStr model ts num qualityMACAPE ts ts num loc ix frc ts ts num loc ix \\n\\nQualityStr sorted QualityStr columns plot label AdaptiveComposition linewidth \\n\\nQualityStr sorted QualityStr columns plot label AdaptiveSelection linewidth alpha \\n\\nplt legend \\n',\n",
       " '\\nQualityStr\\n',\n",
       " '\\nimport pandas\\n\\nimport numpy as np\\n\\nfrom collections import Counter\\n\\nfrom collections import defaultdict\\n\\nfrom collections import deque\\n\\nimport matplotlib pyplot as plt\\n\\nget ipython run line magic matplotlib inline \\n',\n",
       " '\\ntrain pandas read csv home tylorn train csv \\n\\ntest pandas read csv home tylorn test csv \\n',\n",
       " '\\nteam universe set train team set train team set test team set test team \\n',\n",
       " '\\nTEAM SIZE max team universe \\n',\n",
       " '\\ntrain year unique \\n',\n",
       " '\\ntest year unique \\n',\n",
       " '\\n len train team unique len test team unique \\n',\n",
       " '\\n len train team unique len test team unique \\n',\n",
       " '\\nlen set zip train team train team \\n',\n",
       " '\\ntrain pair zip train team train team \\n',\n",
       " '\\nplt hist Counter train pair values bins \\n\\nplt show \\n',\n",
       " '\\nplt hist train score bins \\n\\nplt show \\n',\n",
       " '\\nplt hist train score bins \\n\\nplt show \\n',\n",
       " '\\nplt hist train score train score bins \\n\\nplt show \\n',\n",
       " '\\nplt hist train score train score train score bins \\n\\nplt show \\n',\n",
       " '\\ntrain year pair zip train team train team train year \\n',\n",
       " '\\nplt hist Counter train year pair values bins \\n\\nplt show \\n',\n",
       " '\\ntrue train train train year \\n',\n",
       " '\\nvalidate train train train year \\n',\n",
       " '\\ntrain score mean \\n',\n",
       " '\\ntrain target mean \\n',\n",
       " '\\n\\nclass FeatureExtractor Backup object \\n\\n def init self shift \\n self shift shift\\n self pairs history defaultdict lambda defaultdict list \\n self team history defaultdict lambda defaultdict list \\n\\n def add history self game \\n self pairs history game team game team game year append \\n target int game target \\n score tuple game score game score \\n \\n self pairs history game team game team game year append \\n target int game target \\n score tuple game score game score \\n \\n self team history game team game year append \\n score game score \\n target int game target \\n \\n self team history game team game year append \\n score game score \\n target int game target \\n \\n\\n def get team last games self team years \\n team history self team history team \\n for year in years \\n for game in team history year \\n yield game \\n\\n def get pair last games self pair years \\n team history self pairs history pair \\n for year in years \\n for game in team history year \\n yield game \\n\\n def score mean self scores \\n return sum scores len scores \\n\\n def target mean self targets \\n return sum targets len targets \\n\\n def extract self game \\n features \\n last year games list self get team last games game team game year i self shift for i in xrange \\n features append len last year games \\n features append self target mean x target for x in last year games \\n team mean score years self score mean x score for x in last year games \\n features append team mean score years \\n last year games list self get team last games game team game year i self shift for i in xrange \\n features append len last year games \\n features append self target mean x target for x in last year games \\n team mean score years self score mean x score for x in last year games \\n features append team mean score years \\n last year games list self get team last games game team game year i self shift for i in xrange \\n features append len last year games \\n features append self target mean x target for x in last year games \\n team mean score years self score mean x score for x in last year games \\n features append team mean score years \\n last year games list self get team last games game team game year i self shift for i in xrange \\n features append len last year games \\n features append self target mean x target for x in last year games \\n team mean score years self score mean x score for x in last year games \\n features append team mean score years \\n features append team mean score years team mean score years \\n features append team mean score years team mean score years \\n last year games list self get pair last games game team game team game year i self shift for i in xrange \\n features append len last year games \\n features append self target mean x target for x in last year games \\n last year games list self get pair last games game team game team game year i self shift for i in xrange \\n features append len last year games \\n features append self target mean x target for x in last year games \\n return features\\n',\n",
       " '\\n\\nclass FeatureExtractor object \\n\\n def calc prob self year \\n self first win prob self first win count self matches count \\n cum prob np dot np linalg inv np identity TEAM SIZE self first win prob self first win prob \\n cum prob np dot np linalg inv np identity TEAM SIZE self first win prob self first win prob \\n cum prob np dot np linalg inv np identity TEAM SIZE self first win prob self first win prob \\n cum prob np dot self first win prob self first win prob \\n cum prob np dot self first win prob self first win prob self first win prob \\n cum prob np dot self first win prob self first win prob self first win prob \\n self cum prob year cum prob cum prob np transpose cum prob \\n self cum prob year cum prob cum prob np transpose cum prob \\n self cum prob year cum prob cum prob np transpose cum prob \\n self cum prob year cum prob cum prob np transpose cum prob \\n self cum prob year cum prob cum prob np transpose cum prob \\n self cum prob year cum prob cum prob np transpose cum prob \\n\\n def init self shift \\n self shift shift\\n self pairs history defaultdict lambda defaultdict list \\n self team history defaultdict lambda defaultdict list \\n self matches count np zeros shape TEAM SIZE TEAM SIZE \\n self first win count np zeros shape TEAM SIZE TEAM SIZE \\n self cum prob \\n \\n \\n self cum prob \\n \\n \\n self cum prob \\n \\n \\n self cum prob \\n \\n \\n self cum prob \\n \\n \\n self cum prob \\n \\n \\n self calc prob \\n\\n def push year self year \\n for pair events in self pairs history year iteritems \\n self matches count pair pair len events \\n self first win count pair pair sum x target for x in events \\n self calc prob year \\n\\n def add history self game \\n self pairs history game year game team game team append \\n target int game target \\n score tuple game score game score \\n \\n self pairs history game year game team game team append \\n target int game target \\n score tuple game score game score \\n \\n self team history game team game year append \\n score game score \\n score diff game score game score \\n target int game target \\n \\n self team history game team game year append \\n score game score \\n score diff game score game score \\n target int game target \\n \\n\\n def get team last games self team years \\n team history self team history team \\n for year in years \\n for game in team history year \\n yield game \\n\\n def get pair last games self pair years \\n for year in years \\n for game in self pairs history year pair \\n yield game \\n\\n def score mean self scores \\n return sum scores len scores \\n\\n def target mean self targets \\n return sum targets len targets \\n\\n def diff mean self targets \\n return sum targets len targets \\n\\n def extract self game \\n features \\n last year games list self get team last games game team game year i self shift for i in xrange \\n features append len last year games \\n features append self target mean x target for x in last year games \\n features append self diff mean x score diff for x in last year games \\n team mean score years self score mean x score for x in last year games \\n features append team mean score years \\n last year games list self get team last games game team game year i self shift for i in xrange \\n features append len last year games \\n features append self target mean x target for x in last year games \\n features append self diff mean x score diff for x in last year games \\n team mean score years self score mean x score for x in last year games \\n features append team mean score years \\n last year games list self get team last games game team game year i self shift for i in xrange \\n features append len last year games \\n features append self target mean x target for x in last year games \\n team mean score years self score mean x score for x in last year games \\n features append team mean score years \\n last year games list self get team last games game team game year i self shift for i in xrange \\n features append len last year games \\n features append self target mean x target for x in last year games \\n features append self diff mean x score diff for x in last year games \\n team mean score years self score mean x score for x in last year games \\n features append team mean score years \\n last year games list self get team last games game team game year i self shift for i in xrange \\n features append len last year games \\n features append self target mean x target for x in last year games \\n features append self diff mean x score diff for x in last year games \\n team mean score years self score mean x score for x in last year games \\n features append team mean score years \\n last year games list self get team last games game team game year i self shift for i in xrange \\n features append len last year games \\n features append self target mean x target for x in last year games \\n team mean score years self score mean x score for x in last year games \\n features append team mean score years \\n features append team mean score years team mean score years \\n features append team mean score years team mean score years \\n features append team mean score years team mean score years \\n last year games list self get pair last games game team game team game year i self shift for i in xrange \\n features append len last year games \\n features append self target mean x target for x in last year games \\n last year games list self get pair last games game team game team game year i self shift for i in xrange \\n features append len last year games \\n features append self target mean x target for x in last year games \\n last year games list self get pair last games game team game team game year i self shift for i in xrange \\n features append len last year games \\n features append self target mean x target for x in last year games \\n features append self cum prob game year self shift game team game team \\n features append self cum prob game year self shift game team game team \\n features append self cum prob game year self shift game team game team \\n features append self cum prob game year self shift game team game team \\n features append self cum prob game year self shift game team game team \\n features append self cum prob game year self shift game team game team \\n features append self cum prob game year self shift game team game team \\n features append self cum prob game year self shift game team game team \\n features append self cum prob game year self shift game team game team \\n features append self cum prob game year self shift game team game team \\n features append self cum prob game year self shift game team game team \\n features append self cum prob game year self shift game team game team \\n return features\\n',\n",
       " '\\nfeature extractor FeatureExtractor \\n\\nfeature extractor FeatureExtractor \\n',\n",
       " '\\nnot pushed years deque \\n\\nwith open home tylorn m train train w as f \\n for index game in true train sort values by year iterrows \\n if game year not pushed years \\n if len not pushed years \\n feature extractor push year not pushed years \\n not pushed years popleft \\n not pushed years append game year \\n if game year \\n features feature extractor extract game \\n f write t join map str index int game target features n \\n feature extractor add history game \\n\\nwith open home tylorn m train test w as f \\n for index game in validate train sort values by year iterrows \\n if game year not pushed years \\n if len not pushed years \\n feature extractor push year not pushed years \\n not pushed years popleft \\n not pushed years append game year \\n features feature extractor extract game \\n f write t join map str index int game target features n \\n feature extractor add history game \\n',\n",
       " '\\nnot pushed years deque \\n\\nwith open home tylorn m train train w as f \\n for index game in true train sort values by year iterrows \\n if game year not pushed years \\n if len not pushed years \\n feature extractor push year not pushed years \\n not pushed years popleft \\n not pushed years append game year \\n if game year \\n features feature extractor extract game \\n f write t join map str index int game target features n \\n feature extractor add history game \\n\\nwith open home tylorn m train test w as f \\n for index game in validate train sort values by year iterrows \\n if game year not pushed years \\n if len not pushed years \\n feature extractor push year not pushed years \\n not pushed years popleft \\n not pushed years append game year \\n features feature extractor extract game \\n f write t join map str index int game target features n \\n feature extractor add history game \\n',\n",
       " '\\nfeature extractor FeatureExtractor \\n\\nfeature extractor FeatureExtractor \\n\\nnot pushed years deque \\n\\nwith open home tylorn m train w as f \\n for index game in train sort values by year iterrows \\n if game year not pushed years \\n if len not pushed years \\n feature extractor push year not pushed years \\n not pushed years popleft \\n not pushed years append game year \\n if game year \\n features feature extractor extract game \\n f write t join map str index int game target features n \\n feature extractor add history game \\n\\nfor year in not pushed years \\n feature extractor push year year \\n\\nnot pushed years deque \\n\\nwith open home tylorn m train w as f \\n for index game in train sort values by year iterrows \\n if game year not pushed years \\n if len not pushed years \\n feature extractor push year not pushed years \\n not pushed years popleft \\n not pushed years append game year \\n if game year \\n features feature extractor extract game \\n f write t join map str index int game target features n \\n feature extractor add history game \\n\\nfor year in not pushed years \\n feature extractor push year year \\n\\nwith open home tylorn m test w as f \\n for index game in test sort values by year iterrows \\n if game year \\n features feature extractor extract game \\n f write t join map str game Id features n \\n\\nwith open home tylorn m test w as f \\n for index game in test sort values by year iterrows \\n if game year \\n features feature extractor extract game \\n f write t join map str game Id features n \\n',\n",
       " '\\nresult dict \\n\\nwith open home tylorn m test test matrixnet r as f \\n for line in f xreadlines \\n fields line split t \\n result fields fields \\n\\nwith open home tylorn m test test matrixnet r as f \\n for line in f xreadlines \\n fields line split t \\n result fields fields \\n\\nwith open home tylorn m result w as f \\n f write Id target n \\n for game id prob in result iteritems \\n f write game id prob n \\n',\n",
       " '\\n n nBayesian Ridge Regression n n nComputes a Bayesian Ridge Regression on a synthetic dataset n nSee ref bayesian ridge regression for more information on the regressor n nCompared to the OLS ordinary least squares estimator the coefficient nweights are slightly shifted toward zeros which stabilises them n nAs the prior on the weights is a Gaussian prior the histogram of the nestimated weights is Gaussian n nThe estimation of the model is done by iteratively maximizing the nmarginal log likelihood of the observations n \\n\\nprint doc \\n\\nimport numpy as np\\n\\nimport matplotlib pyplot as plt\\n\\nfrom scipy import stats\\n\\nfrom sklearn linear model import BayesianRidge LinearRegression\\n\\nget ipython run line magic matplotlib inline \\n\\nnp random seed \\n\\n n samples n features \\n\\nX np random randn n samples n features \\n\\nlambda \\n\\nw np zeros n features \\n\\nrelevant features np random randint n features \\n\\nfor i in relevant features \\n w i stats norm rvs loc scale np sqrt lambda \\n\\nalpha \\n\\nnoise stats norm rvs loc scale np sqrt alpha size n samples \\n\\ny np dot X w noise \\n\\nclf BayesianRidge compute score True \\n\\nclf fit X y \\n\\nols LinearRegression \\n\\nols fit X y \\n\\nplt figure figsize \\n\\nplt title Weights of the model \\n\\nplt plot clf coef b label Bayesian Ridge estimate \\n\\nplt plot w g label Ground truth \\n\\nplt plot ols coef r label OLS estimate \\n\\nplt xlabel Features \\n\\nplt ylabel Values of the weights \\n\\nplt legend loc best prop dict size \\n\\nplt figure figsize \\n\\nplt title Histogram of the weights \\n\\nplt hist clf coef bins n features log True \\n\\nplt plot clf coef relevant features np ones len relevant features ro label Relevant features \\n\\nplt ylabel Features \\n\\nplt xlabel Values of the weights \\n\\nplt legend loc lower left \\n\\nplt figure figsize \\n\\nplt title Marginal log likelihood \\n\\nplt plot clf scores \\n\\nplt ylabel Score \\n\\nplt xlabel Iterations \\n\\nplt show \\n',\n",
       " '\\nimport tweepy\\n\\nfrom textblob import TextBlob\\n\\nimport pandas as pd\\n',\n",
       " '\\nconsumer key \\n\\nconsumer secret \\n\\naccess token \\n\\naccess token secret \\n',\n",
       " '\\nauth tweepy OAuthHandler consumer key consumer secret \\n\\nauth set access token access token access token secret \\n',\n",
       " '\\napi tweepy API auth \\n',\n",
       " '\\ntweet data \\n Tweet \\n Polarity \\n Subjectivty \\n \\n',\n",
       " '\\npublic tweets api search Hillary \\n\\nfor tweet in public tweets \\n analysis TextBlob tweet text \\n tweet data Tweet append tweet text \\n tweet data Polarity append str analysis sentiment polarity \\n tweet data Subjectivty append str analysis sentiment subjectivity \\n',\n",
       " '\\ndf pd DataFrame tweet data columns Tweet Polarity Subjectivty \\n\\ndf\\n',\n",
       " '\\npath C Users Raunaq Desktop Tweets SentimentAnalysis csv \\n\\ndf to csv path \\n\\nprint CSV file saved in path \\n',\n",
       " '\\nimport numpy as np\\n\\nimport pandas\\n\\nimport wqio\\n\\nimport pynsqd\\n\\nimport pycvc\\n',\n",
       " '\\nnsqdata pycvc external nsqd red d \\n\\nnsqdata data to csv nsqdata in pycvc csv index False \\n\\nnsqdata medians to csv ED influent medians csv index False \\n\\nnsqdata datacollection tidy to csv nsqtidy csv index False \\n',\n",
       " '\\nmain cols epa rain zone season primary landuse parameter fraction units res qual \\n\\ndf pandas read csv nsqdata example subset csv usecols main cols \\n\\ndf cen df qual isin \\n\\ndf head \\n',\n",
       " '\\nros wqio robustros RobustROSEstimator data df \\n\\nnp median ros estimated values \\n',\n",
       " '\\nnsqdata season medians query parameter Cadmium Cd and season spring \\n',\n",
       " '\\nimport pandas as pd\\n\\nfrom random import random\\n\\nget ipython run line magic matplotlib inline \\n\\nget ipython run line magic load ext autoreload \\n\\nget ipython run line magic autoreload \\n\\nflow list range list range \\n\\npdata pd DataFrame \\n a flow \\n b flow \\n \\n\\npdata b pdata b shift \\n\\ndata pdata iloc random \\n\\ndata plot \\n',\n",
       " '\\nimport numpy as np\\n\\n\\ndef load data data n prev \\n n data should be pd DataFrame n \\n docX docY \\n for i in range len data n prev \\n docX append data iloc i i n prev as matrix \\n docY append data iloc i n prev as matrix \\n alsX np array docX \\n alsY np array docY \\n return alsX alsY \\n\\n\\ndef train test split df test size \\n n This just splits data to training and testing parts n \\n ntrn round len df test size \\n X train y train load data df iloc ntrn \\n X test y test load data df iloc ntrn \\n return X train y train X test y test \\n',\n",
       " '\\nfrom keras models import Sequential\\n\\nfrom keras layers core import Dense Activation\\n\\nfrom keras layers recurrent import LSTM\\n\\nin out neurons \\n\\nhidden neurons \\n\\nmodel Sequential \\n\\nmodel add LSTM hidden neurons input dim in out neurons return sequences False \\n\\nmodel add Dense in out neurons input dim hidden neurons \\n\\nmodel add Activation linear \\n\\nmodel compile loss mean squared error optimizer rmsprop \\n',\n",
       " '\\n X train y train X test y test train test split data \\n\\nmodel fit X train y train batch size nb epoch validation split \\n\\npredicted model predict X test \\n\\nrmse np sqrt predicted y test mean axis \\n',\n",
       " '\\npd DataFrame predicted plot \\n',\n",
       " '\\npd DataFrame y test plot \\n',\n",
       " '\\nimport pandas as pd\\n\\nimport numpy as np\\n\\nimport pysentani as sti\\n',\n",
       " '\\nsurvey pd read excel data anonymous sentani merged cleaned anonymous xlsx \\n',\n",
       " '\\nsurvey access type sti access type survey \\n\\nsurvey electricity monthly sti elec expenditure monthly survey \\n',\n",
       " '\\nsurvey survey survey village name Ajau \\n',\n",
       " '\\nsubsurvey survey access type village name PLN expenditure monthly \\n',\n",
       " '\\nsubsurvey dropna \\n',\n",
       " '\\nsubsurvey mean \\n',\n",
       " '\\n\\ndef Ajau kwh conversion cv \\n converted cv PLN expenditure monthly \\n return converted\\n\\nsurvey Ajau kWh survey apply Ajau kwh conversion axis \\n',\n",
       " '\\nhousehold mean survey groupby access type PLN expenditure monthly Ajau kWh mean \\n\\nhousehold mean reset index inplace True \\n\\nhousehold mean\\n',\n",
       " '\\nsurvey PLN expenditure monthly Ajau kWh mean \\n',\n",
       " '\\nimport pandas as pd\\n\\nimport numpy as np\\n',\n",
       " '\\ndata pd read csv final raw data csv \\n\\ndata head \\n\\ndata rename columns \\n Full count Larvae Larvae count \\n inplace True \\n',\n",
       " '\\n \\n\\nget ipython run line magic matplotlib inline \\n\\nimport matplotlib pyplot as plt\\n\\nhamlet larvae data data Larvae \\n\\nplt scatter data Hamlet data Larvae count \\n\\nplt xlabel Hamlet \\n\\nplt ylabel Larvae count \\n\\nplt title Hamlet vs Larvae count \\n\\nplt ylim \\n\\nplt xlim \\n',\n",
       " '\\nhamlet larvae data data Larvae \\n\\nnew hamlet larvae hamlet larvae Hamlet Larvae count \\n\\nnew hamlet larvae groupby Hamlet mean plot kind bar \\n\\nplt xlabel Hamlet \\n\\nplt ylabel Average count of larvae \\n\\nplt title Average number of larvae per hamlet \\n',\n",
       " '\\nhamlet larvae data data Larvae \\n\\nnew hamlet larvae hamlet larvae Hamlet Larvae count \\n\\nnew hamlet larvae Hamlet value counts plot kind bar \\n\\nplt xlabel Hamlet \\n\\nplt ylabel Households with larvae \\n\\nplt title Households with mosquito larvae per hamlet \\n\\nplt ylim \\n',\n",
       " '\\nfrom scipy stats import gaussian kde\\n\\nvstacked np vstack hamlet larvae Hamlet hamlet larvae Larvae count \\n\\ndensity color gaussian kde vstacked vstacked \\n\\nidx density color argsort \\n\\nhamlet larvae Hamlet hamlet larvae Hamlet idx \\n\\nhamlet larvae Larvae count hamlet larvae Larvae count idx \\n\\ndensity color density color idx \\n',\n",
       " '\\n fig ax plt subplots \\n\\nax scatter hamlet larvae Hamlet hamlet larvae Larvae count c density color s edgecolor \\n\\nplt xlim \\n\\nplt ylim \\n\\nplt xlabel Hamlet \\n\\nplt ylabel Count of larvae \\n\\nplt title Density plot of count of larvae in a hamlet \\n\\nplt show \\n',\n",
       " '\\nget ipython run line magic matplotlib inline \\n\\nimport pandas as pd\\n\\nimport numpy as np\\n\\nimport seaborn as sns\\n\\nimport matplotlib pyplot as plt\\n',\n",
       " '\\nmessages line rstrip for line in open smsspamcollection SMSSpamCollection \\n\\nprint len messages \\n',\n",
       " '\\nfor message no message in enumerate messages \\n print message no message \\n print n \\n',\n",
       " '\\nimport pandas as pd\\n',\n",
       " '\\nmessages pd read csv smsspamcollection SMSSpamCollection sep t names label message \\n\\nmessages head \\n',\n",
       " '\\nmessages describe \\n',\n",
       " '\\nmessages groupby label describe \\n',\n",
       " '\\nmessages length messages message apply len \\n\\nmessages head \\n',\n",
       " '\\nimport matplotlib pyplot as plt\\n\\nimport seaborn as sns\\n\\nget ipython run line magic matplotlib inline \\n',\n",
       " '\\nmessages length plot bins kind hist \\n',\n",
       " '\\nmessages length describe \\n',\n",
       " '\\nmessages messages length message iloc \\n',\n",
       " '\\nmessages hist column length by label bins figsize \\n',\n",
       " '\\nimport string\\n\\nmess Sample message Notice it has punctuation \\n\\nnopunc char for char in mess if char not in string punctuation \\n\\nnopunc join nopunc \\n',\n",
       " '\\nfrom nltk corpus import stopwords\\n\\nstopwords words english \\n',\n",
       " '\\nnopunc split \\n',\n",
       " '\\nclean mess word for word in nopunc split if word lower not in stopwords words english \\n',\n",
       " '\\nclean mess\\n',\n",
       " '\\n\\ndef text process mess \\n n Takes in a string of text then performs the following n Remove all punctuation n Remove all stopwords n Returns a list of the cleaned text n \\n nopunc char for char in mess if char not in string punctuation \\n nopunc join nopunc \\n return word for word in nopunc split if word lower not in stopwords words english \\n',\n",
       " '\\nmessages head \\n',\n",
       " '\\nmessages message head apply text process \\n',\n",
       " '\\nmessages head \\n',\n",
       " '\\nfrom sklearn feature extraction text import CountVectorizer\\n',\n",
       " '\\nbow transformer CountVectorizer analyzer text process fit messages message \\n\\nprint len bow transformer vocabulary \\n',\n",
       " '\\nmessage messages message \\n\\nprint message \\n',\n",
       " '\\nbow bow transformer transform message \\n\\nprint bow \\n\\nprint bow shape \\n',\n",
       " '\\nprint bow transformer get feature names \\n\\nprint bow transformer get feature names \\n',\n",
       " '\\nmessages bow bow transformer transform messages message \\n',\n",
       " '\\nprint Shape of Sparse Matrix messages bow shape \\n\\nprint Amount of Non Zero occurences messages bow nnz \\n',\n",
       " '\\nsparsity messages bow nnz messages bow shape messages bow shape \\n\\nprint sparsity format round sparsity \\n',\n",
       " '\\nfrom sklearn feature extraction text import TfidfTransformer\\n\\ntfidf transformer TfidfTransformer fit messages bow \\n\\ntfidf tfidf transformer transform bow \\n\\nprint tfidf \\n',\n",
       " '\\nprint tfidf transformer idf bow transformer vocabulary u \\n\\nprint tfidf transformer idf bow transformer vocabulary university \\n',\n",
       " '\\nmessages tfidf tfidf transformer transform messages bow \\n\\nprint messages tfidf shape \\n',\n",
       " '\\nfrom sklearn naive bayes import MultinomialNB\\n\\nspam detect model MultinomialNB fit messages tfidf messages label \\n',\n",
       " '\\nprint predicted spam detect model predict tfidf \\n\\nprint expected messages label \\n',\n",
       " '\\nall predictions spam detect model predict messages tfidf \\n\\nprint all predictions \\n',\n",
       " '\\nfrom sklearn metrics import classification report\\n\\nprint classification report messages label all predictions \\n',\n",
       " '\\nfrom sklearn model selection import train test split\\n\\n msg train msg test label train label test train test split messages message messages label test size \\n\\nprint len msg train len msg test len msg train len msg test \\n',\n",
       " '\\nfrom sklearn pipeline import Pipeline\\n\\npipeline Pipeline bow CountVectorizer analyzer text process tfidf TfidfTransformer classifier MultinomialNB \\n',\n",
       " '\\npipeline fit msg train label train \\n',\n",
       " '\\npredictions pipeline predict msg test \\n',\n",
       " '\\nprint classification report predictions label test \\n',\n",
       " '\\nimport numpy as np\\n\\nimport pandas as pd\\n\\nfrom pandas import DataFrame Series\\n\\nunames user id gender age occupation zip \\n\\nimport os\\n\\nos popen pwd read \\n',\n",
       " '\\nusers pd read table movielens users dat sep header None names unames engine python \\n\\nrnames user id movie id rating timestamp \\n\\nratings pd read table movielens ratings dat sep header None names rnames engine python \\n\\nmnames user id title genres \\n\\nmovies pd read table movielens movies dat sep header None names mnames engine python \\n',\n",
       " '\\nusers \\n',\n",
       " '\\nratings \\n',\n",
       " '\\nmovies \\n',\n",
       " '\\nratings\\n',\n",
       " '\\ndata pd merge pd merge ratings users movies \\n\\ndata\\n',\n",
       " '\\ndata ix \\n',\n",
       " '\\nmean ratings data pivot table rating index title columns gender aggfunc mean \\n\\nmean ratings \\n',\n",
       " '\\nrating by title data groupby title size \\n\\nrating by title \\n',\n",
       " '\\nactive titles rating by title index rating by title \\n\\nactive titles\\n',\n",
       " '\\nmean ratings mean ratings ix active titles \\n\\nmean ratings\\n',\n",
       " '\\ntop female rating mean ratings sort values by F ascending False \\n\\ntop female rating \\n',\n",
       " '\\nmean ratings diff mean ratings M mean ratings F \\n\\nsorted by diff mean ratings sort values by diff \\n\\nsorted by diff \\n',\n",
       " '\\nrating std by title data groupby title rating std \\n\\nrating std by title rating std by title ix active titles \\n\\nrating std by title sort values ascending False \\n',\n",
       " '\\nimport math\\n\\nimport datetime\\n\\nimport pandas as pd\\n\\nimport seaborn as sns\\n\\nimport matplotlib pyplot as plt\\n\\nget ipython run line magic matplotlib inline \\n',\n",
       " '\\npage id appledaily tw \\n\\npath post page id post csv \\n',\n",
       " '\\ndf pd read csv path encoding utf \\n',\n",
       " '\\ndf head \\n',\n",
       " '\\ndf status link \\n',\n",
       " '\\nlen df \\n',\n",
       " '\\ndf df df num reactions df status message notnull reindex \\n',\n",
       " '\\nlen df \\n',\n",
       " '\\ndf datetime df status published apply lambda x datetime datetime strptime x Y m d H M S \\n\\ndf weekday df datetime apply lambda x x weekday name \\n\\ndf hour df datetime apply lambda x x hour \\n',\n",
       " '\\ndf plot x datetime y num likes num loves num wows num hahas num sads num angrys figsize \\n',\n",
       " '\\ndf plot x datetime y num reactions num comments num shares figsize \\n',\n",
       " '\\nimport datetime\\n\\ndelta datetime df datetime shift df datetime \\n\\ndelta datetime df pd Series delta datetime describe apply str \\n\\ndelta datetime df delta datetime df to frame name frequent of posts \\n\\ndelta datetime df\\n',\n",
       " '\\n\\ndef weekday d \\n list key Monday Tuesday Wednesday Thursday Friday Saturday Sunday \\n list value \\n for one in list key \\n if one in d keys \\n list value append d one \\n else \\n list value append \\n df pd DataFrame index list key data \\n weekday list value \\n reset index \\n return df\\n',\n",
       " '\\ndf weekday weekday dict df weekday value counts \\n\\ndf weekday\\n',\n",
       " '\\nsns barplot x index y weekday data df weekday \\n',\n",
       " '\\n\\ndef hour d \\n list key \\n list value \\n for one in list key \\n if one in d keys \\n list value append d one \\n else \\n list value append \\n df pd DataFrame index list key data \\n hour list value \\n reset index \\n return df\\n',\n",
       " '\\ndf hour hour dict df hour value counts \\n\\ndf hour\\n',\n",
       " '\\nax sns barplot x index y hour data df hour \\n',\n",
       " '\\ndf status type df status type value counts to frame name status type \\n\\ndf status type\\n',\n",
       " '\\nsns barplot x index y status type data df status type reset index \\n',\n",
       " '\\nsns stripplot x status type y num reactions data df jitter True \\n',\n",
       " '\\nsns stripplot x weekday y num reactions data df jitter True \\n',\n",
       " '\\nsns stripplot x hour y num reactions data df jitter True \\n',\n",
       " '\\ng sns FacetGrid df col status type \\n\\ng map plt hist num reactions \\n',\n",
       " '\\ndf reaction df num likes num loves num wows num hahas num sads num angrys \\n\\ncolormap plt cm viridis\\n\\nplt title Pearson Correlation of Features y size \\n\\nsns heatmap df reaction astype float corr linewidths vmax square True cmap colormap linecolor white annot True \\n',\n",
       " '\\ndf tmp df num reactions num comments num shares \\n\\ncolormap plt cm viridis\\n\\nplt title Pearson Correlation of Features y size \\n\\nsns heatmap df tmp astype float corr linewidths vmax square True cmap colormap linecolor white annot True \\n',\n",
       " '\\nimport jieba\\n\\nimport jieba analyse\\n\\nimport operator\\n\\nfrom wordcloud import WordCloud\\n\\njieba set dictionary home wy anaconda envs python lib python site packages jieba extra dict dict txt big \\n',\n",
       " '\\nlist df status message \\n',\n",
       " '\\nfor one in jieba cut list df status message \\n print one \\n',\n",
       " '\\njieba analyse extract tags list df status message topK \\n',\n",
       " '\\n\\ndef jieba extract message list \\n word count \\n \\n \\n for message in message list \\n seg list jieba analyse extract tags message topK \\n for seg in seg list \\n if not seg in word count \\n word count seg \\n else \\n word count seg \\n sorted word count sorted word count items key operator itemgetter \\n sorted word count reverse \\n return sorted word count\\n\\nsorted word count jieba extract list df status message \\n',\n",
       " '\\nprint sorted word count \\n',\n",
       " '\\ntpath home wy font NotoSansCJKtc Black otf \\n\\nwordcloud WordCloud max font size relative scaling width height font path tpath fit words sorted word count \\n\\nplt imshow wordcloud \\n\\nplt axis off \\n\\nplt show \\n',\n",
       " '\\ntpath home wy font NotoSansCJKtc Black otf \\n\\nwordcloud WordCloud max font size relative scaling width height font path tpath fit words sorted word count \\n\\nplt imshow wordcloud \\n\\nplt axis off \\n\\nplt show \\n',\n",
       " '\\nc path path comment page id comment csv \\n\\nc df pd read csv c path \\n',\n",
       " '\\nc df head \\n',\n",
       " '\\nc df c df c df comment message notnull reindex \\n',\n",
       " '\\nsorted comment message jieba extract list c df comment message \\n\\nprint sorted comment message \\n\\ntpath home wy font NotoSansCJKtc Black otf \\n\\nwordcloud WordCloud max font size relative scaling width height font path tpath fit words sorted comment message \\n\\nplt figure \\n\\nplt imshow wordcloud \\n\\nplt axis off \\n\\nplt show \\n',\n",
       " '\\nc df c df c df comment author notnull reindex \\n',\n",
       " '\\n\\ndef word count data list \\n d \\n \\n \\n for one in data list \\n if one not in d \\n d one \\n else \\n d one \\n return d\\n',\n",
       " '\\nd word count list c df comment author \\n\\ncomment authors k d k for k in sorted d key d get reverse True \\n\\nprint comment authors \\n\\ntpath home wy font NotoSansCJKtc Black otf \\n\\nwordcloud WordCloud max font size relative scaling width height font path tpath fit words comment authors \\n\\nplt figure \\n\\nplt imshow wordcloud \\n\\nplt axis off \\n\\nplt show \\n',\n",
       " '\\nimport xlsxwriter\\n',\n",
       " '\\ndf num reactions df num reactions describe to frame name reactions \\n\\ndf num reactions\\n',\n",
       " '\\ndf num comments df num comments describe to frame name comments \\n\\ndf num comments\\n',\n",
       " '\\ndf num shares df num shares describe to frame name shares \\n\\ndf num shares\\n',\n",
       " '\\nexcel path excel page id analysis xlsx \\n\\nwriter pd ExcelWriter excel path engine xlsxwriter \\n\\ndf num reactions to excel writer sheet name page id startcol startrow \\n\\ndf num comments to excel writer sheet name page id startcol startrow \\n\\ndf num shares to excel writer sheet name page id startcol startrow \\n\\ndelta datetime df to excel writer sheet name page id startcol startrow \\n\\ndf status type to excel writer sheet name page id startcol startrow \\n\\ndf weekday set index index to excel writer sheet name page id startcol startrow \\n\\ndf hour set index index to excel writer sheet name page id startcol startrow \\n\\nworkbook writer book\\n\\nchart workbook add chart \\n type column \\n \\n\\nchart add series \\n categories page id A A \\n values page id B B \\n \\n\\nchart set title \\n name \\n \\n\\nchart set x axis \\n name status type \\n \\n\\nchart set y axis \\n name count \\n \\n\\nworksheet writer sheets page id \\n\\nworksheet insert chart D chart \\n\\nchart workbook add chart \\n type column \\n \\n\\nchart add series \\n categories page id A A \\n values page id B B \\n \\n\\nchart set title \\n name \\n \\n\\nchart set x axis \\n name hour \\n \\n\\nchart set y axis \\n name count \\n \\n\\nworksheet writer sheets page id \\n\\nworksheet insert chart D chart \\n\\nchart workbook add chart \\n type column \\n \\n\\nchart add series \\n categories page id A A \\n values page id B B \\n \\n\\nchart set title \\n name \\n \\n\\nchart set x axis \\n name weekday \\n \\n\\nchart set y axis \\n name count \\n \\n\\nworksheet writer sheets page id \\n\\nworksheet insert chart D chart \\n\\ndf plot x datetime y num likes num loves num wows num hahas num sads num angrys \\n\\nplt savefig image image png \\n\\nworksheet insert image L image image png \\n',\n",
       " '\\nimport json\\n\\nimport pandas as pd\\n',\n",
       " '\\nwith open yelp dataset challenge round yelp academic dataset business json as data file \\n businesses df pd DataFrame json loads line for line in data file readlines \\n',\n",
       " '\\nUS States AL AK AZ AR CA CO CT DC DE FL GA HI ID IL IN IA KS KY LA ME MD MA MI MN MS MO MT NE NV NH NJ NM NY NC ND OH OK OR PA RI SC SD TN TX UT VT VA WA WV WI WY \\n\\nbusinesses df businesses df businesses df state isin US States \\n',\n",
       " '\\nbusinesses df businesses df loc business id stars \\n\\nbusinesses df to csv businesses shortened csv encoding utf \\n',\n",
       " '\\nwith open yelp dataset challenge round yelp academic dataset review json as data file \\n reviews df pd DataFrame json loads line for line in data file readlines \\n',\n",
       " '\\nimport re\\n\\nfrom matplotlib import pyplot\\n\\nfrom collections import Counter\\n\\nget ipython run line magic matplotlib inline \\n\\nimport json\\n\\nimport pandas as pd\\n',\n",
       " '\\nreviews \\n\\nwith open dataset yelp academic dataset review json as data file \\n for line in data file \\n reviews append json loads line \\n\\nreviews df pd DataFrame reviews \\n\\ndel reviews\\n',\n",
       " '\\nbusinesses \\n\\nwith open dataset yelp academic dataset business json as data file \\n for line in data file \\n businesses append json loads line \\n\\nbusinesses df pd DataFrame businesses \\n\\ndel businesses\\n',\n",
       " '\\nstates set businesses df state \\n\\nstates remove TAM \\n',\n",
       " '\\nkarlsruhe states BW NW \\n\\nedinburgh states EDH SCB ELN FIF HAM KHL MLN NTH SCB XGL \\n\\nmontreal states QC \\n\\nwaterloo states ON \\n\\nus states state for state in states if not state in karlsruhe states or state in edinburgh states or state in montreal states or state in waterloo states \\n\\nlocation mapping dict x Karlsruhe for x in karlsruhe states x Edinburgh for x in edinburgh states x Montreal for x in montreal states x Waterloo for x in waterloo states x USA for x in us states \\n',\n",
       " '\\nbusinesses df location businesses df state map location mapping \\n\\nbad biz id businesses df businesses df location isnull business id item \\n\\nbusinesses df businesses df businesses df location notnull \\n\\nreviews df reviews df reviews df business id bad biz id \\n',\n",
       " '\\nbusiness loc dict businesses df set index business id to dict location \\n\\nreviews df location reviews df business id map business loc dict \\n',\n",
       " '\\nreviews \\n Karlsruhe Counter \\n Edinburgh Counter \\n Montreal Counter \\n Waterloo Counter \\n USA Counter \\n \\n\\nfor index review in reviews df iterrows \\n biz loc review location \\n reviews biz loc review stars \\n',\n",
       " '\\nfor place counter in reviews items \\n total stars float sum counter values \\n proportions k v total stars for k v in counter items \\n pyplot bar counter keys x for x in proportions values \\n pyplot xlabel Star Rating \\n pyplot axis \\n pyplot ylabel Percent of all Reviews \\n pyplot title place \\n pyplot show \\n',\n",
       " '\\n\\ndef cleanup string \\n return re sub a zA Z string \\n\\nreviews df text reviews df text map cleanup \\n',\n",
       " '\\nus reviews reviews df reviews df location USA \\n',\n",
       " '\\nus reviews us reviews us reviews text \\n',\n",
       " '\\ndate words date dating boyfriend girlfriend bf gf partner boy friend girl friend fianc fiance fianceemarry marriage married wedding honeymoon honey moon anniversary \\n',\n",
       " '\\n\\ndef includes date word review \\n if any date word in review text for date word in date words \\n return True\\n else \\n return False\\n\\ndate us reviews us reviews us reviews apply includes date word \\n',\n",
       " '\\nus reviews \\n',\n",
       " '\\nimport pandas as pd numpy as np seaborn as sns matplotlib pyplot as plt datetime\\n\\nfrom datetime import timedelta\\n\\nget ipython run line magic matplotlib inline \\n',\n",
       " '\\nbd original pd read csv assets billboard csv header \\n',\n",
       " '\\ntype bd original \\n',\n",
       " '\\nbd original head \\n',\n",
       " '\\nbd original rename columns \\n year year released \\n artist inverted artist \\n date entered date entered \\n date peaked date peaked \\n inplace True \\n',\n",
       " '\\nbd original date entered pd to datetime bd original date entered \\n\\nbd original date peaked pd to datetime bd original date peaked \\n',\n",
       " '\\nbd original weeks on chart bd original ix notnull sum axis \\n',\n",
       " '\\nbd original days to peak bd original date peaked bd original date entered \\n',\n",
       " '\\nbd original head \\n',\n",
       " '\\nbd nan dropped bd original dropna how all axis \\n\\nbd nan dropped shape\\n',\n",
       " '\\nbd nan dropped head \\n',\n",
       " '\\nbd nan dropped describe include all \\n',\n",
       " '\\nbd melt pd melt bd nan dropped id vars year artist track time genre date entered date peaked weeks on chart days to peak value vars list bd nan dropped columns values var name Week value name Ranking \\n',\n",
       " '\\nbd melt info \\n',\n",
       " '\\nbd melt Week bd melt Week map lambda x x replace x replace st replace nd replace rd replace th replace week \\n',\n",
       " '\\nbd melt head \\n',\n",
       " '\\nbd melt Week bd melt Week astype int \\n',\n",
       " '\\nweek date \\n\\nfor index date in enumerate bd melt date entered \\n d date\\n w timedelta days int bd melt Week index \\n week date insert index d w \\n\\nbd melt week date week date\\n',\n",
       " '\\nbd melt head \\n',\n",
       " '\\nbd melt dtypes\\n',\n",
       " '\\nbd melt genre bd melt genre astype category \\n',\n",
       " '\\nbd melt rank on chart x for x in bd melt Ranking \\n\\nbd melt info \\n',\n",
       " '\\nbd track counts pd pivot table bd melt index track aggfunc count \\n',\n",
       " '\\nbd track counts head \\n',\n",
       " '\\nbd melt genre pd melt bd nan dropped id vars genre track value vars list bd nan dropped iloc var name Week value name Ranking \\n',\n",
       " '\\nbd genre counts pd pivot table bd melt genre index genre track aggfunc count \\n',\n",
       " '\\nbd genre counts head \\n',\n",
       " '\\nget ipython run line magic pylab inline \\n\\npylab rcParams figure figsize \\n\\nimport numpy as np\\n\\nimport imageio\\n\\nfrom scipy ndimage import filters\\n\\nfrom scipy import io\\n',\n",
       " '\\nInputs beer coke inp bmp cups board inp bmp people inp bmp \\n\\nImages map imageio imread Inputs \\n\\nImages map lambda Image Image astype np float Images \\n\\nDepthFilters \\n\\nfor i in range \\n filterFName CodedApertureData filts filt scl d mat i \\n DepthFilters append io loadmat filterFName filts \\n',\n",
       " '\\nImageIndex \\n\\n\\ndef PixelError Window \\n return np sum Window \\n\\nResultImages \\n\\nDepthMaps \\n\\nfor Input in Inputs \\n Deblurred \\n for Index in range \\n Deblurred append io loadmat s d Input Index deblurred \\n TargetImage Images ImageIndex \\n ErrorsR map lambda i TargetImage filters convolve Deblurred i DepthFilters i mode reflect range \\n ErrorsG map lambda i TargetImage filters convolve Deblurred i DepthFilters i mode reflect range \\n ErrorsB map lambda i TargetImage filters convolve Deblurred i DepthFilters i mode reflect range \\n PxErrorsR map lambda i filters generic filter ErrorsR i PixelError size mode reflect range \\n PxErrorsG map lambda i filters generic filter ErrorsG i PixelError size mode reflect range \\n PxErrorsB map lambda i filters generic filter ErrorsB i PixelError size mode reflect range \\n BestR np argmin PxErrorsR axis \\n BestG np argmin PxErrorsG axis \\n BestB np argmin PxErrorsB axis \\n AllInFocusImage np zeros like TargetImage \\n for h in range AllInFocusImage shape \\n for w in range AllInFocusImage shape \\n AllInFocusImage h w Deblurred BestR h w h w \\n AllInFocusImage h w Deblurred BestG h w h w \\n AllInFocusImage h w Deblurred BestB h w h w \\n DepthMaps append BestR \\n ResultImages append AllInFocusImage \\n ImageIndex \\n',\n",
       " '\\nfor i in range \\n subplot i \\n imshow Images i \\n subplot i \\n imshow np clip ResultImages i \\n imageio imsave s deblurred bmp Inputs i np clip ResultImages i \\n',\n",
       " '\\nfor i in range \\n subplot i \\n imshow DepthMaps i cmap prism \\n',\n",
       " '\\nget ipython run line magic pylab inline \\n\\nimport tradingWithPython as twp\\n\\nimport tradingWithPython lib yahooFinance as yf\\n\\nimport tradingWithPython lib cboe as cboe\\n\\nfrom sklearn import neighbors\\n\\nimport pandas as pd\\n\\nfigsize \\n\\ntwp extra setNotebookStyle \\n',\n",
       " '\\nvxx yf getHistoricData VXX adj close \\n\\nvol cboe getHistoricData \\n',\n",
       " '\\ndata pd DataFrame \\n VIX vol VIX \\n VXV vol VXV \\n VXX vxx \\n dropna \\n\\ndata plot logy True \\n',\n",
       " '\\nk \\n\\nR data VXX pct change \\n\\nX pd DataFrame \\n a data VIX data VXV \\n b pd rolling sum R k \\n \\n\\nY R shift \\n\\nuniqueIdx pd concat X Y axis dropna index\\n\\nX X reindex uniqueIdx \\n\\nY Y reindex uniqueIdx \\n\\nX plot kind scatter x a y b \\n',\n",
       " '\\n\\nclass Normalizer object \\n normalizes data to zero mean and percentiles range \\n\\n def init self data \\n self mean data mean \\n self rng data quantile data quantile \\n\\n def transform self data \\n normalize dataset \\n return data self mean self rng \\n\\nnormalizer Normalizer X \\n\\nXn normalizer transform X \\n\\nXn plot kind scatter x a y b \\n\\ntitle normalized dataset \\n',\n",
       " '\\nknn neighbors NearestNeighbors \\n\\nknn fit Xn \\n\\nx \\n\\nxn normalizer transform x \\n\\nprint Transformed x n xn \\n\\n dist idx knn kneighbors xn \\n\\nidx idx \\n\\n a b a b \\n\\nax Xn plot kind scatter x a y b label Dataset \\n\\nplot xn xn go \\n\\nXn ix idx plot kind scatter x a y b color red ax ax label Neighbors \\n\\ntitle Neighbors \\n',\n",
       " '\\navgRet Y idx mean \\n\\nsharpe sqrt avgRet Y idx std \\n\\nprint Average return f Sharpe f avgRet sharpe \\n\\nfigure \\n\\nY idx plot kind bar \\n\\ntitle Next day returns of neighbors \\n',\n",
       " '\\n\\nclass NNTrader object \\n nearest neighbor trading class \\n\\n def init self X Y n \\n Init class n X independent variables n Y dependent variable n n number of neighbors to use n \\n self normalizer Normalizer X \\n self X self normalizer transform X \\n self Y Y copy \\n self knn neighbors NearestNeighbors n \\n self knn fit self X \\n\\n def findNeighbors self x \\n find neighbors for point x Returns neighboring points from Y \\n xn normalizer transform x \\n dist idx self knn kneighbors xn \\n idx idx \\n return self Y idx \\n\\n def advice self x thresh \\n get n x input variable n thresh sharpe threshold to go long or short n returns trading advice short no action long n \\n Y self findNeighbors x \\n avgRet Y mean \\n sharpe sqrt avgRet Y std \\n if sharpe thresh \\n return \\n elif sharpe thresh \\n return \\n else \\n return \\n',\n",
       " '\\nsplit \\n\\nN \\n\\nPNL pd DataFrame index X index columns N \\n\\nthresh \\n\\nfor n in N \\n print Simulating with i neighbors n \\n trader NNTrader X ix split Y ix split n n \\n pnl pd Series index Y index \\n for idx in X index split \\n x X ix idx \\n tradeDir trader advice x thresh thresh \\n pnl idx Y idx tradeDir \\n PNL n pnl\\n\\nPNL split cumsum plot \\n\\n Y split cumsum plot ax gca linewidth \\n',\n",
       " '\\nprint twp sharpe PNL ix split \\n\\nprint twp sharpe Y split \\n',\n",
       " '\\nimport numpy as np\\n\\nimport pandas as pd\\n\\nfrom pandas import Series DataFrame\\n',\n",
       " '\\ndframe DataFrame np arange reshape \\n',\n",
       " '\\nblender np random permutation \\n',\n",
       " '\\nblender\\n',\n",
       " '\\ndframe\\n',\n",
       " '\\ndframe take blender \\n',\n",
       " '\\nbox np array \\n',\n",
       " '\\nshaker np random randint len box size \\n',\n",
       " '\\nshaker\\n',\n",
       " '\\nhand grabs box take shaker \\n\\nhand grabs\\n',\n",
       " '\\nimport numpy as np\\n\\nimport pandas as pd\\n\\nfrom pandas import DataFrame Series\\n\\nfrom numpy random import randn\\n\\nfrom scipy import stats\\n\\nimport matplotlib pyplot as plt\\n\\nimport seaborn as sns\\n\\nget ipython run line magic matplotlib inline \\n',\n",
       " '\\ndf ubs pd read excel ubs prices and earnings opendata xlsx parse cols names Year City MainSection SubSection Value convert floats False \\n',\n",
       " '\\ndf dpp df ubs df ubs MainSection General Domestic Purchasing Power df ubs SubSection Net hourly New York df ubs City isin London Berlin Zurich Geneva Luxembourg Paris Munich df ubs Year pivot index City columns MainSection values Value \\n\\ndf earnnet df ubs df ubs MainSection Earnings Average hourly net df ubs SubSection USD df ubs City isin London Berlin Zurich Geneva Luxembourg Paris Munich df ubs Year pivot index City columns MainSection values Value \\n\\ndf housing df ubs df ubs MainSection Prices Housing df ubs SubSection Unfurnished room Apartment medium USD df ubs City isin London Berlin Zurich Geneva Luxembourg Paris Munich df ubs Year pivot index City columns MainSection values Value \\n\\ndf iphone df ubs df ubs MainSection General Working time req to buy df ubs SubSection iPhone GB net hours df ubs City isin London Berlin Zurich Geneva Luxembourg Paris Munich df ubs Year pivot index City columns MainSection values Value \\n',\n",
       " '\\ndf dpp earnnet pd concat df dpp df earnnet axis join inner reset index \\n\\ndf housing earnnet pd concat df housing df earnnet axis join inner reset index \\n\\ndf comb pd concat df dpp df housing df earnnet axis join inner reset index \\n',\n",
       " '\\nsns lmplot General Domestic Purchasing Power Earnings Average hourly net data df comb fit reg False hue City \\n',\n",
       " '\\nsns lmplot Prices Housing Earnings Average hourly net data df comb fit reg False hue City \\n',\n",
       " '\\ndf housing earnnet Working time req to rent room df housing earnnet Prices Housing df housing earnnet Earnings Average hourly net \\n',\n",
       " '\\ndf housing earnnet sort values by Working time req to rent room inplace True \\n\\ndf housing earnnet head \\n',\n",
       " '\\nsns barplot x City y Working time req to rent room data df housing earnnet \\n',\n",
       " '\\ndf iphone df iphone reset index \\n',\n",
       " '\\nsns barplot x City y General Working time req to buy data df iphone \\n',\n",
       " '\\nimport pandas as pd\\n\\nimport numpy as np\\n\\nimport random as rnd\\n\\nimport seaborn as sns\\n\\nimport matplotlib pyplot as plt\\n\\nget ipython run line magic matplotlib inline \\n\\nfrom sklearn linear model import LogisticRegression\\n\\nfrom sklearn svm import SVC LinearSVC\\n\\nfrom sklearn ensemble import RandomForestClassifier\\n\\nfrom sklearn neighbors import KNeighborsClassifier\\n\\nfrom sklearn naive bayes import GaussianNB\\n',\n",
       " '\\ntrain df pd read csv train csv \\n\\ntest df pd read csv test csv \\n',\n",
       " '\\nprint train df columns values \\n',\n",
       " '\\ntrain df head \\n',\n",
       " '\\nimport pandas as pd\\n',\n",
       " '\\nget ipython system pip install pandas \\n',\n",
       " '\\nimport pandas as pd\\n',\n",
       " '\\ndf pd read excel Billionaire Characteristics Database richpeople xlsx \\n',\n",
       " '\\nget ipython system pip install xlrd \\n',\n",
       " '\\ndf head \\n',\n",
       " '\\ndf columns\\n',\n",
       " '\\nrecent df df year \\n\\nrecent head \\n',\n",
       " '\\nrecent countrycode value counts head \\n',\n",
       " '\\nSwiss billionaires recent recent countrycode CHE \\n\\nSwiss billionaires head \\n',\n",
       " '\\nrecent columns\\n',\n",
       " '\\nrecent sort values by age head \\n',\n",
       " '\\nrecent sort values by age ascending False head \\n',\n",
       " '\\nrecent sort values by networthusbillion ascending False head \\n',\n",
       " '\\nrecent groupby countrycode networthusbillion mean sort values ascending False head \\n',\n",
       " '\\nrecent groupby generationofinheritance count \\n',\n",
       " '\\nrecent recent generationofinheritance Fith generation or longer \\n\\ndf df POS G \\n',\n",
       " '\\nfrom future import print function division\\n\\nimport string\\n\\nimport random\\n\\nimport cPickle as pickle\\n\\nimport numpy as np\\n\\nimport pandas as pd\\n\\nimport statsmodels formula api as smf\\n\\nimport thinkstats \\n\\nimport thinkplot\\n\\nimport matplotlib pyplot as plt\\n\\nimport ess\\n\\nBLUE a cee \\n\\nBLUE f b \\n\\nGREEN b df a \\n\\nGREEN a c \\n\\nPINK fb a \\n\\nRED e a c \\n\\nORANGE fdbf f \\n\\nORANGE ff f \\n\\nPURPLE cab d \\n\\nPURPLE a d a \\n\\nYELLOW ffff \\n\\nBROWN b \\n\\nget ipython run line magic matplotlib inline \\n',\n",
       " '\\nstore pd HDFStore ess resamples h \\n',\n",
       " '\\ncountry map ess make countries store \\n',\n",
       " '\\nFORMULA treatment inwyr f yrbrn f yrbrn f edurank f hincrank f tvtot f rdtot f nwsptot f \\n\\n\\ndef compute delta group country \\n group yrbrn f group yrbrn f \\n group propensity np nan\\n group treatment np nan\\n netuse group netuse f\\n thresh netuse median \\n if thresh \\n thresh \\n group treatment netuse thresh astype int \\n model smf logit FORMULA data group \\n results model fit disp False \\n group propensity results predict group \\n treatment group group treatment \\n control group group treatment \\n series control propensity sort values \\n indices series searchsorted treatment propensity \\n indices indices \\n indices indices len control len control \\n control indices series index indices \\n matches control loc control indices \\n distances treatment propensity values matches propensity values \\n differences treatment rlgdgr f values matches rlgdgr f values \\n caliper differences abs distances \\n delta np mean caliper \\n return delta\\n',\n",
       " '\\n\\ndef process frame df country map \\n grouped df groupby cntry \\n for code group in grouped \\n country country map code \\n delta compute delta group country \\n d dict delta delta \\n country add params d \\n',\n",
       " '\\n\\ndef process all frames store country map num \\n Loops through the store and processes frames n n store store n country map map from code to Country n num how many resamplings to process n reg func function used to compute regression n formula string Patsy formula n model num which model we re running n \\n for i key in enumerate store keys \\n if i num \\n break\\n print i key \\n df store get key \\n process frame df country map \\n',\n",
       " '\\nprocess all frames store country map num \\n',\n",
       " '\\nstore close \\n',\n",
       " '\\nwith open ess pkl wb as fp \\n pickle dump country map fp \\n',\n",
       " '\\nwith open ess pkl rb as fp \\n country map pickle load fp \\n',\n",
       " '\\nlen country map DE param seq \\n',\n",
       " '\\nplot counter \\n\\n\\ndef save plot flag False \\n Saves plots in png format n n flag boolean whether to save or not n \\n global plot counter\\n if flag \\n root ess d plot counter \\n thinkplot Save root root formats png \\n plot counter \\n',\n",
       " '\\nxlabel Difference in religiosity point scale \\n',\n",
       " '\\nxlim \\n',\n",
       " '\\nreload ess \\n\\nt ess extract vars country map delta None \\n\\ness plot cis t PURPLE \\n\\nthinkplot Config title Internet use xlabel xlabel xlim xlim \\n\\nsave plot \\n',\n",
       " '\\nreload ess \\n\\ncdfnames delta \\n\\ness plot cdfs country map ess extract vars cdfnames cdfnames \\n\\nthinkplot Config xlabel xlabel xlim xlim ylabel CDF loc upper left \\n\\nsave plot \\n',\n",
       " '\\nreload ess \\n\\nvarnames delta \\n\\nts ess make table country map varnames ess extract vars \\n\\ness print table ts \\n',\n",
       " '\\nimport pandas as pd\\n\\nfrom sqlalchemy import create engine\\n',\n",
       " '\\nengine create engine postgresql format os environ PGUSER os environ PGPASSWORD os environ PGHOST os environ PGDATABASE \\n',\n",
       " '\\nimport features\\n\\ndb features FeatureStorage \\n',\n",
       " '\\nfeature tables features burst binned lengths features burst length aggregates features burst lengths features cell numbers features cell ordering features cell ordering differences features cell timings features initial cell directions features interpacket timings features size windows \\n',\n",
       " '\\nmaster indices pd read sql select exampleid from raw frontpage examples engine \\n',\n",
       " '\\nlen master indices \\n',\n",
       " '\\n\\ndef to list x \\n return list x exampleid values \\n',\n",
       " '\\nfeature indices \\n \\n \\n\\nfor table in feature tables \\n featind pd read sql select exampleid from format table engine \\n feature indices update \\n table featind \\n \\n',\n",
       " '\\nfor table in feature tables \\n print table len feature indices table \\n',\n",
       " '\\n set to list indices set to list feature indices features burst binned lengths \\n',\n",
       " '\\ndb get trace cells \\n',\n",
       " '\\ndb get trace cells \\n',\n",
       " '\\ndf features pd read sql select from features frontpage features engine \\n',\n",
       " '\\nimport numpy as np\\n\\nfrom numpy random import randn\\n\\nimport pandas as pd\\n\\nfrom scipy import stats\\n\\nimport matplotlib as mpl\\n\\nimport matplotlib pyplot as plt\\n\\nimport seaborn as sns\\n\\nget ipython run line magic matplotlib inline \\n',\n",
       " '\\ntit df pd read csv train csv \\n',\n",
       " '\\ntit df head \\n',\n",
       " '\\ntit df info \\n',\n",
       " '\\nsns barplot y Age x Sex data tit df \\n',\n",
       " '\\nfrom bokeh charts import Bar output notebook show\\n\\np Bar tit df Sex values Age agg mean \\n\\noutput notebook \\n\\nshow p \\n',\n",
       " '\\nsns barplot y Age x Pclass data tit df \\n',\n",
       " '\\nfrom bokeh charts import Bar output notebook show\\n\\np Bar tit df Pclass values Age agg mean \\n\\noutput notebook \\n\\nshow p \\n',\n",
       " '\\nsns factorplot Sex data tit df kind count \\n',\n",
       " '\\nsns factorplot Pclass data tit df kind count \\n',\n",
       " '\\ntit df Age hist bins \\n',\n",
       " '\\nfrom bokeh charts import Histogram output notebook show\\n\\nint age tit df tit df Age \\n\\np Histogram int age Age bins \\n\\noutput notebook \\n\\nshow p \\n',\n",
       " '\\nsns factorplot Sex data tit df hue Pclass kind count \\n',\n",
       " '\\n\\ndef male female child passenger \\n age sex passenger\\n if age \\n return child \\n else \\n return sex\\n',\n",
       " '\\ntit df person tit df Age Sex apply male female child axis \\n',\n",
       " '\\ntit df head \\n',\n",
       " '\\nsns factorplot Pclass data tit df hue person kind count \\n',\n",
       " '\\ntit df Age hist bins \\n',\n",
       " '\\ntit df Age mean \\n',\n",
       " '\\ntit df person value counts \\n',\n",
       " '\\noldest tit df Age max \\n\\nfig sns FacetGrid tit df hue Sex aspect \\n\\nfig map sns kdeplot Age shade True \\n\\nfig set xlim oldest \\n\\nfig add legend \\n',\n",
       " '\\noldest tit df Age max \\n\\nfig sns FacetGrid tit df hue person aspect \\n\\nfig map sns kdeplot Age shade True \\n\\nfig set xlim oldest \\n\\nfig add legend \\n',\n",
       " '\\nfrom bokeh import mpl\\n\\nfrom bokeh charts import output notebook show\\n\\noldest tit df Age max \\n\\nfig sns FacetGrid tit df hue person aspect \\n\\nfig map sns kdeplot Age shade True \\n\\nfig set xlim oldest \\n\\nfig add legend \\n\\noutput notebook \\n\\nshow mpl to bokeh \\n',\n",
       " '\\noldest tit df Age max \\n\\nfig sns FacetGrid tit df hue Pclass aspect \\n\\nfig map sns kdeplot Age shade True \\n\\nfig set xlim oldest \\n\\nfig add legend \\n',\n",
       " '\\ndeck tit df Cabin dropna \\n',\n",
       " '\\ndeck head \\n',\n",
       " '\\nlevels level for level in deck \\n',\n",
       " '\\nlevels \\n',\n",
       " '\\ncabin df pd DataFrame levels \\n',\n",
       " '\\ncabin df columns Cabin \\n',\n",
       " '\\nsns factorplot Cabin data cabin df palette winter d kind count order list ABCDEFGT \\n',\n",
       " '\\ncabin df cabin df cabin df Cabin T \\n',\n",
       " '\\nsns factorplot Cabin data cabin df palette winter d kind count order list ABCDEFG \\n',\n",
       " '\\nsns factorplot Embarked data tit df hue Pclass kind count order list CQS \\n',\n",
       " '\\ntit df head \\n',\n",
       " '\\ntit df Alone tit df SibSp tit df Parch \\n\\ntit df Alone loc tit df Alone With Family \\n\\ntit df Alone loc tit df Alone Alone \\n',\n",
       " '\\ntit df head \\n',\n",
       " '\\nsns factorplot Alone data tit df kind count \\n',\n",
       " '\\ntit df Survivor tit df Survived map \\n no \\n yes \\n \\n\\nsns factorplot Survivor data tit df kind count palette Set \\n',\n",
       " '\\nsns factorplot Pclass data tit df hue Survivor kind count \\n',\n",
       " '\\nsns factorplot Pclass Survived hue person data tit df \\n',\n",
       " '\\nsns lmplot Age Survived data tit df \\n',\n",
       " '\\nsns lmplot Age Survived hue Pclass data tit df \\n',\n",
       " '\\ngenerations \\n\\nsns lmplot Age Survived hue Pclass data tit df x bins generations \\n',\n",
       " '\\nsns lmplot Age Survived hue Sex data tit df x bins generations \\n',\n",
       " '\\ntit df Deck tit df Cabin str \\n',\n",
       " '\\nsns factorplot Deck Survived data tit df order list ABCDEFGT \\n',\n",
       " '\\nsns factorplot Survivor hue Deck data tit df kind count \\n',\n",
       " '\\nsns factorplot Survivor hue Alone data tit df kind count \\n',\n",
       " '\\nsns factorplot Survived Alone hue Pclass data tit df \\n',\n",
       " '\\nget ipython run line magic matplotlib inline \\n\\nimport pandas as pd\\n',\n",
       " '\\nfrom IPython core display import HTML\\n\\ncss open style table css read open style notebook css read \\n\\nHTML style style format css \\n',\n",
       " '\\ncast pd DataFrame from csv data cast csv index col None \\n\\ncast head \\n',\n",
       " '\\nrelease dates pd DataFrame from csv data release dates csv index col None parse dates date infer datetime format True \\n\\nrelease dates head \\n',\n",
       " '\\nrd release dates\\n\\nrd rd rd title str contains Christmas \\n\\nrd rd rd country USA \\n\\nrd date dt month value counts sort index plot kind bar \\n',\n",
       " '\\nrd release dates\\n\\nrd rd rd title str startswith The Hobbit \\n\\nrd rd rd country USA \\n\\nrd date dt month value counts sort index plot kind bar \\n',\n",
       " '\\nrd release dates\\n\\nrd rd rd title str contains Romance \\n\\nrd rd rd country USA \\n\\nrd date dt dayofweek value counts sort index plot kind bar \\n',\n",
       " '\\nrd release dates\\n\\nrd rd rd title str contains Action \\n\\nrd rd rd country USA \\n\\nrd date dt dayofweek value counts sort index plot kind bar \\n',\n",
       " '\\nusa release dates release dates country USA \\n\\nc cast\\n\\nc c c name Judi Dench \\n\\nc c c year \\n\\nc merge usa sort date \\n',\n",
       " '\\nc cast\\n\\nc c c name Judi Dench \\n\\nm c merge usa sort date \\n\\nm date dt month value counts sort index plot kind bar \\n',\n",
       " '\\nc cast\\n\\nc c c name Tom Cruise \\n\\nm c merge usa sort date \\n\\nm date dt month value counts sort index plot kind bar \\n',\n",
       " '\\nimport re\\n\\nimport string\\n\\nfrom collections import Counter\\n\\nfrom bokeh charts import Bar output file show\\n\\nfrom bokeh io import output notebook\\n\\nimport pandas as pd\\n',\n",
       " '\\n\\ndef letter frequency count file name \\n with open file name as f \\n text clean \\n counter Counter \\n for line in f \\n counter Counter join re findall a zA Z line lower \\n return counter\\n',\n",
       " '\\nletter counter letter frequency count Users KillMe Desktop ayawaqu \\n',\n",
       " '\\nletters dataframe pd DataFrame from dict letter counter orient index reset index \\n\\nletters dataframe letters dataframe rename columns \\n index character \\n count \\n \\n',\n",
       " '\\npic Bar letters dataframe character values count title Character Frequency legend None \\n\\noutput notebook \\n\\nshow pic \\n',\n",
       " '\\nimport glob\\n\\nimport homer clusterer\\n\\nimport os\\n\\nimport pandas as pd\\n',\n",
       " '\\nfiles sorted glob glob working wel \\n\\nbase working directory home ubuntu homer working cos raw \\n\\nbase cluster directory home ubuntu homer working cos clusters \\n\\nthreshold \\n\\nfiles files \\n\\nfiles\\n',\n",
       " '\\nfor filename in files \\n date filename split split \\n working directory base working directory date \\n get ipython system sudo mkdir working directory \\n get ipython system sudo chmod working directory \\n',\n",
       " '\\nfor filename in files \\n date filename split split \\n working directory base cluster directory date \\n get ipython system sudo mkdir working directory \\n get ipython system sudo chmod working directory \\n',\n",
       " '\\nfor filename in files \\n date filename split split \\n print date \\n cos raw dir base working directory date \\n clusters dir base cluster directory date \\n weighted edge list pd read csv filename \\n unweighted weighted edge list weighted edge list Count threshold \\n homer clusterer find clusters unweighted W W cos raw dir clusters dir date \\n']"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1745/1745 [00:00<00:00, 6149.35it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_clean_data = []\n",
    "for c in tqdm(test_corpus):\n",
    "    token = split_func_name(c)\n",
    "    token = [t for t in token if t]\n",
    "    test_clean_data.append(' '.join(token))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_documents = test_clean_data\n",
    "# vectorizer.fit(documents)\n",
    "test_X = vectorizer.transform(test_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1745x10000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 24737 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lda_results = loaded_model.transform(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lda_results.dump('./test_lda_results_1_30_2.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_graphs[]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python allennlp",
   "language": "python",
   "name": "allennlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
