{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agglomerative Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn import cluster\n",
    "import scipy.cluster.hierarchy as sch\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from extract_func import process_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('./func_counter.json','r') as f:\n",
    "    func_counter = json.load(f)\n",
    "sklearn_counter = {k: func_counter[k] for k in func_counter if k.startswith('sklearn.')}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what is the point to do clustering before choosing representative functions?\n",
    "Why can't we just sort all the functions and choose short ones? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_path = '/projects/bdata/jupyter/target'\n",
    "notebooks = os.listdir(nb_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filter notebooks (which import sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_notebooks = []\n",
    "for nb in tqdm(notebooks):\n",
    "    with open(os.path.join(nb_path, nb),'r') as f:\n",
    "#         print('--')\n",
    "        content = f.read()\n",
    "    tokens = content.split()\n",
    "    if 'sklearn' in tokens:\n",
    "        sklearn_notebooks.append(nb)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func2vector = {}\n",
    "vector_size = len(sklearn_notebooks)\n",
    "err_files = []\n",
    "for i, nb in enumerate(sklearn_notebooks):\n",
    "    if i%10000 == 0:\n",
    "        print('Log: {} notebooks processed'.format(i))\n",
    "    funcs = []\n",
    "    try:\n",
    "        funcs, linenos = process_file(os.path.join(nb_path, nb))\n",
    "    except Exception as e:\n",
    "        err_files.append(nb)\n",
    "    funcs = [func for func in funcs if func.startswith('sklearn.')]\n",
    "    for func in funcs:\n",
    "#         if not func.startswith('sklearn'):\n",
    "#             continue\n",
    "        if func not in func2vector:\n",
    "            func2vector[func] = np.zeros(vector_size)\n",
    "        func2vector[func][i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2func = list(func2vector.keys())\n",
    "func2idx = {f: i for i, f in enumerate(idx2func)}\n",
    "vectors = [func2vector[f] for f in idx2func]\n",
    "occur_matrix = np.stack(vectors)\n",
    "cooccur_matrix = np.dot(occur_matrix, occur_matrix.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('sklearn_cooccur_mat.npy', cooccur_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cluster.AgglomerativeClustering(n_clusters = None, distance_threshold=0.1, affinity=\"precomputed\", linkage=\"average\").fit(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = []\n",
    "n_samples = len(idx2func)\n",
    "for child in model.children_:\n",
    "    clusters.append([])\n",
    "    for branch in child:\n",
    "        if branch < n_samples:\n",
    "            clusters[-1].append(idx2func[branch])\n",
    "        else:\n",
    "            clusters[-1]+=clusters[branch-n_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./cluster.json','r') as f:\n",
    "    clusters = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_points = []\n",
    "last_point = ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_representatives_in_cluster(cluster):\n",
    "    \"\"\"\n",
    "    cluster: [\"sklearn.linear_model.LogisticRegression\",\n",
    "              \"sklearn.linear_model.LogisticRegression.fit\"]\n",
    "    \"\"\"\n",
    "    representatives = []\n",
    "    if len(cluster)>10 or len(cluster)<3:\n",
    "        return representatives\n",
    "    cluster = sorted(cluster)\n",
    "    last_point = \"*\"\n",
    "    for func in cluster:\n",
    "        if not func.startswith(last_point):\n",
    "            last_point = func\n",
    "            representatives.append(last_point)\n",
    "    return representatives\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_points = []\n",
    "for i, c in enumerate(clusters['clusters']):\n",
    "    points = find_representatives_in_cluster(c)\n",
    "    decision_points+=points\n",
    "#     if 'sklearn.base.clone.fit' in points:\n",
    "    if 'sklearn.cluster.Birch.fit' in points:\n",
    "#         sklearn.cluster.Birch.fit\n",
    "        print(i)\n",
    "        print(sorted(c))\n",
    "decision_points = list(set(decision_points))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rp_counter = {f: sklearn_counter[f] for f in sorted(decision_points) if f in sklearn_counter}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dendrogram(model, **kwargs):\n",
    "\n",
    "    # Children of hierarchical clustering\n",
    "    children = model.children_\n",
    "\n",
    "    # Distances between each pair of children\n",
    "    # Since we don't have this information, we can use a uniform one for plotting\n",
    "    distance = np.arange(children.shape[0])\n",
    "\n",
    "    # The number of observations contained in each cluster level\n",
    "    no_of_observations = np.arange(2, children.shape[0]+2)\n",
    "\n",
    "    # Create linkage matrix and then plot the dendrogram\n",
    "    linkage_matrix = np.column_stack([children, distance, no_of_observations]).astype(float)\n",
    "\n",
    "    # Plot the corresponding dendrogram\n",
    "    dendrogram(linkage_matrix, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = AgglomerativeClustering(n_clusters=3)\n",
    "\n",
    "# model = model.fit(x)\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plot_dendrogram(model, labels=model.labels_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 1- cooccur_matrix/cooccur_matrix.max()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python allennlp",
   "language": "python",
   "name": "allennlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
