{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What does thid notebook do?  \n",
    "using agglomerative clustering to find posible decision points in all notebooks  \n",
    "\n",
    "# File mapping \n",
    "sklearn | clustering | ---: `./sklearn_dp.txt`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agglomerative Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn import cluster\n",
    "import scipy.cluster.hierarchy as sch\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import import_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from extract_func import process_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('./temp.out','r') as f:\n",
    "    func_counter = json.load(f)[\"func_counter\"]\n",
    "statsmodels_counter = {k: func_counter[k] for k in func_counter if k.startswith('statsmodels.')}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what is the point to do clustering before choosing representative functions?\n",
    "Why can't we just sort all the functions and choose short ones? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb_path = '/projects/bdata/jupyter/target'\n",
    "nb_path = '/home/gezhang/data/jupyter/target'\n",
    "notebooks = os.listdir(nb_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filter notebooks (which import sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statsmodels_notebooks = []\n",
    "for i, nb in enumerate(notebooks):\n",
    "    if i%10000==0:\n",
    "        print(i)\n",
    "    with open(os.path.join(nb_path, nb),'r') as f:\n",
    "#         print('--')\n",
    "        content = f.read()\n",
    "    tokens = content.split()\n",
    "    if 'statsmodels' in tokens:\n",
    "        statsmodels_notebooks.append(nb)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# delete loops:\n",
    "eg  \n",
    "**sklearn.tree.DecisionTreeClassifier.fit.fit.fit**  \n",
    "is equal to   \n",
    "**sklearn.tree.DecisionTreeClassifier.fit**  \n",
    "  \n",
    "the only difference is that the first one is called multiple times  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_loop(func):\n",
    "    \"\"\"\n",
    "    delete continue loops in a function\n",
    "    simplify **sklearn.tree.DecisionTreeClassifier.fit.fit.fit** \n",
    "          to **sklearn.tree.DecisionTreeClassifier.fit**\n",
    "    \"\"\"\n",
    "    tokens = func.split('.')\n",
    "    new_tokens = []\n",
    "    for t in tokens:\n",
    "        if new_tokens == [] or t!=new_tokens[-1]:\n",
    "            new_tokens.append(t)\n",
    "    return '.'.join(new_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_loop_statsmodels_funcs = []\n",
    "for k in statsmodels_counter:\n",
    "    t = delete_loop(k)\n",
    "    if t not in no_loop_statsmodels_funcs:\n",
    "        no_loop_statsmodels_funcs.append(t)\n",
    "no_loop_statsmodels_funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func2vector = {}\n",
    "vector_size = len(statsmodels_notebooks)\n",
    "err_files = []\n",
    "for i, nb in enumerate(statsmodels_notebooks):\n",
    "    if i%10000 == 0:\n",
    "        print('Log: {} notebooks processed'.format(i))\n",
    "    funcs = []\n",
    "    try:\n",
    "        funcs, linenos = process_file(os.path.join(nb_path, nb))\n",
    "    except Exception as e:\n",
    "        err_files.append(nb)\n",
    "    funcs = [func for func in funcs if func in no_loop_statsmodels_funcs]\n",
    "    for func in funcs:\n",
    "#         if not func.startswith('statsmodels'):\n",
    "#             continue\n",
    "        if func not in func2vector:\n",
    "            func2vector[func] = np.zeros(vector_size)\n",
    "        func2vector[func][i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2func = list(func2vector.keys())\n",
    "func2idx = {f: i for i, f in enumerate(idx2func)}\n",
    "vectors = [func2vector[f] for f in idx2func]\n",
    "occur_matrix = np.stack(vectors)\n",
    "cooccur_matrix = np.dot(occur_matrix, occur_matrix.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('statsmodels_cooccur_mat.npy', cooccur_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cooccur_matrix = np.load('keras_cooccur_mat.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 1- cooccur_matrix/cooccur_matrix.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cluster.AgglomerativeClustering(n_clusters = None, distance_threshold=0.4, affinity=\"precomputed\", linkage=\"average\").fit(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = []\n",
    "n_samples = len(idx2func)\n",
    "for child in model.children_:\n",
    "    clusters.append([])\n",
    "    for branch in child:\n",
    "        if branch < n_samples:\n",
    "            clusters[-1].append(idx2func[branch])\n",
    "        else:\n",
    "            clusters[-1]+=clusters[branch-n_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./statsmodels_cluster.json','w') as fout:\n",
    "    json.dump(clusters,fout, ensure_ascii=False,indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./keras_cluster.json','r') as f:\n",
    "    clusters = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_clusters = [sorted(c) for c in clusters if len(c)<=3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_root = []\n",
    "sets = []\n",
    "has_root = []\n",
    "\n",
    "def find_root_in_cluster(cluster):\n",
    "    \"\"\"\n",
    "    find a representative function in a cluster like:\n",
    "     ['sklearn.preprocessing.MinMaxScaler',\n",
    "      'sklearn.preprocessing.MinMaxScaler.fit_transform',\n",
    "      'sklearn.preprocessing.MinMaxScaler.transform']\n",
    "    return a list []\n",
    "    \"\"\"\n",
    "    if len(cluster)>3:\n",
    "        return []\n",
    "    cluster = sorted(cluster)\n",
    "    last_root = '*'\n",
    "    cluster_roots = []\n",
    "    for f in cluster:\n",
    "        if not f.startswith(last_root):\n",
    "            cluster_roots.append(f)\n",
    "            last_root = f\n",
    "        elif f not in has_root:\n",
    "            has_root.append(f)\n",
    "    cluster_roots = [r for r in cluster_roots if not any([r.startswith(root) for root in roots])]\n",
    "    return cluster_roots\n",
    "            \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roots = []\n",
    "for c in tqdm(temp_clusters):\n",
    "#     print('-'*20)\n",
    "#     print(c)\n",
    "    roots+=find_root_in_cluster(c)\n",
    "#     print(find_root_in_cluster(c))\n",
    "roots = sorted(set(roots))\n",
    "# {r: sklearn_counter[r] for r in roots if r.endswith(\".predict\")}\n",
    "# [r for r in roots if sklearn_counter[r]>200]\n",
    "roots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statsmodels_dp = {r: statsmodels_counter[r] for r in roots if statsmodels_counter[r]>5 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in statsmodels_dp:\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./statsmodels_dp.txt','w') as fout:\n",
    "    fout.write('\\n'.join(statsmodels_dp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./statsmodels_dp.txt','r') as f:\n",
    "    statsmodels_dp = f.read().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract structure from statsmodels_roots\n",
    "statsmodels_roots = ['.'.join(f.split('.')[:-1]) for f in statsmodels_dp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roots_counter = {}\n",
    "for r in statsmodels_roots:\n",
    "    if r not in roots_counter:\n",
    "        roots_counter[r]=0\n",
    "    roots_counter[r]+=1\n",
    "frequent_roots = {r: roots_counter[r] for r in roots_counter if roots_counter[r]>=5}\n",
    "for r in frequent_roots:\n",
    "#     if r==\"statsmodels\":continue\n",
    "    try:\n",
    "        mod = import_module(r)\n",
    "#         print(mod.__all__)\n",
    "        try:\n",
    "            dps = ['{}.{}'.format(r, obj) for obj in mod.__all__] \n",
    "        except:\n",
    "            dps =  ['{}.{}'.format(r, obj) for obj in dir(mod) if not obj.startswith('_')]\n",
    "#         dps = ['{}.{}'.format(r, obj) for obj in dir(mod) if not obj.startswith('_')]\n",
    "        statsmodels_dp+=dps\n",
    "#         print(dps)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "statsmodels_dp = sorted(list(set(statsmodels_dp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in statsmodels_dp:\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = func2vector[\"sklearn.cluster.Birch.fit\"]\n",
    "v2 = func2vector[\"sklearn.cluster.Birch\"]\n",
    "v3 = func2vector[\"sklearn.cluster.AgglomerativeClustering.labels_.astype\"]\n",
    "np.linalg.norm(v1- v3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
